{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BisQue Documentation Bio-Image Semantic Query User Environment https://bioimage.ucsb.edu/bisque Project Source Current Github: https://github.com/UCSB-VRL/bisque Docs : https://UCSB-VRL.github.io/bisque-dev Legacy Bitbucket : https://bitbucket.org/CBIucsb/bisque Wiki : http://biodev.ece.ucsb.edu/projects/bisquik Clone BisQue Explore the source by checking out the bisque-stable branch from legacy mercurial or current github git clone https://github.com/UCSB-VRL/bisque bisque-stable Docs Bisque Development Environment (artifact/guides) Bisque setup from source code Bisque setup in Docker Jupyter Notebooks Common Errors Bisque Module development Module Development Instructions: TODO Bisque Planteome Module Bisque Rancher Deployment Environment Rancher 2.0 with RKE Setup Postgresql Server setup for Bisque","title":"Home"},{"location":"#bisque-documentation","text":"Bio-Image Semantic Query User Environment https://bioimage.ucsb.edu/bisque","title":"BisQue Documentation"},{"location":"#project-source","text":"","title":"Project Source"},{"location":"#current","text":"Github: https://github.com/UCSB-VRL/bisque Docs : https://UCSB-VRL.github.io/bisque-dev Legacy Bitbucket : https://bitbucket.org/CBIucsb/bisque Wiki : http://biodev.ece.ucsb.edu/projects/bisquik","title":"Current"},{"location":"#clone-bisque","text":"Explore the source by checking out the bisque-stable branch from legacy mercurial or current github git clone https://github.com/UCSB-VRL/bisque bisque-stable","title":"Clone BisQue"},{"location":"#docs","text":"","title":"Docs"},{"location":"#bisque-development-environment-artifactguides","text":"Bisque setup from source code Bisque setup in Docker Jupyter Notebooks Common Errors","title":"Bisque Development Environment (artifact/guides)"},{"location":"#bisque-module-development","text":"Module Development Instructions: TODO Bisque Planteome Module","title":"Bisque Module development"},{"location":"#bisque-rancher-deployment-environment","text":"Rancher 2.0 with RKE Setup Postgresql Server setup for Bisque","title":"Bisque Rancher Deployment Environment"},{"location":"guides/about/","text":"About Wiki: http://biodev.ece.ucsb.edu/projects/bisquik Source Bitbucket: https://bitbucket.org/CBIucsb/bisque","title":"About"},{"location":"guides/about/#about","text":"Wiki: http://biodev.ece.ucsb.edu/projects/bisquik","title":"About"},{"location":"guides/about/#source","text":"Bitbucket: https://bitbucket.org/CBIucsb/bisque","title":"Source"},{"location":"guides/bisque/","text":"Bisque Development Environment Setup Instructions Project Source - https://bitbucket.org/CBIucsb/bisque - http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable Reference - Bique Bioimage Google Groups - Instructions on installing bisque using docker Setup for Ubuntu 16.04 Pre-requisites sudo apt-get install -y python python-dev python-virtualenv python-numpy python-scipy sudo apt-get install -y libxml2-dev libxslt1-dev libhdf5-dev sudo apt-get install -y libmysqlclient-dev libpq-dev mercurial git cmake sudo apt-get install -y postgresql postgresql-client libsqlite3-dev sudo apt-get install -y python-paver python-setuptools sudo apt-get install -y graphviz libgraphviz-dev pkg-config sudo apt-get install -y openslide-tools python-openslide sudo apt-get install -y libfftw3-dev libbz2-dev libz-dev sudo apt-get install -y liblcms2-dev libtiff-dev libpng-dev sudo apt-get install -y libgdcm2.6 libopenslide-dev libopenslide0 Install Openjpeg git clone https://github.com/uclouvain/openjpeg cd openjpeg mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release sudo make -j4 install sudo ldconfig Install BioImageConvert - BioImageConvert Source Repository - Prebuilt Binaries Repository - Setup for pre-built binaries (below) # Ubuntu 18 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu18_2.5.0.tar.gz # Ubuntu 16 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu16_2.4.3.tar.gz tar -xvzf imgcnv_ubuntu16_2.4.3.tar.gz sudo cp imgcnv_ubuntu16_2.4.3/imgcnv /usr/local/bin/ sudo cp imgcnv_ubuntu16_2.4.3/libimgcnv.so.2.4.3 /usr/local/lib/ sudo ln -s /usr/local/lib/libimgcnv.so.2.4.3 /usr/local/lib/libimgcnv.so.2.4 sudo ln -s /usr/local/lib/libimgcnv.so.2.4 /usr/local/lib/libimgcnv.so.2 sudo ln -s /usr/local/lib/libimgcnv.so.2 /usr/local/lib/libimgcnv.so sudo ldconfig Alternately, Compile by source and Install hg clone --insecure http://biodev.ece.ucsb.edu/hg/imgcnv cd imgcnv make -j6 sudo make install A. Download the Bootstrap installer (Use Python 2.7) $ mkdir bisque cd bisque $ wget http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable/contrib/bootstrap/bisque-bootstrap.py $ python bisque-bootstrap.py bqenv # Activate Virtualenv bisque$ source bqenv/bin/activate Fix the requirements.txt #Fix the requirements.txt file using sed -i s/.*psycopg2==2.6.1.*/psycopg2==2.7.1./ requirements.txt psycopg2==2.7.1 Minimatic==1.0 Paste==1.7.5.1 httplib2==0.7.3 #tgext.registration==0.1dev Install separately since packages may be deprecated in PyPi easy_install http://biodev.ece.ucsb.edu/binaries/depot/tgext.registration2/tgext.registration2-0.5.2.tar.gz Now Install requirements pip install -i https://biodev.ece.ucsb.edu/py/bisque/stretch/+simple/ -r requirements.txt Alternate Index Url for Development: https://biodev.ece.ucsb.edu/py/bisque/dev/+simple B. Configure Bisque Environment $ paver setup Expected log tail Installing collected packages: bqengine Running setup.py develop for bqengine Successfully installed bqengine Now run: bq-admin setup $ bq-admin setup Expected log tail ... Found imgcnv version 2.4.3 Imgcnv is installed and no-precompiled version exists. Using installed version Top level site variables are: bisque.admin_email=YourEmail@YourOrganization bisque.admin_id=admin bisque.organization=Your Organization bisque.paths.root=. bisque.server=http://loup.ece.ucsb.edu:8088 bisque.title=Image Repository Change a site variable [Y]? N ... Running setup_config() from bq.websetup CALLING function install_preferences at 0x7f47f191f848 Initialize your database with: $ bq-admin setup createdb You can start bisque with: $ bq-admin server start then point your browser to: http://loup.ece.ucsb.edu:8088 If you need to shutdown the servers, then use: $ bq-admin server stop You can login as admin and change the default password. Send Installation/Registration report [Y]? N Add \"config/runtime-bisque.cfg\" for module configuration and docker image registry - Edit and add run config from config-defaults/runtime-bisque.defaults to config/runtime-bisque.cfg C. Run Bisque Server Start/Stop the server $ bq-admin server start $ bq-admin server stop Overview of Installation (Just for review) source bqenv/bin/activate paver setup [server|engine] bq-admin setup [server|engine] bq-admin deploy public Open in browser D. Upload Dataset Upload an image file from your local directory to Bisque Browse uploaded dataset in the file explorer Select Browse menu item in the navigation bar and click on dataset Thereafter click on the files tab and click through the local database folder in the left document tree section. Select the uploaded file to view This visualization is by default 2D in nature. The various slices can be stepped through or played using the scroll bar on righ bottom of the image view area A 3D visualization can be explored by clicking the cube icon towards the right of share and delete icons on the tool bar right below the navigation bar. E. Module Load Module Install Module packages and dependencies Ensure ~/bisque/config/runtime-bisque.cfg has the configuration as below runtime.staging_base = staging/ docker.hub = biodev.ece.ucsb.edu:5000 Make sure the runtime-module.cfg is as per below module_enabled = True runtime.platforms=command [command] environments = Staged executable = python MetaData.py files = MetaData.py Now build/compile this module cd ~/bisque/modules/MetaData python setup.py Open up the Bisque web interface at http://loup.ece.ucsb.edu:8088 Login using admin:admin Update the email address in the users context menu item (This is important) Open Module Manager from the admin user context menu Paste the following URL in the Module- Engine Module http://loup.ece.ucsb.edu:8088/engine_service/ List of engine modules could be seen Drag-n-Drop say MetaData module from the list to the left window and close the window Run Module Refresh the page, go to Analyse and select Metadata module there Select a single image for test Execute Run command and observe the updates in the results section F. Debug Module Every module run will generate a model execution identifier to track this execution. You can observe this after clicking the run command and you have a result. It could be success or fail in the Results section http://loup.ece.ucsb.edu:8088/module_service/MetaData/?wpublic=1#mex=http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF The mex identifier (==MEX_ID=00-ZmuAoE43wTxByycmaCnvBF==) observed from URL can be used to locate this execution code resulting logs in the staging folders change directory to this staging folder and observe the files there cd ~/bisque/staging/00-ZmuAoE43wTxByycmaCnvBF rahul@loup:~/bisque/staging/00-ZmuAoE43wTxByycmaCnvBF$ tree . \u251c\u2500\u2500 MetaData.log \u251c\u2500\u2500 MetaData.py \u251c\u2500\u2500 python.log \u2514\u2500\u2500 state00-ZmuAoE43wTxByycmaCnvBF.bq 0 directories, 4 files Here we can see that the MetaData.log and python.log files are generated In case of issues these are the log files that we need to look into for detailed error reporting main log file is the bisque_8088.log file at the ~/bisque home directory 13:45:11,996 INFO [bq.root] [admin] POST http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF?view=short 13:45:12,001 INFO [bq.module_server] MEX APPEND http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF?view=short 13:45:12,205 INFO [bq.engine_service.command_run] SUCCESS Command python MetaData.py http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF admin:00-ZmuAoE43wTxByycmaCnvBF with 0 13:45:12,206 INFO [bq.engine_service.command_run] All processes have returned [ finished ] 13:45:12,206 INFO [bq.engine_service.command_run] finishing 1 mexes - [{ files : [ MetaData.py ], status : finished , executable : [ python , MetaData.py , http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh , http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF , admin:00-ZmuAoE43wTxByycmaCnvBF ], initial_dir : /home/rahul/repository/github/bisque-dev , module_enabled : True , named_args : { bisque_token : admin:00-ZmuAoE43wTxByycmaCnvBF , image_url : http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh , mex_url : http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF }, bisque_token : u admin:00-ZmuAoE43wTxByycmaCnvBF , environments : Staged , runtime.staging_base : staging/ , mex_id : 00-ZmuAoE43wTxByycmaCnvBF , runtime.matlab_launcher : config-defaults/templates/matlab_launcher_SYS.tmpl , arguments : [], module_dir : /home/rahul/repository/github/bisque-dev , staging_path : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF , iterables : False, log_name : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF/python.log , runtime.matlab_home : , mex_url : http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF , rundir : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF , runtime.platforms : command }] This will show us the COMMAND that was run to execute this module with MEX_URL python MetaData.py \\ http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh \\ http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF \\ admin:00-ZmuAoE43wTxByycmaCnvBF This is the command that we can use to debug and rerun/replay this MEX in order to update the execution result. This is different from running the module again from the web interface since that will produce a new MEX_ID and create another staging folder corresponding to that. G. Module MEX/UI Click on Browse- mex to go to the Model execution page Here we will be able to see the various MEX sessions that were run for all modules under execution We will be able to obtain the MEX_ID and view its state from here by selecting one of the executions of interest. In case we select a module that has errors in it then it would turn out to show the status with messages in red Now let us go to Browse- dataset and select the file that we ran the module for. Here we will be able to view the Annotation that was added by the Metadata module to this image. Annotation can be visualized using the \"Annotations\" tab on the right The Model execution runs can be seen in the \"Analysis\" tab on the right Further Docker Condor based modules require additional setup ==TODO== - Install Docker (https://docs.docker.com/install/linux/docker-ce/ubuntu/) - Also go through the post-installation steps Install Condor (https://research.cs.wisc.edu/htcondor/ubuntu/) Need to understand the configurations as well for master slave setup rahul@bqdev:~$ /etc/init.d/condor restart [ ok ] Restarting condor (via systemctl): condor.service. rahul@bqdev:~$ ps -ef | grep condor","title":"BisQue"},{"location":"guides/bisque/#bisque-development-environment-setup-instructions","text":"Project Source - https://bitbucket.org/CBIucsb/bisque - http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable Reference - Bique Bioimage Google Groups - Instructions on installing bisque using docker","title":"Bisque Development Environment Setup Instructions"},{"location":"guides/bisque/#setup-for-ubuntu-1604","text":"","title":"Setup for Ubuntu 16.04"},{"location":"guides/bisque/#pre-requisites","text":"sudo apt-get install -y python python-dev python-virtualenv python-numpy python-scipy sudo apt-get install -y libxml2-dev libxslt1-dev libhdf5-dev sudo apt-get install -y libmysqlclient-dev libpq-dev mercurial git cmake sudo apt-get install -y postgresql postgresql-client libsqlite3-dev sudo apt-get install -y python-paver python-setuptools sudo apt-get install -y graphviz libgraphviz-dev pkg-config sudo apt-get install -y openslide-tools python-openslide sudo apt-get install -y libfftw3-dev libbz2-dev libz-dev sudo apt-get install -y liblcms2-dev libtiff-dev libpng-dev sudo apt-get install -y libgdcm2.6 libopenslide-dev libopenslide0 Install Openjpeg git clone https://github.com/uclouvain/openjpeg cd openjpeg mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release sudo make -j4 install sudo ldconfig Install BioImageConvert - BioImageConvert Source Repository - Prebuilt Binaries Repository - Setup for pre-built binaries (below) # Ubuntu 18 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu18_2.5.0.tar.gz # Ubuntu 16 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu16_2.4.3.tar.gz tar -xvzf imgcnv_ubuntu16_2.4.3.tar.gz sudo cp imgcnv_ubuntu16_2.4.3/imgcnv /usr/local/bin/ sudo cp imgcnv_ubuntu16_2.4.3/libimgcnv.so.2.4.3 /usr/local/lib/ sudo ln -s /usr/local/lib/libimgcnv.so.2.4.3 /usr/local/lib/libimgcnv.so.2.4 sudo ln -s /usr/local/lib/libimgcnv.so.2.4 /usr/local/lib/libimgcnv.so.2 sudo ln -s /usr/local/lib/libimgcnv.so.2 /usr/local/lib/libimgcnv.so sudo ldconfig Alternately, Compile by source and Install hg clone --insecure http://biodev.ece.ucsb.edu/hg/imgcnv cd imgcnv make -j6 sudo make install","title":"Pre-requisites"},{"location":"guides/bisque/#a-download-the-bootstrap-installer-use-python-27","text":"$ mkdir bisque cd bisque $ wget http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable/contrib/bootstrap/bisque-bootstrap.py $ python bisque-bootstrap.py bqenv # Activate Virtualenv bisque$ source bqenv/bin/activate Fix the requirements.txt #Fix the requirements.txt file using sed -i s/.*psycopg2==2.6.1.*/psycopg2==2.7.1./ requirements.txt psycopg2==2.7.1 Minimatic==1.0 Paste==1.7.5.1 httplib2==0.7.3 #tgext.registration==0.1dev Install separately since packages may be deprecated in PyPi easy_install http://biodev.ece.ucsb.edu/binaries/depot/tgext.registration2/tgext.registration2-0.5.2.tar.gz Now Install requirements pip install -i https://biodev.ece.ucsb.edu/py/bisque/stretch/+simple/ -r requirements.txt Alternate Index Url for Development: https://biodev.ece.ucsb.edu/py/bisque/dev/+simple","title":"A. Download the Bootstrap installer (Use Python 2.7)"},{"location":"guides/bisque/#b-configure-bisque-environment","text":"$ paver setup Expected log tail Installing collected packages: bqengine Running setup.py develop for bqengine Successfully installed bqengine Now run: bq-admin setup $ bq-admin setup Expected log tail ... Found imgcnv version 2.4.3 Imgcnv is installed and no-precompiled version exists. Using installed version Top level site variables are: bisque.admin_email=YourEmail@YourOrganization bisque.admin_id=admin bisque.organization=Your Organization bisque.paths.root=. bisque.server=http://loup.ece.ucsb.edu:8088 bisque.title=Image Repository Change a site variable [Y]? N ... Running setup_config() from bq.websetup CALLING function install_preferences at 0x7f47f191f848 Initialize your database with: $ bq-admin setup createdb You can start bisque with: $ bq-admin server start then point your browser to: http://loup.ece.ucsb.edu:8088 If you need to shutdown the servers, then use: $ bq-admin server stop You can login as admin and change the default password. Send Installation/Registration report [Y]? N Add \"config/runtime-bisque.cfg\" for module configuration and docker image registry - Edit and add run config from config-defaults/runtime-bisque.defaults to config/runtime-bisque.cfg","title":"B. Configure Bisque Environment"},{"location":"guides/bisque/#c-run-bisque-server","text":"Start/Stop the server $ bq-admin server start $ bq-admin server stop Overview of Installation (Just for review) source bqenv/bin/activate paver setup [server|engine] bq-admin setup [server|engine] bq-admin deploy public Open in browser","title":"C. Run Bisque Server"},{"location":"guides/bisque/#d-upload-dataset","text":"Upload an image file from your local directory to Bisque Browse uploaded dataset in the file explorer Select Browse menu item in the navigation bar and click on dataset Thereafter click on the files tab and click through the local database folder in the left document tree section. Select the uploaded file to view This visualization is by default 2D in nature. The various slices can be stepped through or played using the scroll bar on righ bottom of the image view area A 3D visualization can be explored by clicking the cube icon towards the right of share and delete icons on the tool bar right below the navigation bar.","title":"D. Upload Dataset"},{"location":"guides/bisque/#e-module","text":"","title":"E. Module"},{"location":"guides/bisque/#load-module","text":"Install Module packages and dependencies Ensure ~/bisque/config/runtime-bisque.cfg has the configuration as below runtime.staging_base = staging/ docker.hub = biodev.ece.ucsb.edu:5000 Make sure the runtime-module.cfg is as per below module_enabled = True runtime.platforms=command [command] environments = Staged executable = python MetaData.py files = MetaData.py Now build/compile this module cd ~/bisque/modules/MetaData python setup.py Open up the Bisque web interface at http://loup.ece.ucsb.edu:8088 Login using admin:admin Update the email address in the users context menu item (This is important) Open Module Manager from the admin user context menu Paste the following URL in the Module- Engine Module http://loup.ece.ucsb.edu:8088/engine_service/ List of engine modules could be seen Drag-n-Drop say MetaData module from the list to the left window and close the window","title":"Load Module"},{"location":"guides/bisque/#run-module","text":"Refresh the page, go to Analyse and select Metadata module there Select a single image for test Execute Run command and observe the updates in the results section","title":"Run Module"},{"location":"guides/bisque/#f-debug-module","text":"Every module run will generate a model execution identifier to track this execution. You can observe this after clicking the run command and you have a result. It could be success or fail in the Results section http://loup.ece.ucsb.edu:8088/module_service/MetaData/?wpublic=1#mex=http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF The mex identifier (==MEX_ID=00-ZmuAoE43wTxByycmaCnvBF==) observed from URL can be used to locate this execution code resulting logs in the staging folders change directory to this staging folder and observe the files there cd ~/bisque/staging/00-ZmuAoE43wTxByycmaCnvBF rahul@loup:~/bisque/staging/00-ZmuAoE43wTxByycmaCnvBF$ tree . \u251c\u2500\u2500 MetaData.log \u251c\u2500\u2500 MetaData.py \u251c\u2500\u2500 python.log \u2514\u2500\u2500 state00-ZmuAoE43wTxByycmaCnvBF.bq 0 directories, 4 files Here we can see that the MetaData.log and python.log files are generated In case of issues these are the log files that we need to look into for detailed error reporting main log file is the bisque_8088.log file at the ~/bisque home directory 13:45:11,996 INFO [bq.root] [admin] POST http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF?view=short 13:45:12,001 INFO [bq.module_server] MEX APPEND http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF?view=short 13:45:12,205 INFO [bq.engine_service.command_run] SUCCESS Command python MetaData.py http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF admin:00-ZmuAoE43wTxByycmaCnvBF with 0 13:45:12,206 INFO [bq.engine_service.command_run] All processes have returned [ finished ] 13:45:12,206 INFO [bq.engine_service.command_run] finishing 1 mexes - [{ files : [ MetaData.py ], status : finished , executable : [ python , MetaData.py , http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh , http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF , admin:00-ZmuAoE43wTxByycmaCnvBF ], initial_dir : /home/rahul/repository/github/bisque-dev , module_enabled : True , named_args : { bisque_token : admin:00-ZmuAoE43wTxByycmaCnvBF , image_url : http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh , mex_url : http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF }, bisque_token : u admin:00-ZmuAoE43wTxByycmaCnvBF , environments : Staged , runtime.staging_base : staging/ , mex_id : 00-ZmuAoE43wTxByycmaCnvBF , runtime.matlab_launcher : config-defaults/templates/matlab_launcher_SYS.tmpl , arguments : [], module_dir : /home/rahul/repository/github/bisque-dev , staging_path : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF , iterables : False, log_name : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF/python.log , runtime.matlab_home : , mex_url : http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF , rundir : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF , runtime.platforms : command }] This will show us the COMMAND that was run to execute this module with MEX_URL python MetaData.py \\ http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh \\ http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF \\ admin:00-ZmuAoE43wTxByycmaCnvBF This is the command that we can use to debug and rerun/replay this MEX in order to update the execution result. This is different from running the module again from the web interface since that will produce a new MEX_ID and create another staging folder corresponding to that.","title":"F. Debug Module"},{"location":"guides/bisque/#g-module-mexui","text":"Click on Browse- mex to go to the Model execution page Here we will be able to see the various MEX sessions that were run for all modules under execution We will be able to obtain the MEX_ID and view its state from here by selecting one of the executions of interest. In case we select a module that has errors in it then it would turn out to show the status with messages in red Now let us go to Browse- dataset and select the file that we ran the module for. Here we will be able to view the Annotation that was added by the Metadata module to this image. Annotation can be visualized using the \"Annotations\" tab on the right The Model execution runs can be seen in the \"Analysis\" tab on the right","title":"G. Module MEX/UI"},{"location":"guides/bisque/#further","text":"","title":"Further"},{"location":"guides/bisque/#docker-condor-based-modules-require-additional-setup","text":"==TODO== - Install Docker (https://docs.docker.com/install/linux/docker-ce/ubuntu/) - Also go through the post-installation steps Install Condor (https://research.cs.wisc.edu/htcondor/ubuntu/) Need to understand the configurations as well for master slave setup rahul@bqdev:~$ /etc/init.d/condor restart [ ok ] Restarting condor (via systemctl): condor.service. rahul@bqdev:~$ ps -ef | grep condor","title":"Docker &amp; Condor based modules require additional setup"},{"location":"guides/bisque_docker/","text":"Bisque Docker Environment Setup Instructions Docker/Project Source Docker Hub Image Bitbucket CBIucsb/bisque-stable (TODO: Test with Github version!!) Bique Bioimage Google Groups Pre-requisite Install docker (light weight VM) on you laptop: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Instructions on installing bisque using docker https://bitbucket.org/CBIucsb/bisque/src/default/README.md Installation Use the installer script to setup bisque docker image wget https://bitbucket.org/CBIucsb/bisque-stable/downloads/build_bisque_docker_modules.sh sh build_bisque_docker_modules.sh You can modify this script to create a modules folder and mount/load modules from the host machine into docker Run Environment Start Docker and mount directories for run # Start xterm -e \\ docker run --name bisque --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ cbiucsb/bisque05:stable Stop Docker docker stop $(docker ps -a -q --filter ancestor=cbiucsb/bisque05:stable --format= {{.ID}} ) Advanced environment for working with database URI as, sqlite:///data/bisque.db postgresql://rahul:rahul@localhost/bqmurks58 or postgresql://dbhost:5432/bisque docker run --name bisque --rm -p 8080:8080 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ -e BISQUE_DBURL=postgresql://rahul:rahul@localhost/bqmurks58 \\ cbiucsb/bisque05:stable This will start the bisque docker/server Client Service http://0.0.0.0:8080 Engine Service http://0.0.0.0:8080/engine_service Module Service http://0.0.0.0:8080/module_service Features http://0.0.0.0:8080/features Use \"admin:admin\" to authenticate into this local environment To manipulate and check logs, connect to docker bash bisque-host$ docker exec -t -i bisque /bin/bash Develop in Docker (TODO !!) Use the dev docker container xterm -e \\ docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ cbiucsb/bisque05:dev Bash into the docker container and develop docker exec -t -i bisque-dev /bin/bash # Source and develop docker-container$ source /usr/lib/bisque/bin/activate $ apt-get update $ apt-get install python-dev vim $ ln -s /usr/include/python2.7/ /usr/lib/bisque/local/include/python2.7 Test/Build a Module (TODO !! python-dev not setup) # Setup/Test Dream3D Module $ cd /source/modules/Dream3D $ python setup.py","title":"Docker"},{"location":"guides/bisque_docker/#bisque-docker-environment-setup-instructions","text":"","title":"Bisque Docker Environment Setup Instructions"},{"location":"guides/bisque_docker/#dockerproject-source","text":"Docker Hub Image Bitbucket CBIucsb/bisque-stable (TODO: Test with Github version!!) Bique Bioimage Google Groups","title":"Docker/Project Source"},{"location":"guides/bisque_docker/#pre-requisite","text":"Install docker (light weight VM) on you laptop: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Instructions on installing bisque using docker https://bitbucket.org/CBIucsb/bisque/src/default/README.md","title":"Pre-requisite"},{"location":"guides/bisque_docker/#installation","text":"Use the installer script to setup bisque docker image wget https://bitbucket.org/CBIucsb/bisque-stable/downloads/build_bisque_docker_modules.sh sh build_bisque_docker_modules.sh You can modify this script to create a modules folder and mount/load modules from the host machine into docker","title":"Installation"},{"location":"guides/bisque_docker/#run-environment","text":"Start Docker and mount directories for run # Start xterm -e \\ docker run --name bisque --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ cbiucsb/bisque05:stable Stop Docker docker stop $(docker ps -a -q --filter ancestor=cbiucsb/bisque05:stable --format= {{.ID}} ) Advanced environment for working with database URI as, sqlite:///data/bisque.db postgresql://rahul:rahul@localhost/bqmurks58 or postgresql://dbhost:5432/bisque docker run --name bisque --rm -p 8080:8080 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ -e BISQUE_DBURL=postgresql://rahul:rahul@localhost/bqmurks58 \\ cbiucsb/bisque05:stable This will start the bisque docker/server Client Service http://0.0.0.0:8080 Engine Service http://0.0.0.0:8080/engine_service Module Service http://0.0.0.0:8080/module_service Features http://0.0.0.0:8080/features Use \"admin:admin\" to authenticate into this local environment To manipulate and check logs, connect to docker bash bisque-host$ docker exec -t -i bisque /bin/bash","title":"Run Environment"},{"location":"guides/bisque_docker/#develop-in-docker-todo","text":"Use the dev docker container xterm -e \\ docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ cbiucsb/bisque05:dev Bash into the docker container and develop docker exec -t -i bisque-dev /bin/bash # Source and develop docker-container$ source /usr/lib/bisque/bin/activate $ apt-get update $ apt-get install python-dev vim $ ln -s /usr/include/python2.7/ /usr/lib/bisque/local/include/python2.7 Test/Build a Module (TODO !! python-dev not setup) # Setup/Test Dream3D Module $ cd /source/modules/Dream3D $ python setup.py","title":"Develop in Docker (TODO !!)"},{"location":"guides/bisque_module/","text":"Rancher: http://saw.ece.ucsb.edu:8080 Container organization and plan for rancher setup Main Containers Instance Name Host or IP Image Name Remarks condor-nodes IP ADDR biodev.ece.ucsb.edu:5000/condor One master and 4 worker nodes elasticsearch 2 IP ADDR rancher/elasticsearch-conf:v0.5.0 One container each elasticsearch-[clients, datanodes masters] elasticsearch base/data IP ADDR elasticsearch:2.4.3-alpine Two containers(base data volume) for each elasticsearch-[clients, datanodes masters] elasticsearch kopf IP ADDR rancher/kopf:v0.4.0 One container healthcheck IP ADDR rancher/healthcheck:v0.3.6 One on each instance ipsec cni-driver, connectivity-check, router IP ADDR rancher/net:v0.13.11 One on each instance for executing start-cni-driver, connectivity-check, and start-ipsec ipsec-ipsec IP ADDR rancher/net:holder One on each instance network-services-meta IP ADDR rancher/metadata:v0.10.2 One on each instance start.sh,rancher-metadata,-reload-interval-limit=1000,-subscribe network-services-meta-dns IP ADDR rancher/dns:v0.17.3 One on each instance rancher-dns,--listen,169.254.169.250:53,--metadata-server=localhost network-services-manager IP ADDR rancher/network-manager:v0.7.20 One on each instance plugin-manager,--disable-cni-setup,--metadata-address,169.254.169.250 nfs-driver IP ADDR rancher/storage-nfs:v0.9.1 One on each instance rancher-agent IP ADDR rancher/agent:v1.2.10 One on each instance scheduler IP ADDR rancher/scheduler:v0.8.3 One instance scheduler,--metadata-address,169.254.169.250 janitor-cleanup IP ADDR meltwater/docker-cleanup:1.8.0 One on each instance kibana IP ADDR kibana:5.3.0 One instance kibana rancher IP ADDR rancher/lb-service-haproxy:v0.7.9, rancher/nginx:v1.9.4-3, rancher/nginx-conf:v0.2.0 One instance production logsvc IP ADDR biodev.ece.ucsb.edu:5000/logger_ucsb:dev One instance LetsEncrypt IP ADDR janeczku/rancher-letsencrypt:v0.4.0 One instance jenkins IP ADDR jenkins/jenkins:lts One instance jenkins plugins IP ADDR biodev.ece.ucsb.edu:5000/jenkins-cbi-plugins:v0.1.1 One instance Modules Containers Instance Name Host or IP Image Name Remarks Dream3D IP ADDR biodev.ece.ucsb.edu:5000/bisque_dream3d Dream3D Module Predict Strength IP ADDR biodev.ece.ucsb.edu:5000/predict_strength Predict Strength Module Connoisseur IP ADDR biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Predict Strength Module Reference : https://github.com/pndaly/BisQue_Platform_Guide Sample Deep Learning Module: Planteome Deep Segment Analysis Overall Source code stack","title":"Bisque module"},{"location":"guides/bisque_module/#rancher-httpsaweceucsbedu8080","text":"Container organization and plan for rancher setup","title":"Rancher: http://saw.ece.ucsb.edu:8080"},{"location":"guides/bisque_module/#main-containers","text":"Instance Name Host or IP Image Name Remarks condor-nodes IP ADDR biodev.ece.ucsb.edu:5000/condor One master and 4 worker nodes elasticsearch 2 IP ADDR rancher/elasticsearch-conf:v0.5.0 One container each elasticsearch-[clients, datanodes masters] elasticsearch base/data IP ADDR elasticsearch:2.4.3-alpine Two containers(base data volume) for each elasticsearch-[clients, datanodes masters] elasticsearch kopf IP ADDR rancher/kopf:v0.4.0 One container healthcheck IP ADDR rancher/healthcheck:v0.3.6 One on each instance ipsec cni-driver, connectivity-check, router IP ADDR rancher/net:v0.13.11 One on each instance for executing start-cni-driver, connectivity-check, and start-ipsec ipsec-ipsec IP ADDR rancher/net:holder One on each instance network-services-meta IP ADDR rancher/metadata:v0.10.2 One on each instance start.sh,rancher-metadata,-reload-interval-limit=1000,-subscribe network-services-meta-dns IP ADDR rancher/dns:v0.17.3 One on each instance rancher-dns,--listen,169.254.169.250:53,--metadata-server=localhost network-services-manager IP ADDR rancher/network-manager:v0.7.20 One on each instance plugin-manager,--disable-cni-setup,--metadata-address,169.254.169.250 nfs-driver IP ADDR rancher/storage-nfs:v0.9.1 One on each instance rancher-agent IP ADDR rancher/agent:v1.2.10 One on each instance scheduler IP ADDR rancher/scheduler:v0.8.3 One instance scheduler,--metadata-address,169.254.169.250 janitor-cleanup IP ADDR meltwater/docker-cleanup:1.8.0 One on each instance kibana IP ADDR kibana:5.3.0 One instance kibana rancher IP ADDR rancher/lb-service-haproxy:v0.7.9, rancher/nginx:v1.9.4-3, rancher/nginx-conf:v0.2.0 One instance production logsvc IP ADDR biodev.ece.ucsb.edu:5000/logger_ucsb:dev One instance LetsEncrypt IP ADDR janeczku/rancher-letsencrypt:v0.4.0 One instance jenkins IP ADDR jenkins/jenkins:lts One instance jenkins plugins IP ADDR biodev.ece.ucsb.edu:5000/jenkins-cbi-plugins:v0.1.1 One instance","title":"Main Containers"},{"location":"guides/bisque_module/#modules-containers","text":"Instance Name Host or IP Image Name Remarks Dream3D IP ADDR biodev.ece.ucsb.edu:5000/bisque_dream3d Dream3D Module Predict Strength IP ADDR biodev.ece.ucsb.edu:5000/predict_strength Predict Strength Module Connoisseur IP ADDR biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Predict Strength Module","title":"Modules Containers"},{"location":"guides/bisque_module/#reference-httpsgithubcompndalybisque_platform_guide","text":"Sample Deep Learning Module: Planteome Deep Segment Analysis","title":"Reference: https://github.com/pndaly/BisQue_Platform_Guide"},{"location":"guides/bisque_module/#overall-source-code-stack","text":"","title":"Overall Source code stack"},{"location":"guides/bisque_module_planteome/","text":"PLANTEOME DEEP SEGMENT ANALYSIS MODULE This module segments a marked object (creating a graphical object) within an input image. Then the module will classify either the entire original image or the segment created in the first step. This uses PyTorch in order to do this deep segmentation. Pre-requisite: link Working bisque environment at http://loup.ece.ucsb.edu:8088/ Docker should be enabled and setup on this environment The bisque deployment server should have access to good CPU RAM since Torch Scikit-image is used Access to module folder for deploying this module (Say at ~/bisque/module) Add/Deploy the Planteome module Identify the bisque module folder at path ~/bisque/module Login to http://loup.ece.ucsb.edu:8088/ with credentials admin:admin Download the Planteome module to the bisque module folder Register the module to Bisque by opening the Module Manager Provide the engine URL(http://loup.ece.ucsb.edu:8088/engine_service) in the Engine service section and click load. This will list the modules by querying the engine service. Select the module named \"Planteome\", drag this module to the left pane and drop it to register. Build/Configure the module Edit the runtime-module.cfg with relevant configuration Changed the docker.hub configuration to biodev.ece.ucsb.edu:5000 View/Edit the Dockerfile for the relevant packages Edited versions for numpy\\==1.16.1 scikit-image\\==0.14.2 Source to your bisque python environment \"workon bqenv\" for installation pip install -r requirements.txt python setup.py This will install all the dependency in the modules requirements.txt file, build modules code, and create a docker image for running it. Based on your \"docker.hub\" configuration the setup will push the docker image to registry as [biodev.ece.ucsb.edu:5000/bisque_uplanteome] Steps to run the algorithm from Bisque Client Select an image to be analyzed. The foreground/background annotation tooltip can be found on the top-right of the module image viewer. Mark the part of the image to be segmented with foreground line(s) annotation(s). Mark the image with background annotations around the object to be segmented. Select which deep classifier to use, whether to segment the image, the segmentation quality, and whether to classify the entire image or the segmented object instead. Press the 'RUN' button. Analysis may take some time depending on the image and segmentation quality. Results are given in visual and table formats, depending on whether the segmentation and classification functionalities respectively were enabled in the options. Isolated/Development test setup Make sure you have annotated the image and have an a mex identifier availablefor manual test/run. This can be done by, - Opening the module and select image - Annotate the image as per the directions above - Configure and run the module once - Make note of the mex URL for this run by looking at the docker_run.log in the staging folder. This will be used for replaying the test run from the modules folder. Additional module execution information - When we annotate the image and click RUN on the module user interface - The Planteome module is created from biodev.ece.ucsb.edu:5000/bisque_uplanteome:latest docker create biodev.ece.ucsb.edu:5000/bisque_uplanteome \\ python PlanteomeDeepSegment.py \\ http://loup.ece.ucsb.edu:8088/module_service/mex/00-NKpU4CWiHfupgckuXBNFDd \\ admin:00-NKpU4CWiHfupgckuXBNFDd Simple True 3 False - This module is run with the hash id of the docker create Setup/Run the docker container for test Build the docker container docker build --no-cache -t bisque_uplanteome -f Dockerfile . docker build -t biodev.ece.ucsb.edu:5000/bisque_uplanteome . Run the container and bash into it docker run -it --net=host biodev.ece.ucsb.edu:5000/bisque_uplanteome bash test run the code on mex identifier python PlanteomeDeepSegment.py http://loup.ece.ucsb.edu:8088/module_service/mex/00-NKpU4CWiHfupgckuXBNFDd admin:00-NKpU4CWiHfupgckuXBNFDd Simple True 3 False Version: 0.3 Author(s): Dimitrios Trigkakis, Justin Preece","title":"Bisque module planteome"},{"location":"guides/bisque_module_planteome/#planteome-deep-segment-analysis-module","text":"This module segments a marked object (creating a graphical object) within an input image. Then the module will classify either the entire original image or the segment created in the first step. This uses PyTorch in order to do this deep segmentation.","title":"PLANTEOME DEEP SEGMENT ANALYSIS MODULE"},{"location":"guides/bisque_module_planteome/#pre-requisite-link","text":"Working bisque environment at http://loup.ece.ucsb.edu:8088/ Docker should be enabled and setup on this environment The bisque deployment server should have access to good CPU RAM since Torch Scikit-image is used Access to module folder for deploying this module (Say at ~/bisque/module)","title":"Pre-requisite: link"},{"location":"guides/bisque_module_planteome/#adddeploy-the-planteome-module","text":"Identify the bisque module folder at path ~/bisque/module Login to http://loup.ece.ucsb.edu:8088/ with credentials admin:admin Download the Planteome module to the bisque module folder Register the module to Bisque by opening the Module Manager Provide the engine URL(http://loup.ece.ucsb.edu:8088/engine_service) in the Engine service section and click load. This will list the modules by querying the engine service. Select the module named \"Planteome\", drag this module to the left pane and drop it to register.","title":"Add/Deploy the Planteome module"},{"location":"guides/bisque_module_planteome/#buildconfigure-the-module","text":"Edit the runtime-module.cfg with relevant configuration Changed the docker.hub configuration to biodev.ece.ucsb.edu:5000 View/Edit the Dockerfile for the relevant packages Edited versions for numpy\\==1.16.1 scikit-image\\==0.14.2 Source to your bisque python environment \"workon bqenv\" for installation pip install -r requirements.txt python setup.py This will install all the dependency in the modules requirements.txt file, build modules code, and create a docker image for running it. Based on your \"docker.hub\" configuration the setup will push the docker image to registry as [biodev.ece.ucsb.edu:5000/bisque_uplanteome]","title":"Build/Configure the module"},{"location":"guides/bisque_module_planteome/#steps-to-run-the-algorithm-from-bisque-client","text":"Select an image to be analyzed. The foreground/background annotation tooltip can be found on the top-right of the module image viewer. Mark the part of the image to be segmented with foreground line(s) annotation(s). Mark the image with background annotations around the object to be segmented. Select which deep classifier to use, whether to segment the image, the segmentation quality, and whether to classify the entire image or the segmented object instead. Press the 'RUN' button. Analysis may take some time depending on the image and segmentation quality. Results are given in visual and table formats, depending on whether the segmentation and classification functionalities respectively were enabled in the options.","title":"Steps to run the algorithm from Bisque Client"},{"location":"guides/bisque_module_planteome/#isolateddevelopment-test-setup","text":"Make sure you have annotated the image and have an a mex identifier availablefor manual test/run. This can be done by, - Opening the module and select image - Annotate the image as per the directions above - Configure and run the module once - Make note of the mex URL for this run by looking at the docker_run.log in the staging folder. This will be used for replaying the test run from the modules folder. Additional module execution information - When we annotate the image and click RUN on the module user interface - The Planteome module is created from biodev.ece.ucsb.edu:5000/bisque_uplanteome:latest docker create biodev.ece.ucsb.edu:5000/bisque_uplanteome \\ python PlanteomeDeepSegment.py \\ http://loup.ece.ucsb.edu:8088/module_service/mex/00-NKpU4CWiHfupgckuXBNFDd \\ admin:00-NKpU4CWiHfupgckuXBNFDd Simple True 3 False - This module is run with the hash id of the docker create","title":"Isolated/Development test setup"},{"location":"guides/bisque_module_planteome/#setuprun-the-docker-container-for-test","text":"Build the docker container docker build --no-cache -t bisque_uplanteome -f Dockerfile . docker build -t biodev.ece.ucsb.edu:5000/bisque_uplanteome . Run the container and bash into it docker run -it --net=host biodev.ece.ucsb.edu:5000/bisque_uplanteome bash test run the code on mex identifier python PlanteomeDeepSegment.py http://loup.ece.ucsb.edu:8088/module_service/mex/00-NKpU4CWiHfupgckuXBNFDd admin:00-NKpU4CWiHfupgckuXBNFDd Simple True 3 False Version: 0.3 Author(s): Dimitrios Trigkakis, Justin Preece","title":"Setup/Run the docker container for test"},{"location":"guides/connoisseur/","text":"Setup Connoisseur Information Connoisseur is the DL module for distributed compute. Condor is running on each node TODO Figure out the configurations !!! 1. OpenCV 3.4.1 Installation Guide: Link 1 # Verify your installation pkg-config --libs opencv 2. CUDA 8.0 and cuDNN v7.1.4 Installation Guide: Link1 , Link2 Download cuda_8.0.61 $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Sun_Sep__4_22:14:01_CDT_2016 Cuda compilation tools, release 8.0, V8.0.44 Download libcudnn7-dev_7.1.4 cuDNN v7.1.4 Runtime Library for Ubuntu16.04 (Deb) cuDNN v7.1.4 Developer Library for Ubuntu16.04 (Deb) 3. CAFFE Setup caffe 1.0 using the official guide Installation Overview mkdir build cd build cmake .. make all make install make runtest $ caffe --version caffe version 1.0.0 Make sure you configure the path env in ~/.bashrc export PATH=$PATH:/usr/local/cuda/bin:~/cv/caffe/build/tools export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/cv/caffe/.build_release/lib export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/:~/cv/caffe/.build_release/lib Also add the following in ~/[python_env_folder]/bin/activate script after the PATH variable setup export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/ Test it all out to ensure caffe and opencv are functional (bqenv) rahul@bqdev:~/ws/bisque$ python Python 2.7.12 (default, Dec 4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2 import cv2 import caffe Also, ensure Caffe runs on the system $ caffe --version caffe version 1.0.0 4. Install Connoisseur Pre-requisite: (caffe and opencv may be the cause) source ~/bisque/bqenv/bin/activate pip install lmdb pyvoro Installation In Bisque top directory unzip the contents of module/connoisseur.zip Activate Env source ~/bisque/bqenv/bin/activate Ensure that caffe and opencv is importable in python Install connoisseur cd ~/bisque/connoisseur pip install -e connoisseur Move the module to Bisque modules mv ~/bisque/connoisseur/modules/Connoisseur/ ~/bisque/modules/ Deactivate Env source ~/bisque/bqenv/bin/deactivate Re/Start the Bisque server $ paver setup bq-admin setup $ bq-admin server start $ bq-admin server stop Make sure the service is available at http://0.0.0.0:8080/services Verify the log in module manager has no errors Thereafter look for the loaded services at the URL http://0.0.0.0:8080/services Look for the service named connoisseur in this list 5. Load Connoisseur Now lets add the connoisseur user group for loading the module Create a connoisseur group Add this group to the user/admin Restart the services $ bq-admin server start bq-admin server stop Load the module as user/admin Open the module from navbar using Analyze- Classification- Connoisseur TODO: Configuration, load model and run a test on Connoisseur","title":"Connoisseur"},{"location":"guides/connoisseur/#setup-connoisseur","text":"","title":"Setup Connoisseur"},{"location":"guides/connoisseur/#information","text":"Connoisseur is the DL module for distributed compute. Condor is running on each node","title":"Information"},{"location":"guides/connoisseur/#todo","text":"Figure out the configurations !!!","title":"TODO"},{"location":"guides/connoisseur/#1-opencv-341","text":"Installation Guide: Link 1 # Verify your installation pkg-config --libs opencv","title":"1. OpenCV 3.4.1"},{"location":"guides/connoisseur/#2-cuda-80-and-cudnn-v714","text":"Installation Guide: Link1 , Link2 Download cuda_8.0.61 $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Sun_Sep__4_22:14:01_CDT_2016 Cuda compilation tools, release 8.0, V8.0.44 Download libcudnn7-dev_7.1.4 cuDNN v7.1.4 Runtime Library for Ubuntu16.04 (Deb) cuDNN v7.1.4 Developer Library for Ubuntu16.04 (Deb)","title":"2. CUDA 8.0 and cuDNN v7.1.4"},{"location":"guides/connoisseur/#3-caffe","text":"Setup caffe 1.0 using the official guide Installation Overview mkdir build cd build cmake .. make all make install make runtest $ caffe --version caffe version 1.0.0 Make sure you configure the path env in ~/.bashrc export PATH=$PATH:/usr/local/cuda/bin:~/cv/caffe/build/tools export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/cv/caffe/.build_release/lib export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/:~/cv/caffe/.build_release/lib Also add the following in ~/[python_env_folder]/bin/activate script after the PATH variable setup export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/ Test it all out to ensure caffe and opencv are functional (bqenv) rahul@bqdev:~/ws/bisque$ python Python 2.7.12 (default, Dec 4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2 import cv2 import caffe Also, ensure Caffe runs on the system $ caffe --version caffe version 1.0.0","title":"3. CAFFE"},{"location":"guides/connoisseur/#4-install-connoisseur","text":"Pre-requisite: (caffe and opencv may be the cause) source ~/bisque/bqenv/bin/activate pip install lmdb pyvoro Installation In Bisque top directory unzip the contents of module/connoisseur.zip Activate Env source ~/bisque/bqenv/bin/activate Ensure that caffe and opencv is importable in python Install connoisseur cd ~/bisque/connoisseur pip install -e connoisseur Move the module to Bisque modules mv ~/bisque/connoisseur/modules/Connoisseur/ ~/bisque/modules/ Deactivate Env source ~/bisque/bqenv/bin/deactivate Re/Start the Bisque server $ paver setup bq-admin setup $ bq-admin server start $ bq-admin server stop Make sure the service is available at http://0.0.0.0:8080/services Verify the log in module manager has no errors Thereafter look for the loaded services at the URL http://0.0.0.0:8080/services Look for the service named connoisseur in this list","title":"4. Install Connoisseur"},{"location":"guides/connoisseur/#5-load-connoisseur","text":"Now lets add the connoisseur user group for loading the module Create a connoisseur group Add this group to the user/admin Restart the services $ bq-admin server start bq-admin server stop Load the module as user/admin Open the module from navbar using Analyze- Classification- Connoisseur TODO: Configuration, load model and run a test on Connoisseur","title":"5. Load Connoisseur"},{"location":"guides/errors/","text":"Errors Solution to errors encountered during the setup 1. Port in use error If you see an error like below it may be due to port not being free File /opt/nagare-home/lib/python2.7/site-packages/Paste-1.7.5.1-py2.7.egg/paste/httpserver.py , line 1105, in server_close self.thread_pool.shutdown(60) AttributeError: WSGIThreadPoolServer object has no attribute thread_pool Solution is to kill the process holding the port sudo kill $(sudo lsof -t -i:8080) 2. Import Error on a library Issue: ImportError: libhdf5_serial.so.100: cannot open shared object file: No such file or directory Solution to fix the installed packages pip uninstall h5py pip uninstall tables pip install --no-cache-dir tables","title":"Errors"},{"location":"guides/errors/#errors","text":"Solution to errors encountered during the setup","title":"Errors"},{"location":"guides/errors/#1-port-in-use-error","text":"If you see an error like below it may be due to port not being free File /opt/nagare-home/lib/python2.7/site-packages/Paste-1.7.5.1-py2.7.egg/paste/httpserver.py , line 1105, in server_close self.thread_pool.shutdown(60) AttributeError: WSGIThreadPoolServer object has no attribute thread_pool Solution is to kill the process holding the port sudo kill $(sudo lsof -t -i:8080)","title":"1. Port in use error"},{"location":"guides/errors/#2-import-error-on-a-library","text":"Issue: ImportError: libhdf5_serial.so.100: cannot open shared object file: No such file or directory Solution to fix the installed packages pip uninstall h5py pip uninstall tables pip install --no-cache-dir tables","title":"2. Import Error on a library"},{"location":"guides/hggit/","text":"Version Control Instructions (HG-GIT) Maintain the HG- GIT code push using http://hg-git.github.io Current Release Version: 0.5.10 Instructions :https://til.secretgeek.net/mercurial/convert_hg_to_git.html Source HG :https://biodev.ece.ucsb.edu/hg/bisque-stable Target Git :https://github.com/UCSB-VRL/bisque.git hg- git push Install $ sudo easy_install hg-git $ hg clone --insecure https://biodev.ece.ucsb.edu/hg/bisque-stable Make sure the following is in your .hgrc file: vim ~/.hgrc [git] intree = 1 [extensions] hgext.bookmarks = hggit = [ui] ignore = ~/.hgignore Add a master branch name to the repo and export cd bisque-stable hg bookmark -r default master # so a ref gets created hg gexport --debug # exporting hg commits/objects to git Initialize and prepare the target git repo git init git remote add origin https://github.com/UCSB-VRL/bisque.git git remote -v # verify the remote repo track the large files using LFS https://git-lfs.github.com/ curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash git lfs install git lfs track \"*.ipch\" git lfs track \"*.sdf\" git lfs track \"*.sav\" git lfs track \"*.model\" git add .gitattributes Check the commits using 'git status' and prepare for github git pull origin master git add --all git commit -m \"hg-git from https://biodev.ece.ucsb.edu/hg/bisque-stable/\" git push origin master Current Advice: Stare and compare that the new git repo matches the old hg repo, until you are completely satisfied.","title":"Hggit"},{"location":"guides/hggit/#version-control-instructions-hg-git","text":"Maintain the HG- GIT code push using http://hg-git.github.io Current Release Version: 0.5.10 Instructions :https://til.secretgeek.net/mercurial/convert_hg_to_git.html Source HG :https://biodev.ece.ucsb.edu/hg/bisque-stable Target Git :https://github.com/UCSB-VRL/bisque.git hg- git push Install $ sudo easy_install hg-git $ hg clone --insecure https://biodev.ece.ucsb.edu/hg/bisque-stable Make sure the following is in your .hgrc file: vim ~/.hgrc [git] intree = 1 [extensions] hgext.bookmarks = hggit = [ui] ignore = ~/.hgignore Add a master branch name to the repo and export cd bisque-stable hg bookmark -r default master # so a ref gets created hg gexport --debug # exporting hg commits/objects to git Initialize and prepare the target git repo git init git remote add origin https://github.com/UCSB-VRL/bisque.git git remote -v # verify the remote repo track the large files using LFS https://git-lfs.github.com/ curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash git lfs install git lfs track \"*.ipch\" git lfs track \"*.sdf\" git lfs track \"*.sav\" git lfs track \"*.model\" git add .gitattributes Check the commits using 'git status' and prepare for github git pull origin master git add --all git commit -m \"hg-git from https://biodev.ece.ucsb.edu/hg/bisque-stable/\" git push origin master Current Advice: Stare and compare that the new git repo matches the old hg repo, until you are completely satisfied.","title":"Version Control Instructions (HG-GIT)"},{"location":"guides/jupyter_notebooks/","text":"Reference Video demonstration at Youtube Cyverse Webinar Reference Updated Bisque notebooks on CyVerse github account Data Sample data from AWS S3 Download public/shared data from Cyverse Bisque Download public/shared data from UCSB Biodev Bisque Notebooks Use the three samples from CBIucsb Bitbucket repository as described in the video demonstration 1_upload_images_group.ipynb 2_upload_image_graphical_annotations.ipynb 3_modify_annotations.ipynb General Instructions Install bisque on local and the relevant packages in the virtual env Modify the URL and credentials in the notebooks Download the tif/lsm data from the sample data share above and drop it in relevant folders Go through each step in the notebook to understand and execute them","title":"Jupyter"},{"location":"guides/jupyter_notebooks/#reference","text":"Video demonstration at Youtube Cyverse Webinar Reference Updated Bisque notebooks on CyVerse github account","title":"Reference"},{"location":"guides/jupyter_notebooks/#data","text":"Sample data from AWS S3 Download public/shared data from Cyverse Bisque Download public/shared data from UCSB Biodev Bisque","title":"Data"},{"location":"guides/jupyter_notebooks/#notebooks","text":"Use the three samples from CBIucsb Bitbucket repository as described in the video demonstration 1_upload_images_group.ipynb 2_upload_image_graphical_annotations.ipynb 3_modify_annotations.ipynb","title":"Notebooks"},{"location":"guides/jupyter_notebooks/#general-instructions","text":"Install bisque on local and the relevant packages in the virtual env Modify the URL and credentials in the notebooks Download the tif/lsm data from the sample data share above and drop it in relevant folders Go through each step in the notebook to understand and execute them","title":"General Instructions"},{"location":"guides/rancher2_bisque/","text":"Rancher 2.0 Setup (with Kubernetes engine) Nomenclature (Kubernetes like) Name Kubernetes concepts Container Pod (Simplest Kubernetes object representing a set of containers on the cluster) Services Workload (Units of work that are running on the cluster, these can be pods or deployments) Load Balancer Ingress Stack Namespace (A virtual cluster) Environment Project (Administration)/Cluster (Compute machines that run containerized applications) Host Node (Physical or virtual machines making up the cluster) Catalog Helm A. Cluster Description Deployment Google Slides B. Lets Encrypt on Ubuntu 16.04 Bind the hostname to the IP address by creating an A record in DNS Letsencrypt ACME challenge at TCP/80 on host Open up firewall for this \"sudo ufw allow 80 sudo ufw allow 80 443\" Verify the port 80 availability sudo netstat -peanut | grep :80 Setup certificates at /etc/letsencrypt sudo certbot certonly --standalone --dry-run \\ --cert-name loup.ece.ucsb.edu -d loup.ece.ucsb.edu C. Master Rancher 2.0 Install/Startup Rancher: https://rancher.com/docs/rancher/v2.x/en/installation/single-node/ - Rancher etcd data persisted at /var/lib/rancher - Since port 80 is occupied by rancher/rancher, a rancher/rancher-agent cannot be run on this node. docker run -d --restart=unless-stopped \\ -p 8080:80 -p 8443:443 \\ -v /var/log/rancher/auditlog:/var/log/auditlog \\ -v /host/rancher:/var/lib/rancher \\ -e AUDIT_LEVEL=1 \\ rancher/rancher:stable You will have rancher accessible at https://loup.ece.ucsb.edu:8443 if everything goes fine Create the YAML configuration based on docker-compose.yaml (In case of migration) Create an access key for Rancher CLI operations (Doesnt work on self-signed certs) endpoint : https://loup.ece.ucsb.edu:8443/v3 access-key: token-xt47w secret-key: bearer-tok: token-xt47w: Migration CLI Download the docker-compose and rancher-compose.yml files from existing rancher user interface for migration https://rancher.com/docs/rancher/v2.x/en/v1.6-migration Migration using CLI tools migration-tools export --url RANCHER_URL --access-key RANCHER_ACCESS_KEY \\ --secret-key RANCHER_SECRET_KEY --export-dir EXPORT_DIR ./migration-tools parse --docker-file compose/docker-compose.yml \\ --rancher-file compose/rancher-compose.yml D. Setup Cluster RKE/custom-nodes Port requirements Open up ports based on the CNI provider requirements - Use Canal as the provider in this case # API/UI Clients sudo ufw allow 22,80,443/tcp # Etcd Plane Nodes sudo ufw allow 2379,2380,9099,6443/tcp sudo ufw allow 8472/udp # Control Plane Nodes sudo ufw allow 2379,2380,10250,6443,9099,10254/tcp sudo ufw allow 8472/udp # Worker Plane Nodes sudo ufw allow 6443,9099,10254/tcp sudo ufw allow 8472/udp # Workload sudo ufw allow 30000:32767/tcp sudo ufw allow 30000:32767/udp # Others sudo ufw allow 2376/tcp Create cluster Create a cluster in rancher-ui named \"bq-cluster\" Select \"custom\" local/remote nodes option to create this cluster Run the below command for running the rancher-agent/workers sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 --server https://loup.ece.ucsb.edu:8443 \\ --token 7z2ncgjj4482m48fpsj7xjmc8lc9n6bsxh7qcjrsr6rcxrzhzl6prz \\ --ca-checksum d522680b13d7aabe4dc57bb2776e28759852c336d0cf0e0f9fed5d3fb7b495e8 \\ --etcd --controlplane --worker The final \"bq-cluster\" state can be visualized upon creation A docker ps on a node of the cluster (as created above) would look like below screenshot Add more nodes as worker, by running above command on those nodes so that they register with the rancher2 and become part of this cluster. The nodes on a cluster can be visualized in rancher cluster - nodes menu. Create a namespace bqdev within this cluster Bisque Development environment where workloads are deployed and tested E. Setup Volume Mount the host directory for volume using NFS and setup the nfs client access for the cluster https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04 Setup folders # Create the path on host system sudo mkdir /opt/bisque/ -p \\ sudo mkdir /opt/bisque/data -p \\ sudo mkdir /opt/bisque/local/workdir -p # Allow other users to edit this sudo chown -R nobody:nogroup /opt/bisque/ Open up the ports used by NFS # Access from specific machines sudo ufw allow from 192.168.1.129 to any port nfs sudo ufw allow from 192.168.1.133 to any port nfs # Specific ports in case above doesnt work sudo ufw allow 32768:65535/tcp sudo ufw allow 32768:65535/udp sudo ufw allow 2049/tcp sudo ufw allow 2049/udp sudo ufw allow 111/tcp sudo ufw allow 111/udp Now add NFS host configuration at /etc/exports /opt/bisque 192.168.1.129(rw,sync,no_root_squash,no_subtree_check) /opt/bisque 192.168.1.133(rw,sync,no_root_squash,no_subtree_check) restart the nfs server on the NFS host machine sudo systemctl restart nfs-kernel-server Mount the NFS folder on the client machine sudo mount 192.168.1.123:/opt/bisque/ /run/bisque/ Verify the mount on a client system using df -h AND https://www.claudiokuenzler.com/blog/786/rancher-2.0-create-persistent-volume-from-nfs-share Create a persistent volume in the cluster Set local path option on the node as /run/bisque We can see all the volumes that are created in the Volumes section of the \"bq-cluster\" workload F. Setup Workload (on the cluster) Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at custom registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc or we can use a publicly deployed image at https://hub.docker.com Test workload configuration Name: ucsb-bisque05-svc Pods: 2 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 8080-tcp-NodePort-Random 27000-tcp-NodePort-Random Alternately, we can use 8080-tcp-ClusterIP(Internal)-Same 27000-tcp-ClusterIP(Internal)-Same Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /tmp/bisque Scaling: No change Command: (Only, in case needed. Not used with the current Image) Entrypoint: /builder/run-bisque.sh Command: bootstrap start Working Dir: /source Console: Interactive TTY (-i -t) Networking: No Change Labels: No change Security: No change Finally we can see the overall state of pods in the workload within the clusters Environment Configuration Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 We should see the overview of workloads deployed as below G. Load Balancing (using L7 Ingress) Add Ingress configuration for load balancing with name \"bq-website\" Configure the target(ucsb-bisque05-svc) pods so that the port 8080 is exposed through the ingress controller Load Balancing section of the workload will showcase the list of ingress controllers along with the mapping H. Monitoring/Debugging Using cluster kubectl shell from the cluster web UI # Fetch namespaces kubectl get pods --all-namespaces kubectl get pods -n bqdev # Fetch logs on a pod/container kubectl logs postgres-564d9f79d5-z2sxl -n bqdev Use Cluster dashboard for all cluster monitoring and configuration I. Uninstall Rancher Stop Rancher Containers # Master: for the rancher server container docker stop $(docker ps -a -q --filter ancestor=rancher/rancher:stable --format= {{.ID}} ) # Workers: for all k8s containers docker stop $(docker ps -f name=k8s* --format= {{.ID}} ) Clean the container, images and volumes docker rm -f $(docker ps -a -f name=k8s* --format= {{.ID}} ) docker rmi -f $(docker images -q rancher/* ) docker volume rm $(docker volume ls -q) Unmount and remove data (/var/lib/kubelet/pods/XXX, /var/lib/kubelet, /var/lib/rancher) # Unmount directories for mount in $(mount | grep tmpfs | grep /var/lib/kubelet | awk { print $3 } ) /var/lib/kubelet /var/lib/rancher; do sudo umount $mount; done # Clean the directories sudo rm -rf /etc/ceph \\ /etc/cni \\ /etc/kubernetes \\ /opt/cni \\ /opt/rke \\ /run/secrets/kubernetes.io \\ /run/calico \\ /run/flannel \\ /var/lib/calico \\ /var/lib/etcd \\ /var/lib/cni \\ /var/lib/kubelet \\ /var/lib/rancher/rke/log \\ /var/log/containers \\ /var/log/pods \\ /var/run/calico # Mounted host directories sudo rm -rf /host/rancher/ sudo rm -rf /var/log/rancher/auditlog Remove the existing network interface ip address show ip link delete interface_name Additional References ==TODO== 1.) Mail server setup https://www.linuxbabe.com/mail-server/ubuntu-16-04-iredmail-server-installation 2.) Migration from Rancher 1.x to 2.x individual workload/containers to rancher-kubernetes using rancher-cli (doesnt work with self-signed certificates) https://rancher.com/blog/2018/2018-08-02-journey-from-cattle-to-k8s/ 3.) Reference on Ingress Controllers Load Balancers add in workloads /k8s-in-rancher/load-balancers-and-ingress Tried using built in Ingress for bq-website.bqdev.192.168.1.129.xip.io but failed to work for engine service If you want to expose the workload container to outside world then use NodePort otherwise work with ClusterIp(Internal Only) port configuration https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/ 4.) PostgreSQL server Setup PostgreSql 10.4 on Rancher workload This is used in the Bisque configuration as environment variable BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres","title":"Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/rancher2_bisque/#rancher-20-setup-with-kubernetes-engine","text":"","title":"Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/rancher2_bisque/#nomenclature-kubernetes-like","text":"Name Kubernetes concepts Container Pod (Simplest Kubernetes object representing a set of containers on the cluster) Services Workload (Units of work that are running on the cluster, these can be pods or deployments) Load Balancer Ingress Stack Namespace (A virtual cluster) Environment Project (Administration)/Cluster (Compute machines that run containerized applications) Host Node (Physical or virtual machines making up the cluster) Catalog Helm","title":"Nomenclature (Kubernetes like)"},{"location":"guides/rancher2_bisque/#a-cluster-description","text":"Deployment Google Slides","title":"A. Cluster Description"},{"location":"guides/rancher2_bisque/#b-lets-encrypt-on-ubuntu-1604","text":"Bind the hostname to the IP address by creating an A record in DNS Letsencrypt ACME challenge at TCP/80 on host Open up firewall for this \"sudo ufw allow 80 sudo ufw allow 80 443\" Verify the port 80 availability sudo netstat -peanut | grep :80 Setup certificates at /etc/letsencrypt sudo certbot certonly --standalone --dry-run \\ --cert-name loup.ece.ucsb.edu -d loup.ece.ucsb.edu","title":"B. Lets Encrypt on Ubuntu 16.04"},{"location":"guides/rancher2_bisque/#c-master-rancher-20","text":"Install/Startup Rancher: https://rancher.com/docs/rancher/v2.x/en/installation/single-node/ - Rancher etcd data persisted at /var/lib/rancher - Since port 80 is occupied by rancher/rancher, a rancher/rancher-agent cannot be run on this node. docker run -d --restart=unless-stopped \\ -p 8080:80 -p 8443:443 \\ -v /var/log/rancher/auditlog:/var/log/auditlog \\ -v /host/rancher:/var/lib/rancher \\ -e AUDIT_LEVEL=1 \\ rancher/rancher:stable You will have rancher accessible at https://loup.ece.ucsb.edu:8443 if everything goes fine Create the YAML configuration based on docker-compose.yaml (In case of migration) Create an access key for Rancher CLI operations (Doesnt work on self-signed certs) endpoint : https://loup.ece.ucsb.edu:8443/v3 access-key: token-xt47w secret-key: bearer-tok: token-xt47w:","title":"C. Master Rancher 2.0"},{"location":"guides/rancher2_bisque/#migration-cli","text":"Download the docker-compose and rancher-compose.yml files from existing rancher user interface for migration https://rancher.com/docs/rancher/v2.x/en/v1.6-migration Migration using CLI tools migration-tools export --url RANCHER_URL --access-key RANCHER_ACCESS_KEY \\ --secret-key RANCHER_SECRET_KEY --export-dir EXPORT_DIR ./migration-tools parse --docker-file compose/docker-compose.yml \\ --rancher-file compose/rancher-compose.yml","title":"Migration CLI"},{"location":"guides/rancher2_bisque/#d-setup-cluster-rkecustom-nodes","text":"","title":"D. Setup Cluster RKE/custom-nodes"},{"location":"guides/rancher2_bisque/#port-requirements","text":"Open up ports based on the CNI provider requirements - Use Canal as the provider in this case # API/UI Clients sudo ufw allow 22,80,443/tcp # Etcd Plane Nodes sudo ufw allow 2379,2380,9099,6443/tcp sudo ufw allow 8472/udp # Control Plane Nodes sudo ufw allow 2379,2380,10250,6443,9099,10254/tcp sudo ufw allow 8472/udp # Worker Plane Nodes sudo ufw allow 6443,9099,10254/tcp sudo ufw allow 8472/udp # Workload sudo ufw allow 30000:32767/tcp sudo ufw allow 30000:32767/udp # Others sudo ufw allow 2376/tcp","title":"Port requirements"},{"location":"guides/rancher2_bisque/#create-cluster","text":"Create a cluster in rancher-ui named \"bq-cluster\" Select \"custom\" local/remote nodes option to create this cluster Run the below command for running the rancher-agent/workers sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 --server https://loup.ece.ucsb.edu:8443 \\ --token 7z2ncgjj4482m48fpsj7xjmc8lc9n6bsxh7qcjrsr6rcxrzhzl6prz \\ --ca-checksum d522680b13d7aabe4dc57bb2776e28759852c336d0cf0e0f9fed5d3fb7b495e8 \\ --etcd --controlplane --worker The final \"bq-cluster\" state can be visualized upon creation A docker ps on a node of the cluster (as created above) would look like below screenshot Add more nodes as worker, by running above command on those nodes so that they register with the rancher2 and become part of this cluster. The nodes on a cluster can be visualized in rancher cluster - nodes menu.","title":"Create cluster"},{"location":"guides/rancher2_bisque/#create-a-namespace-bqdev-within-this-cluster","text":"Bisque Development environment where workloads are deployed and tested","title":"Create a namespace bqdev within this cluster"},{"location":"guides/rancher2_bisque/#e-setup-volume","text":"Mount the host directory for volume using NFS and setup the nfs client access for the cluster https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04 Setup folders # Create the path on host system sudo mkdir /opt/bisque/ -p \\ sudo mkdir /opt/bisque/data -p \\ sudo mkdir /opt/bisque/local/workdir -p # Allow other users to edit this sudo chown -R nobody:nogroup /opt/bisque/ Open up the ports used by NFS # Access from specific machines sudo ufw allow from 192.168.1.129 to any port nfs sudo ufw allow from 192.168.1.133 to any port nfs # Specific ports in case above doesnt work sudo ufw allow 32768:65535/tcp sudo ufw allow 32768:65535/udp sudo ufw allow 2049/tcp sudo ufw allow 2049/udp sudo ufw allow 111/tcp sudo ufw allow 111/udp Now add NFS host configuration at /etc/exports /opt/bisque 192.168.1.129(rw,sync,no_root_squash,no_subtree_check) /opt/bisque 192.168.1.133(rw,sync,no_root_squash,no_subtree_check) restart the nfs server on the NFS host machine sudo systemctl restart nfs-kernel-server Mount the NFS folder on the client machine sudo mount 192.168.1.123:/opt/bisque/ /run/bisque/ Verify the mount on a client system using df -h AND https://www.claudiokuenzler.com/blog/786/rancher-2.0-create-persistent-volume-from-nfs-share Create a persistent volume in the cluster Set local path option on the node as /run/bisque We can see all the volumes that are created in the Volumes section of the \"bq-cluster\" workload","title":"E. Setup Volume"},{"location":"guides/rancher2_bisque/#f-setup-workload-on-the-cluster","text":"Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at custom registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc or we can use a publicly deployed image at https://hub.docker.com","title":"F. Setup Workload (on the cluster)"},{"location":"guides/rancher2_bisque/#test-workload-configuration","text":"Name: ucsb-bisque05-svc Pods: 2 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 8080-tcp-NodePort-Random 27000-tcp-NodePort-Random Alternately, we can use 8080-tcp-ClusterIP(Internal)-Same 27000-tcp-ClusterIP(Internal)-Same Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /tmp/bisque Scaling: No change Command: (Only, in case needed. Not used with the current Image) Entrypoint: /builder/run-bisque.sh Command: bootstrap start Working Dir: /source Console: Interactive TTY (-i -t) Networking: No Change Labels: No change Security: No change Finally we can see the overall state of pods in the workload within the clusters","title":"Test workload configuration"},{"location":"guides/rancher2_bisque/#environment-configuration","text":"Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 We should see the overview of workloads deployed as below","title":"Environment Configuration"},{"location":"guides/rancher2_bisque/#g-load-balancing-using-l7-ingress","text":"Add Ingress configuration for load balancing with name \"bq-website\" Configure the target(ucsb-bisque05-svc) pods so that the port 8080 is exposed through the ingress controller Load Balancing section of the workload will showcase the list of ingress controllers along with the mapping","title":"G. Load Balancing (using L7 Ingress)"},{"location":"guides/rancher2_bisque/#h-monitoringdebugging","text":"Using cluster kubectl shell from the cluster web UI # Fetch namespaces kubectl get pods --all-namespaces kubectl get pods -n bqdev # Fetch logs on a pod/container kubectl logs postgres-564d9f79d5-z2sxl -n bqdev Use Cluster dashboard for all cluster monitoring and configuration","title":"H. Monitoring/Debugging"},{"location":"guides/rancher2_bisque/#i-uninstall-rancher","text":"Stop Rancher Containers # Master: for the rancher server container docker stop $(docker ps -a -q --filter ancestor=rancher/rancher:stable --format= {{.ID}} ) # Workers: for all k8s containers docker stop $(docker ps -f name=k8s* --format= {{.ID}} ) Clean the container, images and volumes docker rm -f $(docker ps -a -f name=k8s* --format= {{.ID}} ) docker rmi -f $(docker images -q rancher/* ) docker volume rm $(docker volume ls -q) Unmount and remove data (/var/lib/kubelet/pods/XXX, /var/lib/kubelet, /var/lib/rancher) # Unmount directories for mount in $(mount | grep tmpfs | grep /var/lib/kubelet | awk { print $3 } ) /var/lib/kubelet /var/lib/rancher; do sudo umount $mount; done # Clean the directories sudo rm -rf /etc/ceph \\ /etc/cni \\ /etc/kubernetes \\ /opt/cni \\ /opt/rke \\ /run/secrets/kubernetes.io \\ /run/calico \\ /run/flannel \\ /var/lib/calico \\ /var/lib/etcd \\ /var/lib/cni \\ /var/lib/kubelet \\ /var/lib/rancher/rke/log \\ /var/log/containers \\ /var/log/pods \\ /var/run/calico # Mounted host directories sudo rm -rf /host/rancher/ sudo rm -rf /var/log/rancher/auditlog Remove the existing network interface ip address show ip link delete interface_name","title":"I. Uninstall Rancher"},{"location":"guides/rancher2_bisque/#additional-references","text":"==TODO==","title":"Additional References"},{"location":"guides/rancher2_bisque/#1-mail-server-setup","text":"https://www.linuxbabe.com/mail-server/ubuntu-16-04-iredmail-server-installation","title":"1.) Mail server setup"},{"location":"guides/rancher2_bisque/#2-migration-from-rancher-1x-to-2x","text":"individual workload/containers to rancher-kubernetes using rancher-cli (doesnt work with self-signed certificates) https://rancher.com/blog/2018/2018-08-02-journey-from-cattle-to-k8s/","title":"2.) Migration from Rancher 1.x to 2.x"},{"location":"guides/rancher2_bisque/#3-reference-on-ingress-controllers","text":"Load Balancers add in workloads /k8s-in-rancher/load-balancers-and-ingress Tried using built in Ingress for bq-website.bqdev.192.168.1.129.xip.io but failed to work for engine service If you want to expose the workload container to outside world then use NodePort otherwise work with ClusterIp(Internal Only) port configuration https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/","title":"3.) Reference on Ingress Controllers"},{"location":"guides/rancher2_bisque/#4-postgresql-server","text":"Setup PostgreSql 10.4 on Rancher workload This is used in the Bisque configuration as environment variable BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres","title":"4.) PostgreSQL server"},{"location":"guides/rancher2_bisque_gpu/","text":"Bisque Workload on Rancher 2.0 Setup (with Kubernetes engine) Pre-requisite Rancher 2.0 with bq-cluster workload instructions Rancher 2.0 with PostgreSql workload instructions General instructions Setup Cluster with /rke-clusters/custom-nodes Note: Assuming we have bq-cluster with postgres and persistent volumes Setup Workload on the bq-cluster Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at custom registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc or another available at vishwakarmarhl/ucsb-bisque05-svc:dev Bisque Service Workload configuration Name: ucsb-bisque05-svc Pods: 2 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 8080-tcp-NodePort-Random 27000-tcp-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /run/bisque Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change Environment Configuration Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 Condor provisioning Condor Master - Image: biodev.ece.ucsb.edu:5000/condor - Ports: 9618, 9886 as HostPort - Environment CONDOR_DAEMONS = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT CONDOR_MANAGER_HOST = master Condor Worker - Same configuration as above - Ports: 9886 NodePort Random Workload (bq-cluster) Dashboard Connoisseur/GPU Workload provisioning Here lets take a look at connoissuer service in Bisque. We will run a Bisque H1 server disabling the engine_service and connoisseur in part (A) and later another service in part (B) with Connoisseur and engine service for the actual compute. Pre-requisites Create a namespace \"connoisseur\" and deploy everything isolated from the existing bisque-svc Create a postgres database for this deployment psql -h 10.42.0.15 -U postgres --password -p 5432 postgres \"create database connoissuer;\" \"grant all privileges on database connoisseur to postgres;\" Create a volume bqcon-vol mounted at 192.168.1.123:/opt/bisque/connoisseur over NFS A.) Bisque Client Service Workload configuration Name: bq-connoisseur-client-svc Pods: 1 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Port Mapping: 80-tcp-NodePort-Random 27000-tcp-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host that has GPU, Say \"arkady\" Health Check: No change Volumes: Persistent Volume claim of bqcon-vol and set the mount point as /run/bisque/ Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change Environment Configuration Bisque client service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/connoisseur BISQUE_SERVERS_H1_SERVICES_DISABLED = engine_service,connoisseur B.) Bisque Engine Service Workload configuration All the same configuration as above but skipping one environment variable Skip the disable variable -- BISQUE_SERVERS_H1_SERVICES_DISABLED = engine_service,connoisseur Verify connoisseur GPU variables NVIDIA_REQUIRE_CUDA=cuda =8.0 NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility CUDA_PKG_VERSION=8-0=8.0.61-1 CAFFE_PKG_VERSION=0.15.13-1ubuntu16.04+cuda8.0 CAFFE_VERSION=0.15 CUDA_VERSION=8.0.61 CONDOR_MANAGER_HOST=master.condor CONDOR_DAEMONS=MASTER,SCHEDD,SHARED_PORT Workload (bq-cluster) with Namespace Connoissuer TODO: - Fix Caffe and CUDA within the image - Probably write your own Docker file for a new CUDA/GPU enabled container bisque@bq-connoisseur-engine-svc-74755f798b-6lbgk:/source$ caffe deveice_query -gpu all E0213 00:36:24.855478 1798 caffe.cpp:77] Available caffe actions: E0213 00:36:24.856573 1798 caffe.cpp:80] device_query E0213 00:36:24.856690 1798 caffe.cpp:80] test E0213 00:36:24.856793 1798 caffe.cpp:80] time E0213 00:36:24.856899 1798 caffe.cpp:80] train F0213 00:36:24.857019 1798 caffe.cpp:82] Unknown action: deveice_query *** Check failure stack trace: *** @ 0x7fafbc7c65cd google::LogMessage::Fail() @ 0x7fafbc7c8433 google::LogMessage::SendToLog() @ 0x7fafbc7c615b google::LogMessage::Flush() @ 0x7fafbc7c8e1e google::LogMessageFatal::~LogMessageFatal() @ 0x40863a main @ 0x7fafbb218830 __libc_start_main @ 0x408dd9 _start @ (nil) (unknown) Aborted (core dumped)","title":"Bisque Workload on Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/rancher2_bisque_gpu/#bisque-workload-on-rancher-20-setup-with-kubernetes-engine","text":"","title":"Bisque Workload on Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/rancher2_bisque_gpu/#pre-requisite","text":"Rancher 2.0 with bq-cluster workload instructions Rancher 2.0 with PostgreSql workload instructions","title":"Pre-requisite"},{"location":"guides/rancher2_bisque_gpu/#general-instructions","text":"Setup Cluster with /rke-clusters/custom-nodes","title":"General instructions"},{"location":"guides/rancher2_bisque_gpu/#note-assuming-we-have-bq-cluster-with-postgres-and-persistent-volumes","text":"","title":"Note: Assuming we have bq-cluster with postgres and persistent volumes"},{"location":"guides/rancher2_bisque_gpu/#setup-workload-on-the-bq-cluster","text":"Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at custom registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc or another available at vishwakarmarhl/ucsb-bisque05-svc:dev","title":"Setup Workload on the bq-cluster"},{"location":"guides/rancher2_bisque_gpu/#bisque-service-workload-configuration","text":"Name: ucsb-bisque05-svc Pods: 2 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 8080-tcp-NodePort-Random 27000-tcp-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /run/bisque Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change","title":"Bisque Service Workload configuration"},{"location":"guides/rancher2_bisque_gpu/#environment-configuration","text":"Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3","title":"Environment Configuration"},{"location":"guides/rancher2_bisque_gpu/#condor-provisioning","text":"Condor Master - Image: biodev.ece.ucsb.edu:5000/condor - Ports: 9618, 9886 as HostPort - Environment CONDOR_DAEMONS = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT CONDOR_MANAGER_HOST = master Condor Worker - Same configuration as above - Ports: 9886 NodePort Random","title":"Condor provisioning"},{"location":"guides/rancher2_bisque_gpu/#workload-bq-cluster-dashboard","text":"","title":"Workload (bq-cluster) Dashboard"},{"location":"guides/rancher2_bisque_gpu/#connoisseurgpu-workload-provisioning","text":"Here lets take a look at connoissuer service in Bisque. We will run a Bisque H1 server disabling the engine_service and connoisseur in part (A) and later another service in part (B) with Connoisseur and engine service for the actual compute.","title":"Connoisseur/GPU Workload provisioning"},{"location":"guides/rancher2_bisque_gpu/#pre-requisites","text":"Create a namespace \"connoisseur\" and deploy everything isolated from the existing bisque-svc Create a postgres database for this deployment psql -h 10.42.0.15 -U postgres --password -p 5432 postgres \"create database connoissuer;\" \"grant all privileges on database connoisseur to postgres;\" Create a volume bqcon-vol mounted at 192.168.1.123:/opt/bisque/connoisseur over NFS","title":"Pre-requisites"},{"location":"guides/rancher2_bisque_gpu/#a-bisque-client-service-workload-configuration","text":"Name: bq-connoisseur-client-svc Pods: 1 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Port Mapping: 80-tcp-NodePort-Random 27000-tcp-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host that has GPU, Say \"arkady\" Health Check: No change Volumes: Persistent Volume claim of bqcon-vol and set the mount point as /run/bisque/ Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change","title":"A.) Bisque Client Service Workload configuration"},{"location":"guides/rancher2_bisque_gpu/#environment-configuration_1","text":"Bisque client service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/connoisseur BISQUE_SERVERS_H1_SERVICES_DISABLED = engine_service,connoisseur","title":"Environment Configuration"},{"location":"guides/rancher2_bisque_gpu/#b-bisque-engine-service-workload-configuration","text":"All the same configuration as above but skipping one environment variable Skip the disable variable -- BISQUE_SERVERS_H1_SERVICES_DISABLED = engine_service,connoisseur Verify connoisseur GPU variables NVIDIA_REQUIRE_CUDA=cuda =8.0 NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility CUDA_PKG_VERSION=8-0=8.0.61-1 CAFFE_PKG_VERSION=0.15.13-1ubuntu16.04+cuda8.0 CAFFE_VERSION=0.15 CUDA_VERSION=8.0.61 CONDOR_MANAGER_HOST=master.condor CONDOR_DAEMONS=MASTER,SCHEDD,SHARED_PORT","title":"B.) Bisque Engine Service Workload configuration"},{"location":"guides/rancher2_bisque_gpu/#workload-bq-cluster-with-namespace-connoissuer","text":"TODO: - Fix Caffe and CUDA within the image - Probably write your own Docker file for a new CUDA/GPU enabled container bisque@bq-connoisseur-engine-svc-74755f798b-6lbgk:/source$ caffe deveice_query -gpu all E0213 00:36:24.855478 1798 caffe.cpp:77] Available caffe actions: E0213 00:36:24.856573 1798 caffe.cpp:80] device_query E0213 00:36:24.856690 1798 caffe.cpp:80] test E0213 00:36:24.856793 1798 caffe.cpp:80] time E0213 00:36:24.856899 1798 caffe.cpp:80] train F0213 00:36:24.857019 1798 caffe.cpp:82] Unknown action: deveice_query *** Check failure stack trace: *** @ 0x7fafbc7c65cd google::LogMessage::Fail() @ 0x7fafbc7c8433 google::LogMessage::SendToLog() @ 0x7fafbc7c615b google::LogMessage::Flush() @ 0x7fafbc7c8e1e google::LogMessageFatal::~LogMessageFatal() @ 0x40863a main @ 0x7fafbb218830 __libc_start_main @ 0x408dd9 _start @ (nil) (unknown) Aborted (core dumped)","title":"Workload (bq-cluster) with Namespace Connoissuer"},{"location":"guides/rancher2_jenkins/","text":"Rancher 2 based Jenkins deployment/management guide Jenkins LTS Container (jenkins/jenkins:lts) - (jenkins/jenkins:2.150.3) Dockerfile on jenkinsci/docker Squid http-proxy for server access: https://help.ubuntu.com/lts/serverguide/squid.html.en Command /usr/share/jenkins/rancher/add-docker.sh Volume Mount Link ~/ws/jenkins_home folder in /var/jenkins_home and change ownership on host sudo ln -s /home/rahul/ws/jenkins_home /var/jenkins_home chown -R 1000:1000 ~/ws/jenkins_home chown 1000 /var/jenkins_home /var/run/docker.sock /usr/bin/docker /var/jenkins_home In case you are using rancher and want to deploy the container on a particular node then make sure that folder is NFS mounted and exists. # bqstage(192.168.1.123) node will be used to deploy jenkins sudo mount 192.168.1.123:/var/jenkins_home /var/jenkins_home Test Run docker run -p 8088:8080 -p 50000:50000 -v /var/jenkins_home:/var/jenkins_home jenkins/jenkins:lts Lets create a deployment in a cluster Name: jenkins-bq Image: jenkins/jenkins:lts Namespace: jenkins Port Mapping: 8080-tcp-NodePort-Random Environment Variables: No Change Node Scheduling: Run all the pods on a particular host with GPU capability Health Check: No change Volumes: Persistent Volume claim and set the mount point as /var/jenkins_home Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change Port 8080","title":"Rancher 2 based Jenkins deployment/management guide"},{"location":"guides/rancher2_jenkins/#rancher-2-based-jenkins-deploymentmanagement-guide","text":"Jenkins LTS Container (jenkins/jenkins:lts) - (jenkins/jenkins:2.150.3) Dockerfile on jenkinsci/docker Squid http-proxy for server access: https://help.ubuntu.com/lts/serverguide/squid.html.en","title":"Rancher 2 based Jenkins deployment/management guide"},{"location":"guides/rancher2_jenkins/#command","text":"/usr/share/jenkins/rancher/add-docker.sh","title":"Command"},{"location":"guides/rancher2_jenkins/#volume-mount","text":"Link ~/ws/jenkins_home folder in /var/jenkins_home and change ownership on host sudo ln -s /home/rahul/ws/jenkins_home /var/jenkins_home chown -R 1000:1000 ~/ws/jenkins_home chown 1000 /var/jenkins_home /var/run/docker.sock /usr/bin/docker /var/jenkins_home In case you are using rancher and want to deploy the container on a particular node then make sure that folder is NFS mounted and exists. # bqstage(192.168.1.123) node will be used to deploy jenkins sudo mount 192.168.1.123:/var/jenkins_home /var/jenkins_home","title":"Volume Mount"},{"location":"guides/rancher2_jenkins/#test-run","text":"docker run -p 8088:8080 -p 50000:50000 -v /var/jenkins_home:/var/jenkins_home jenkins/jenkins:lts","title":"Test Run"},{"location":"guides/rancher2_jenkins/#lets-create-a-deployment-in-a-cluster","text":"Name: jenkins-bq Image: jenkins/jenkins:lts Namespace: jenkins Port Mapping: 8080-tcp-NodePort-Random Environment Variables: No Change Node Scheduling: Run all the pods on a particular host with GPU capability Health Check: No change Volumes: Persistent Volume claim and set the mount point as /var/jenkins_home Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change","title":"Lets create a deployment in a cluster"},{"location":"guides/rancher2_jenkins/#port","text":"8080","title":"Port"},{"location":"guides/rancher2_postgresql/","text":"Setup Postgresql for Bisque Database Persistence Idea is, to not use the default Sqlite DB in a cluster environment The data directory is still being used for images/cache etc, and mounted as NFS across this cluster Reference: https://severalnines.com/blog/using-kubernetes-deploy-postgresql Create a Volume: pgdev-vol Path on the host node: /opt/bisque/pg Setup a Postgresql 10 database Image: postgres:10.4 Environment: POSTGRES_DB=postgres POSTGRES_PASSWORD=postgres POSTGRES_USER=postgres Ports: 5432/TCP - ClusterIP - Same as container port Volume: Mount pgdev-vol created above to mount point \"/var/lib/postgresql/data\" Here is the complete Postgres workload configuration screen shot for reference Make sure to claim this volume in the Postgres workload deploy configuration Test database using the nodes IP Test using psql, depending on what IP the postgres server is available at psql -h 10.42.0.15 -U postgres --password -p 5432 postgres psql -h loup.ece.ucsb.edu -U postgres --password -p 5432 postgres","title":"Rancher2 postgresql"},{"location":"guides/rancher2_postgresql/#setup-postgresql-for-bisque-database-persistence","text":"Idea is, to not use the default Sqlite DB in a cluster environment The data directory is still being used for images/cache etc, and mounted as NFS across this cluster Reference: https://severalnines.com/blog/using-kubernetes-deploy-postgresql","title":"Setup Postgresql for Bisque Database Persistence"},{"location":"guides/rancher2_postgresql/#create-a-volume-pgdev-vol","text":"Path on the host node: /opt/bisque/pg","title":"Create a Volume: pgdev-vol"},{"location":"guides/rancher2_postgresql/#setup-a-postgresql-10-database","text":"Image: postgres:10.4 Environment: POSTGRES_DB=postgres POSTGRES_PASSWORD=postgres POSTGRES_USER=postgres Ports: 5432/TCP - ClusterIP - Same as container port Volume: Mount pgdev-vol created above to mount point \"/var/lib/postgresql/data\" Here is the complete Postgres workload configuration screen shot for reference Make sure to claim this volume in the Postgres workload deploy configuration","title":"Setup a Postgresql 10 database"},{"location":"guides/rancher2_postgresql/#test-database-using-the-nodes-ip","text":"Test using psql, depending on what IP the postgres server is available at psql -h 10.42.0.15 -U postgres --password -p 5432 postgres psql -h loup.ece.ucsb.edu -U postgres --password -p 5432 postgres","title":"Test database using the nodes IP"},{"location":"guides/resources/module/connoisseur/","text":"Connoisseur Connoisseur is a machine learning service for BisQue which manages ML models and their creation/execution. Currently it only supports a deep learning framework Caffe. Issues to solve long running requests must run as celery tasks locking models is needed to protect binaries from parallel operations transaction during request is bad, need to update status of the resource in the middle of a long running request physical location for models store for templates need to capture output from long running caffe binary hierarchical models models loaded separately in each process/thread and thus will use more RAM and will be slower INFO API GET /connoisseur or GET /connoisseur/info CLASSIFICATION API GET /connoisseur/MODEL_ID/classify:IMAGE_ID GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points] # default, classify uniformly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points_random] # classify randomly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:segmentation] # segment image pixels producing image mask with class labels GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:regions] # partition image into regions producing low-res polygons GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient_uniform] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:image] # Parameters for all methods: points: number of output points, approximate for some methods goodness: % (0-100), minimum goodness for an output point border: % (0-100) of the image size, border in pixels will be estimated from image's dimensions parameters for point classification random and uniform GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:points[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf parameters for region partitioning GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:regions[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf parameters for segmentation GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:ids] GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:colors] formats: png MODEL INFO API GET /connoisseur/MODEL_ID/class:3/sample:1 RESTful API GET - request classification, preview or training Responses: 204 Empty results 400 Bad Request 401 Unauthorized 500 Internal Server Error * 501 Not Implemented MODEL DEFINITION See classifier_model.py CLASSIFIER OUTPUTS [table/gobs] gob type: str | gob label: str | gob vertices: [(x,y,z,...)] | goodness: float | accuracy: float | color: str [image/png] bytes... STATUS WORKFLOW new classes loaded ----- # needs to re-load all files from template and repeat all steps: 3-8 classes filtered | samples loaded ----- # needs to re-split the samples and other steps: 5-8 samples split | trained | validated | finished ------ TRAINING WORKFLOW The model resource is created by the JS UI and initially may only contain a user-given name and a dataset to train on. After the model resource is created in the system several operations may be requested following a typical training sequence. Define required model parameters a. Pick a model template to fine-tune or train from scratch b. This should define a \"mode\", currently point classification but later also fully convolutional Find classes for the model, this will lock the model for current classes and will not allow further changes Create training samples, using parameters defined in the model. This can be repeated to augment the sample list a. Absolute minimum number of samples per class b. Minimum number of samples to activate sample augmentation Train on available samples a. train the model b. validate the model and update classes performance Set classes that will be used for classification based on observed performance in the training step Model is ready for recognition Repeat step 3 and further to improve the model MODEL MODIFICATION API GET /connoisseur/create/template:CaffeNet[/dataset:UUID] GET /connoisseur/MODEL_ID/update[/classes:init][/classes:filter][/samples:init][/samples:update][/samples:split] GET /connoisseur/MODEL_ID/train[/method:finetune][/method:snapshot] GET /connoisseur/MODEL_ID/validate","title":"Connoisseur #"},{"location":"guides/resources/module/connoisseur/#connoisseur","text":"Connoisseur is a machine learning service for BisQue which manages ML models and their creation/execution. Currently it only supports a deep learning framework Caffe.","title":"Connoisseur"},{"location":"guides/resources/module/connoisseur/#issues-to-solve","text":"long running requests must run as celery tasks locking models is needed to protect binaries from parallel operations transaction during request is bad, need to update status of the resource in the middle of a long running request physical location for models store for templates need to capture output from long running caffe binary hierarchical models models loaded separately in each process/thread and thus will use more RAM and will be slower","title":"Issues to solve"},{"location":"guides/resources/module/connoisseur/#info-api","text":"GET /connoisseur or GET /connoisseur/info","title":"INFO API"},{"location":"guides/resources/module/connoisseur/#classification-api","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points] # default, classify uniformly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points_random] # classify randomly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:segmentation] # segment image pixels producing image mask with class labels GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:regions] # partition image into regions producing low-res polygons GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient_uniform] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:image] # Parameters for all methods: points: number of output points, approximate for some methods goodness: % (0-100), minimum goodness for an output point border: % (0-100) of the image size, border in pixels will be estimated from image's dimensions","title":"CLASSIFICATION API"},{"location":"guides/resources/module/connoisseur/#parameters-for-point-classification-random-and-uniform","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:points[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf","title":"parameters for point classification random and uniform"},{"location":"guides/resources/module/connoisseur/#parameters-for-region-partitioning","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:regions[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf","title":"parameters for region partitioning"},{"location":"guides/resources/module/connoisseur/#parameters-for-segmentation","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:ids] GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:colors] formats: png","title":"parameters for segmentation"},{"location":"guides/resources/module/connoisseur/#model-info-api","text":"GET /connoisseur/MODEL_ID/class:3/sample:1","title":"MODEL INFO API"},{"location":"guides/resources/module/connoisseur/#restful-api","text":"GET - request classification, preview or training Responses: 204 Empty results 400 Bad Request 401 Unauthorized 500 Internal Server Error * 501 Not Implemented","title":"RESTful API"},{"location":"guides/resources/module/connoisseur/#model-definition","text":"See classifier_model.py","title":"MODEL DEFINITION"},{"location":"guides/resources/module/connoisseur/#classifier-outputs","text":"[table/gobs] gob type: str | gob label: str | gob vertices: [(x,y,z,...)] | goodness: float | accuracy: float | color: str [image/png] bytes...","title":"CLASSIFIER OUTPUTS"},{"location":"guides/resources/module/connoisseur/#status-workflow","text":"new classes loaded ----- # needs to re-load all files from template and repeat all steps: 3-8 classes filtered | samples loaded ----- # needs to re-split the samples and other steps: 5-8 samples split | trained | validated | finished ------","title":"STATUS WORKFLOW"},{"location":"guides/resources/module/connoisseur/#training-workflow","text":"The model resource is created by the JS UI and initially may only contain a user-given name and a dataset to train on. After the model resource is created in the system several operations may be requested following a typical training sequence. Define required model parameters a. Pick a model template to fine-tune or train from scratch b. This should define a \"mode\", currently point classification but later also fully convolutional Find classes for the model, this will lock the model for current classes and will not allow further changes Create training samples, using parameters defined in the model. This can be repeated to augment the sample list a. Absolute minimum number of samples per class b. Minimum number of samples to activate sample augmentation Train on available samples a. train the model b. validate the model and update classes performance Set classes that will be used for classification based on observed performance in the training step Model is ready for recognition Repeat step 3 and further to improve the model","title":"TRAINING WORKFLOW"},{"location":"guides/resources/module/connoisseur/#model-modification-api","text":"GET /connoisseur/create/template:CaffeNet[/dataset:UUID] GET /connoisseur/MODEL_ID/update[/classes:init][/classes:filter][/samples:init][/samples:update][/samples:split] GET /connoisseur/MODEL_ID/train[/method:finetune][/method:snapshot] GET /connoisseur/MODEL_ID/validate","title":"MODEL MODIFICATION API"}]}