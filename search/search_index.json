{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BisQue Documentation Bio-Image Semantic Query User Environment https://bioimage.ucsb.edu/bisque Project Source Current Github: https://github.com/UCSB-VRL/bisque Docs : https://UCSB-VRL.github.io/bisque Legacy Bitbucket : https://bitbucket.org/CBIucsb/bisque Wiki : http://biodev.ece.ucsb.edu/projects/bisquik Clone BisQue Explore the source by checking out the bisque-stable branch from legacy mercurial or current github hg clone https://biodev.ece.ucsb.edu/hg/bisque-stable bisque Docs Bisque Development related artifacts/guide Bisque setup from source code Bisque setup in Docker Common Errors Jupyter Notebooks","title":"Home"},{"location":"#bisque-documentation","text":"Bio-Image Semantic Query User Environment https://bioimage.ucsb.edu/bisque","title":"BisQue Documentation"},{"location":"#project-source","text":"Current Github: https://github.com/UCSB-VRL/bisque Docs : https://UCSB-VRL.github.io/bisque Legacy Bitbucket : https://bitbucket.org/CBIucsb/bisque Wiki : http://biodev.ece.ucsb.edu/projects/bisquik","title":"Project Source"},{"location":"#clone-bisque","text":"Explore the source by checking out the bisque-stable branch from legacy mercurial or current github hg clone https://biodev.ece.ucsb.edu/hg/bisque-stable bisque","title":"Clone BisQue"},{"location":"#docs","text":"Bisque Development related artifacts/guide Bisque setup from source code Bisque setup in Docker Common Errors Jupyter Notebooks","title":"Docs"},{"location":"guides/about/","text":"About Wiki: http://biodev.ece.ucsb.edu/projects/bisquik Source Bitbucket: https://bitbucket.org/CBIucsb/bisque","title":"About"},{"location":"guides/about/#about","text":"Wiki: http://biodev.ece.ucsb.edu/projects/bisquik","title":"About"},{"location":"guides/about/#source","text":"Bitbucket: https://bitbucket.org/CBIucsb/bisque","title":"Source"},{"location":"guides/bisque/","text":"Bisque Development Environment Setup Instructions Project Source - https://bitbucket.org/CBIucsb/bisque - http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable Reference - Bique Bioimage Google Groups - Instructions on installing bisque using docker Setup for Ubuntu 16.04 Pre-requisites sudo apt-get install -y python python-dev python-virtualenv python-numpy python-scipy sudo apt-get install -y libxml2-dev libxslt1-dev libhdf5-dev sudo apt-get install -y libmysqlclient-dev libpq-dev mercurial git cmake sudo apt-get install -y postgresql postgresql-client libsqlite3-dev sudo apt-get install -y python-paver python-setuptools sudo apt-get install -y graphviz libgraphviz-dev pkg-config sudo apt-get install -y openslide-tools python-openslide sudo apt-get install -y libfftw3-dev libbz2-dev libz-dev sudo apt-get install -y liblcms2-dev libtiff-dev libpng-dev sudo apt-get install -y libgdcm2.6 libopenslide-dev libopenslide0 Install Openjpeg git clone https://github.com/uclouvain/openjpeg cd openjpeg mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release sudo make -j4 install sudo ldconfig Install BioImageConvert - BioImageConvert Source Repository - Prebuilt Binaries Repository - Setup for pre-built binaries (below) # Ubuntu 18 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu18_2.5.0.tar.gz # Ubuntu 16 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu16_2.4.3.tar.gz tar -xvzf imgcnv_ubuntu16_2.4.3.tar.gz sudo cp imgcnv_ubuntu16_2.4.3/imgcnv /usr/local/bin/ sudo cp imgcnv_ubuntu16_2.4.3/libimgcnv.so.2.4.3 /usr/local/lib/ sudo ln -s /usr/local/lib/libimgcnv.so.2.4.3 /usr/local/lib/libimgcnv.so.2.4 sudo ln -s /usr/local/lib/libimgcnv.so.2.4 /usr/local/lib/libimgcnv.so.2 sudo ln -s /usr/local/lib/libimgcnv.so.2 /usr/local/lib/libimgcnv.so sudo ldconfig Alternately, Compile by source and Install hg clone --insecure http://biodev.ece.ucsb.edu/hg/imgcnv cd imgcnv make -j6 sudo make install A. Download the Bootstrap installer (Use Python 2.7) $ mkdir bisque cd bisque $ wget http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable/contrib/bootstrap/bisque-bootstrap.py $ python bisque-bootstrap.py bqenv # Activate Virtualenv bisque$ source bqenv/bin/activate Fix the requirements.txt #Fix the requirements.txt file using sed -i 's/.*psycopg2==2.6.1.*/psycopg2==2.7.1./' requirements.txt psycopg2==2.7.1 Minimatic==1.0 Paste==1.7.5.1 httplib2==0.7.3 #tgext.registration==0.1dev Install separately since packages may be deprecated in PyPi easy_install http://biodev.ece.ucsb.edu/binaries/depot/tgext.registration2/tgext.registration2-0.5.2.tar.gz Now Install requirements pip install -i https://biodev.ece.ucsb.edu/py/bisque/stretch/+simple/ -r requirements.txt Alternate Index Url for Development: https://biodev.ece.ucsb.edu/py/bisque/dev/+simple B. Configure Bisque Environment $ paver setup Expected log tail Installing collected packages: bqengine Running setup.py develop for bqengine Successfully installed bqengine Now run: bq-admin setup $ bq-admin setup Expected log tail ... Found imgcnv version 2.4.3 Imgcnv is installed and no-precompiled version exists. Using installed version Top level site variables are: bisque.admin_email=YourEmail@YourOrganization bisque.admin_id=admin bisque.organization=Your Organization bisque.paths.root=. bisque.server=http://0.0.0.0:8080 bisque.title=Image Repository Change a site variable [Y]? N ... Running setup_config() from bq.websetup CALLING function install_preferences at 0x7f47f191f848 Initialize your database with: $ bq-admin setup createdb You can start bisque with: $ bq-admin server start then point your browser to: http://0.0.0.0:8080 If you need to shutdown the servers, then use: $ bq-admin server stop You can login as admin and change the default password. Send Installation/Registration report [Y]? N Add \"config/runtime-bisque.cfg\" for module configuration and docker image registry - Edit and add run config from config-defaults/runtime-bisque.defaults to config/runtime-bisque.cfg C. Run Bisque Start/Stop the server $ bq-admin server start $ bq-admin server stop Overview of Installation (Just for review) source bqenv/bin/activate paver setup [server|engine] bq-admin setup [server|engine] bq-admin deploy public Open in browser RUN Local Bisque Dev setup Load Module Workflow Login using admin:admin Open Module Manager from the admin user context menu Paste the following URL in the Module- Engine Module http://0.0.0.0:8080/engine_service/ List of engine modules could be seen Drag-n-Drop say Dream3D from the list to the left window and close the window Go to Analyse and select Dream3D there Further Load Dream 3D data == TODO Manually create a virtualenv for development and install dependencies ``` python -m virtualenv bqenv --no-setuptools source bqenv/bin/activate pip install -r requirements.txt --trusted-host=biodev.ece.ucsb.edu -i https://biodev.ece.ucsb.edu/py/bisque/dev/+simple ``` Install Docker (https://docs.docker.com/install/linux/docker-ce/ubuntu/) Also go through the post-installation steps Install Condor (https://research.cs.wisc.edu/htcondor/ubuntu/) rahul@bqdev:~$ /etc/init.d/condor restart [ ok ] Restarting condor (via systemctl): condor.service. rahul@bqdev:~$ ps -ef | grep condor","title":"BisQue"},{"location":"guides/bisque/#bisque-development-environment-setup-instructions","text":"Project Source - https://bitbucket.org/CBIucsb/bisque - http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable Reference - Bique Bioimage Google Groups - Instructions on installing bisque using docker","title":"Bisque Development Environment Setup Instructions"},{"location":"guides/bisque/#setup-for-ubuntu-1604","text":"","title":"Setup for Ubuntu 16.04"},{"location":"guides/bisque/#pre-requisites","text":"sudo apt-get install -y python python-dev python-virtualenv python-numpy python-scipy sudo apt-get install -y libxml2-dev libxslt1-dev libhdf5-dev sudo apt-get install -y libmysqlclient-dev libpq-dev mercurial git cmake sudo apt-get install -y postgresql postgresql-client libsqlite3-dev sudo apt-get install -y python-paver python-setuptools sudo apt-get install -y graphviz libgraphviz-dev pkg-config sudo apt-get install -y openslide-tools python-openslide sudo apt-get install -y libfftw3-dev libbz2-dev libz-dev sudo apt-get install -y liblcms2-dev libtiff-dev libpng-dev sudo apt-get install -y libgdcm2.6 libopenslide-dev libopenslide0 Install Openjpeg git clone https://github.com/uclouvain/openjpeg cd openjpeg mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release sudo make -j4 install sudo ldconfig Install BioImageConvert - BioImageConvert Source Repository - Prebuilt Binaries Repository - Setup for pre-built binaries (below) # Ubuntu 18 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu18_2.5.0.tar.gz # Ubuntu 16 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu16_2.4.3.tar.gz tar -xvzf imgcnv_ubuntu16_2.4.3.tar.gz sudo cp imgcnv_ubuntu16_2.4.3/imgcnv /usr/local/bin/ sudo cp imgcnv_ubuntu16_2.4.3/libimgcnv.so.2.4.3 /usr/local/lib/ sudo ln -s /usr/local/lib/libimgcnv.so.2.4.3 /usr/local/lib/libimgcnv.so.2.4 sudo ln -s /usr/local/lib/libimgcnv.so.2.4 /usr/local/lib/libimgcnv.so.2 sudo ln -s /usr/local/lib/libimgcnv.so.2 /usr/local/lib/libimgcnv.so sudo ldconfig Alternately, Compile by source and Install hg clone --insecure http://biodev.ece.ucsb.edu/hg/imgcnv cd imgcnv make -j6 sudo make install","title":"Pre-requisites"},{"location":"guides/bisque/#a-download-the-bootstrap-installer-use-python-27","text":"$ mkdir bisque cd bisque $ wget http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable/contrib/bootstrap/bisque-bootstrap.py $ python bisque-bootstrap.py bqenv # Activate Virtualenv bisque$ source bqenv/bin/activate Fix the requirements.txt #Fix the requirements.txt file using sed -i 's/.*psycopg2==2.6.1.*/psycopg2==2.7.1./' requirements.txt psycopg2==2.7.1 Minimatic==1.0 Paste==1.7.5.1 httplib2==0.7.3 #tgext.registration==0.1dev Install separately since packages may be deprecated in PyPi easy_install http://biodev.ece.ucsb.edu/binaries/depot/tgext.registration2/tgext.registration2-0.5.2.tar.gz Now Install requirements pip install -i https://biodev.ece.ucsb.edu/py/bisque/stretch/+simple/ -r requirements.txt Alternate Index Url for Development: https://biodev.ece.ucsb.edu/py/bisque/dev/+simple","title":"A. Download the Bootstrap installer (Use Python 2.7)"},{"location":"guides/bisque/#b-configure-bisque-environment","text":"$ paver setup Expected log tail Installing collected packages: bqengine Running setup.py develop for bqengine Successfully installed bqengine Now run: bq-admin setup $ bq-admin setup Expected log tail ... Found imgcnv version 2.4.3 Imgcnv is installed and no-precompiled version exists. Using installed version Top level site variables are: bisque.admin_email=YourEmail@YourOrganization bisque.admin_id=admin bisque.organization=Your Organization bisque.paths.root=. bisque.server=http://0.0.0.0:8080 bisque.title=Image Repository Change a site variable [Y]? N ... Running setup_config() from bq.websetup CALLING function install_preferences at 0x7f47f191f848 Initialize your database with: $ bq-admin setup createdb You can start bisque with: $ bq-admin server start then point your browser to: http://0.0.0.0:8080 If you need to shutdown the servers, then use: $ bq-admin server stop You can login as admin and change the default password. Send Installation/Registration report [Y]? N Add \"config/runtime-bisque.cfg\" for module configuration and docker image registry - Edit and add run config from config-defaults/runtime-bisque.defaults to config/runtime-bisque.cfg","title":"B. Configure Bisque Environment"},{"location":"guides/bisque/#c-run-bisque","text":"Start/Stop the server $ bq-admin server start $ bq-admin server stop Overview of Installation (Just for review) source bqenv/bin/activate paver setup [server|engine] bq-admin setup [server|engine] bq-admin deploy public Open in browser","title":"C. Run Bisque"},{"location":"guides/bisque/#run","text":"Local Bisque Dev setup","title":"RUN"},{"location":"guides/bisque/#load-module-workflow","text":"Login using admin:admin Open Module Manager from the admin user context menu Paste the following URL in the Module- Engine Module http://0.0.0.0:8080/engine_service/ List of engine modules could be seen Drag-n-Drop say Dream3D from the list to the left window and close the window Go to Analyse and select Dream3D there","title":"Load Module Workflow"},{"location":"guides/bisque/#further","text":"","title":"Further"},{"location":"guides/bisque/#load-dream-3d-data","text":"== TODO Manually create a virtualenv for development and install dependencies ``` python -m virtualenv bqenv --no-setuptools source bqenv/bin/activate pip install -r requirements.txt --trusted-host=biodev.ece.ucsb.edu -i https://biodev.ece.ucsb.edu/py/bisque/dev/+simple ``` Install Docker (https://docs.docker.com/install/linux/docker-ce/ubuntu/) Also go through the post-installation steps Install Condor (https://research.cs.wisc.edu/htcondor/ubuntu/) rahul@bqdev:~$ /etc/init.d/condor restart [ ok ] Restarting condor (via systemctl): condor.service. rahul@bqdev:~$ ps -ef | grep condor","title":"Load Dream 3D data"},{"location":"guides/bisque_docker/","text":"Bisque Docker Environment Setup Instructions Docker/Project Source Docker Hub Image Bitbucket CBIucsb/bisque-stable (TODO: Test with Github version!!) Bique Bioimage Google Groups Pre-requisite Install docker (light weight VM) on you laptop: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Instructions on installing bisque using docker https://bitbucket.org/CBIucsb/bisque/src/default/README.md Installation Use the installer script to setup bisque docker image wget https://bitbucket.org/CBIucsb/bisque-stable/downloads/build_bisque_docker_modules.sh sh build_bisque_docker_modules.sh You can modify this script to create a modules folder and mount/load modules from the host machine into docker Run Environment Start Docker and mount directories for run # Start xterm -e \\ docker run --name bisque --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ 'cbiucsb/bisque05:stable' Stop Docker docker stop $(docker ps -a -q --filter ancestor=cbiucsb/bisque05:stable --format= {{.ID}} ) Advanced environment for working with database URI as, sqlite:///data/bisque.db postgresql://rahul:rahul@localhost/bqmurks58 or postgresql://dbhost:5432/bisque docker run --name bisque --rm -p 8080:8080 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ -e BISQUE_DBURL=postgresql://rahul:rahul@localhost/bqmurks58 \\ 'cbiucsb/bisque05:stable' This will start the bisque docker/server Client Service http://0.0.0.0:8080 Engine Service http://0.0.0.0:8080/engine_service Module Service http://0.0.0.0:8080/module_service Features http://0.0.0.0:8080/features Use \"admin:admin\" to authenticate into this local environment To manipulate and check logs, connect to docker bash bisque-host$ docker exec -t -i bisque /bin/bash Develop in Docker (TODO !!) Use the dev docker container xterm -e \\ docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ 'cbiucsb/bisque05:dev' Bash into the docker container and develop docker exec -t -i bisque-dev /bin/bash # Source and develop docker-container$ source /usr/lib/bisque/bin/activate $ apt-get update $ apt-get install python-dev vim $ ln -s /usr/include/python2.7/ /usr/lib/bisque/local/include/python2.7 Test/Build a Module (TODO !! python-dev not setup) # Setup/Test Dream3D Module $ cd /source/modules/Dream3D $ python setup.py","title":"Docker"},{"location":"guides/bisque_docker/#bisque-docker-environment-setup-instructions","text":"","title":"Bisque Docker Environment Setup Instructions"},{"location":"guides/bisque_docker/#dockerproject-source","text":"Docker Hub Image Bitbucket CBIucsb/bisque-stable (TODO: Test with Github version!!) Bique Bioimage Google Groups","title":"Docker/Project Source"},{"location":"guides/bisque_docker/#pre-requisite","text":"Install docker (light weight VM) on you laptop: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Instructions on installing bisque using docker https://bitbucket.org/CBIucsb/bisque/src/default/README.md","title":"Pre-requisite"},{"location":"guides/bisque_docker/#installation","text":"Use the installer script to setup bisque docker image wget https://bitbucket.org/CBIucsb/bisque-stable/downloads/build_bisque_docker_modules.sh sh build_bisque_docker_modules.sh You can modify this script to create a modules folder and mount/load modules from the host machine into docker","title":"Installation"},{"location":"guides/bisque_docker/#run-environment","text":"Start Docker and mount directories for run # Start xterm -e \\ docker run --name bisque --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ 'cbiucsb/bisque05:stable' Stop Docker docker stop $(docker ps -a -q --filter ancestor=cbiucsb/bisque05:stable --format= {{.ID}} ) Advanced environment for working with database URI as, sqlite:///data/bisque.db postgresql://rahul:rahul@localhost/bqmurks58 or postgresql://dbhost:5432/bisque docker run --name bisque --rm -p 8080:8080 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ -e BISQUE_DBURL=postgresql://rahul:rahul@localhost/bqmurks58 \\ 'cbiucsb/bisque05:stable' This will start the bisque docker/server Client Service http://0.0.0.0:8080 Engine Service http://0.0.0.0:8080/engine_service Module Service http://0.0.0.0:8080/module_service Features http://0.0.0.0:8080/features Use \"admin:admin\" to authenticate into this local environment To manipulate and check logs, connect to docker bash bisque-host$ docker exec -t -i bisque /bin/bash","title":"Run Environment"},{"location":"guides/bisque_docker/#develop-in-docker-todo","text":"Use the dev docker container xterm -e \\ docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ 'cbiucsb/bisque05:dev' Bash into the docker container and develop docker exec -t -i bisque-dev /bin/bash # Source and develop docker-container$ source /usr/lib/bisque/bin/activate $ apt-get update $ apt-get install python-dev vim $ ln -s /usr/include/python2.7/ /usr/lib/bisque/local/include/python2.7 Test/Build a Module (TODO !! python-dev not setup) # Setup/Test Dream3D Module $ cd /source/modules/Dream3D $ python setup.py","title":"Develop in Docker (TODO !!)"},{"location":"guides/bisque_postgres/","text":"Create a Volume: pgdev-vol Path on the host node: /tmp/postgres Setup a Postgresql 10 database Image: postgres:10.4 Environment POSTGRES_DB=postgres POSTGRES_PASSWORD=postgres POSTGRES_USER=postgres Spec for a Rancher 2 workload spec: containers: - env: - name: POSTGRES_DB value: postgres - name: POSTGRES_PASSWORD value: postgres - name: POSTGRES_USER value: postgres image: postgres:10.4 imagePullPolicy: Always name: postgres ports: - containerPort: 5432 hostPort: 5432 name: 5432tcp54320 protocol: TCP resources: {} volumeMounts: - mountPath: /var/lib/postgresql/data name: pgdev-vol - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-7hhqq readOnly: true dnsPolicy: ClusterFirst nodeName: loup volumes: - name: pgdev-vol persistentVolumeClaim: claimName: pgdev-vol - name: default-token-7hhqq secret: defaultMode: 420 secretName: default-token-7hhqq Test database using the nodes IP psql -h 10.42.0.15 -U postgres --password -p 5432 postgres psql -h loup.ece.ucsb.edu -U postgres --password -p 5432 postgres","title":"Bisque postgres"},{"location":"guides/bisque_postgres/#create-a-volume-pgdev-vol","text":"Path on the host node: /tmp/postgres","title":"Create a Volume: pgdev-vol"},{"location":"guides/bisque_postgres/#setup-a-postgresql-10-database","text":"Image: postgres:10.4 Environment POSTGRES_DB=postgres POSTGRES_PASSWORD=postgres POSTGRES_USER=postgres Spec for a Rancher 2 workload spec: containers: - env: - name: POSTGRES_DB value: postgres - name: POSTGRES_PASSWORD value: postgres - name: POSTGRES_USER value: postgres image: postgres:10.4 imagePullPolicy: Always name: postgres ports: - containerPort: 5432 hostPort: 5432 name: 5432tcp54320 protocol: TCP resources: {} volumeMounts: - mountPath: /var/lib/postgresql/data name: pgdev-vol - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-7hhqq readOnly: true dnsPolicy: ClusterFirst nodeName: loup volumes: - name: pgdev-vol persistentVolumeClaim: claimName: pgdev-vol - name: default-token-7hhqq secret: defaultMode: 420 secretName: default-token-7hhqq","title":"Setup a Postgresql 10 database"},{"location":"guides/bisque_postgres/#test-database-using-the-nodes-ip","text":"psql -h 10.42.0.15 -U postgres --password -p 5432 postgres psql -h loup.ece.ucsb.edu -U postgres --password -p 5432 postgres","title":"Test database using the nodes IP"},{"location":"guides/bisque_rancher/","text":"Rancher: http://saw.ece.ucsb.edu:8080 Container organization and plan for rancher setup Main Containers Instance Name Host or IP Image Name Remarks condor-nodes IP ADDR biodev.ece.ucsb.edu:5000/condor One master and 4 worker nodes elasticsearch 2 IP ADDR rancher/elasticsearch-conf:v0.5.0 One container each elasticsearch-[clients, datanodes masters] elasticsearch base/data IP ADDR elasticsearch:2.4.3-alpine Two containers(base data volume) for each elasticsearch-[clients, datanodes masters] elasticsearch kopf IP ADDR rancher/kopf:v0.4.0 One container healthcheck IP ADDR rancher/healthcheck:v0.3.6 One on each instance ipsec cni-driver, connectivity-check, router IP ADDR rancher/net:v0.13.11 One on each instance for executing start-cni-driver, connectivity-check, and start-ipsec ipsec-ipsec IP ADDR rancher/net:holder One on each instance network-services-meta IP ADDR rancher/metadata:v0.10.2 One on each instance start.sh,rancher-metadata,-reload-interval-limit=1000,-subscribe network-services-meta-dns IP ADDR rancher/dns:v0.17.3 One on each instance rancher-dns,--listen,169.254.169.250:53,--metadata-server=localhost network-services-manager IP ADDR rancher/network-manager:v0.7.20 One on each instance plugin-manager,--disable-cni-setup,--metadata-address,169.254.169.250 nfs-driver IP ADDR rancher/storage-nfs:v0.9.1 One on each instance rancher-agent IP ADDR rancher/agent:v1.2.10 One on each instance scheduler IP ADDR rancher/scheduler:v0.8.3 One instance scheduler,--metadata-address,169.254.169.250 janitor-cleanup IP ADDR meltwater/docker-cleanup:1.8.0 One on each instance kibana IP ADDR kibana:5.3.0 One instance kibana rancher IP ADDR rancher/lb-service-haproxy:v0.7.9, rancher/nginx:v1.9.4-3, rancher/nginx-conf:v0.2.0 One instance production logsvc IP ADDR biodev.ece.ucsb.edu:5000/logger_ucsb:dev One instance LetsEncrypt IP ADDR janeczku/rancher-letsencrypt:v0.4.0 One instance jenkins IP ADDR jenkins/jenkins:lts One instance jenkins plugins IP ADDR biodev.ece.ucsb.edu:5000/jenkins-cbi-plugins:v0.1.1 One instance Modules Containers Instance Name Host or IP Image Name Remarks Dream3D IP ADDR biodev.ece.ucsb.edu:5000/bisque_dream3d Dream3D Module Predict Strength IP ADDR biodev.ece.ucsb.edu:5000/predict_strength Predict Strength Module Connoisseur IP ADDR biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Predict Strength Module","title":"Bisque rancher"},{"location":"guides/bisque_rancher/#rancher-httpsaweceucsbedu8080","text":"Container organization and plan for rancher setup","title":"Rancher: http://saw.ece.ucsb.edu:8080"},{"location":"guides/bisque_rancher/#main-containers","text":"Instance Name Host or IP Image Name Remarks condor-nodes IP ADDR biodev.ece.ucsb.edu:5000/condor One master and 4 worker nodes elasticsearch 2 IP ADDR rancher/elasticsearch-conf:v0.5.0 One container each elasticsearch-[clients, datanodes masters] elasticsearch base/data IP ADDR elasticsearch:2.4.3-alpine Two containers(base data volume) for each elasticsearch-[clients, datanodes masters] elasticsearch kopf IP ADDR rancher/kopf:v0.4.0 One container healthcheck IP ADDR rancher/healthcheck:v0.3.6 One on each instance ipsec cni-driver, connectivity-check, router IP ADDR rancher/net:v0.13.11 One on each instance for executing start-cni-driver, connectivity-check, and start-ipsec ipsec-ipsec IP ADDR rancher/net:holder One on each instance network-services-meta IP ADDR rancher/metadata:v0.10.2 One on each instance start.sh,rancher-metadata,-reload-interval-limit=1000,-subscribe network-services-meta-dns IP ADDR rancher/dns:v0.17.3 One on each instance rancher-dns,--listen,169.254.169.250:53,--metadata-server=localhost network-services-manager IP ADDR rancher/network-manager:v0.7.20 One on each instance plugin-manager,--disable-cni-setup,--metadata-address,169.254.169.250 nfs-driver IP ADDR rancher/storage-nfs:v0.9.1 One on each instance rancher-agent IP ADDR rancher/agent:v1.2.10 One on each instance scheduler IP ADDR rancher/scheduler:v0.8.3 One instance scheduler,--metadata-address,169.254.169.250 janitor-cleanup IP ADDR meltwater/docker-cleanup:1.8.0 One on each instance kibana IP ADDR kibana:5.3.0 One instance kibana rancher IP ADDR rancher/lb-service-haproxy:v0.7.9, rancher/nginx:v1.9.4-3, rancher/nginx-conf:v0.2.0 One instance production logsvc IP ADDR biodev.ece.ucsb.edu:5000/logger_ucsb:dev One instance LetsEncrypt IP ADDR janeczku/rancher-letsencrypt:v0.4.0 One instance jenkins IP ADDR jenkins/jenkins:lts One instance jenkins plugins IP ADDR biodev.ece.ucsb.edu:5000/jenkins-cbi-plugins:v0.1.1 One instance","title":"Main Containers"},{"location":"guides/bisque_rancher/#modules-containers","text":"Instance Name Host or IP Image Name Remarks Dream3D IP ADDR biodev.ece.ucsb.edu:5000/bisque_dream3d Dream3D Module Predict Strength IP ADDR biodev.ece.ucsb.edu:5000/predict_strength Predict Strength Module Connoisseur IP ADDR biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Predict Strength Module","title":"Modules Containers"},{"location":"guides/bisque_rancher2/","text":"Bisque Rancher 2.0 Setup (with Kubernetes engine) Nomenclature (Kubernetes like) Name Kubernetes concepts Container Pod (Simplest Kubernetes object representing a set of containers on the cluster) Services Workload (Units of work that are running on the cluster, these can be pods or deployments) Load Balancer Ingress Stack Namespace (A virtual cluster) Environment Project (Administration)/Cluster (Compute machines that run containerized applications) Host Node (Physical or virtual machines making up the cluster) Catalog Helm Cluster Description Lets Encrypt on Ubuntu 16.04 Bind the hostname to the IP address by creating an A record in DNS Letsencrypt ACME challenge at TCP/80 on host Open up firewall for this \"sudo ufw allow 80 sudo ufw allow 80 443\" Verify the port 80 availability sudo netstat -peanut | grep :80 Setup certificates at /etc/letsencrypt sudo certbot certonly --standalone --dry-run \\ --cert-name loup.ece.ucsb.edu -d loup.ece.ucsb.edu Installation Rancher 2.0 Install/Startup Rancher: https://rancher.com/docs/rancher/v2.x/en/installation/single-node/ - Rancher etcd data persisted at /var/lib/rancher - Since port 80 is occupied by rancher/rancher, a rancher/rancher-agent cannot be run on this node. docker run -d --restart=unless-stopped \\ -p 8080:80 -p 8443:443 \\ -v /var/log/rancher/auditlog:/var/log/auditlog \\ -v /host/rancher:/var/lib/rancher \\ -e AUDIT_LEVEL=1 \\ rancher/rancher:stable You will have rancher accessible at https://loup.ece.ucsb.edu:8443 Create the YAML configuration based on docker-compose.yaml (In case of migration) Create an access key for Rancher CLI operations (Doesnt work on self-signed certs) endpoint : https://loup.ece.ucsb.edu/v3 access-key: token-xt47w secret-key: bearer-tok: token-xt47w: Migration CLI Download the docker-compose and rancher-compose.yml files from existing rancher user interface for migration https://rancher.com/docs/rancher/v2.x/en/v1.6-migration Migration using CLI tools # migration-tools export --url RANCHER_URL --access-key RANCHER_ACCESS_KEY \\ # --secret-key RANCHER_SECRET_KEY --export-dir EXPORT_DIR ./migration-tools parse --docker-file compose/docker-compose.yml \\ --rancher-file compose/rancher-compose.yml Setup Cluster /rke-clusters/custom-nodes Port requirements Open up ports based on the CNI provider requirements - Use Canal as the provider in this case # API/UI Clients sudo ufw allow 22,80,443/tcp # Etcd Plane Nodes sudo ufw allow 2379,2380,9099,6443/tcp sudo ufw allow 8472/udp # Control Plane Nodes sudo ufw allow 2379,2380,10250,6443,9099,10254/tcp sudo ufw allow 8472/udp # Worker Plane Nodes sudo ufw allow 6443,9099,10254/tcp sudo ufw allow 8472/udp # Workload sudo ufw allow 30000:32767/tcp sudo ufw allow 30000:32767/udp # Others sudo ufw allow 2376/tcp Create cluster Added a cluster in rancher-ui named \"bqdev-cluster\" and run the below command for running the rancher-agent sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 --server https://loup.ece.ucsb.edu:8443 \\ --token 7z2ncgjj4482m48fpsj7xjmc8lc9n6bsxh7qcjrsr6rcxrzhzl6prz \\ --ca-checksum d522680b13d7aabe4dc57bb2776e28759852c336d0cf0e0f9fed5d3fb7b495e8 \\ --etcd --controlplane --worker Add more nodes as worker only, by running this command on those nodes so that they register with the above main control node sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 --server https://arkady.ece.ucsb.edu \\ --token 298vqgm9fs6kfd8n5rbmwq6mx87g6vdbmr26c7xbf4bgb6t9z4bcjt \\ --worker Create a namespace bqdev within this cluster Bisque Test environment where workloads are deployed Setup Volume Mount the host directory for volume using NFS and setup the nfs client access for the cluster https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04 Setup folders # Create the path on host system sudo mkdir /opt/bisque/ -p \\ sudo mkdir /opt/bisque/data -p \\ sudo mkdir /opt/bisque/local/workdir -p # Allow other users to edit this sudo chown -R nobody:nogroup /opt/bisque/ Open up the ports used by NFS # Access from specific machines sudo ufw allow from 192.168.1.129 to any port nfs sudo ufw allow from 192.168.1.133 to any port nfs # Specific ports in case above doesnt work sudo ufw allow 32768:65535/tcp sudo ufw allow 32768:65535/udp sudo ufw allow 2049/tcp sudo ufw allow 2049/udp sudo ufw allow 111/tcp sudo ufw allow 111/udp Now add NFS host configuration at /etc/exports /opt/bisque 192.168.1.129(rw,sync,no_root_squash,no_subtree_check) /opt/bisque 192.168.1.133(rw,sync,no_root_squash,no_subtree_check) restart the nfs server sudo systemctl restart nfs-kernel-server Mount the NFS folder on the client sudo mount 192.168.1.123:/opt/bisque/ /run/bisque/ Verify the mount on a client system using df -h OR https://www.claudiokuenzler.com/blog/786/rancher-2.0-create-persistent-volume-from-nfs-share - Create a persistent volume in the cluster - Set local path option on the node as /run/bisque Setup Workload on the cluster Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc Workload configuration Name: ucsb-bisque05-svc-wl Pods: 1 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 80-tcp-ClusterIP(Internal)-Same 27000-tcp-ClusterIP(Internal)-Same Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /tmp/bisque Scaling: No change Command: Entrypoint: /builder/run-bisque.sh Command: bootstrap start Working Dir: /source Console: Interactive TTY (-i -t) Networking: No Change Labels: No change Security: No change Environment Configuration Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 Connoisseur GPU variables NVIDIA_REQUIRE_CUDA=cuda =8.0 NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility CUDA_PKG_VERSION=8-0=8.0.61-1 CAFFE_PKG_VERSION=0.15.13-1ubuntu16.04+cuda8.0 CAFFE_VERSION=0.15 CUDA_VERSION=8.0.61 IMGCNV=imgcnv_ubuntu16_2.4.3 CONDOR_MANAGER_HOST=master.condor CONDOR_DAEMONS=MASTER,SCHEDD,SHARED_PORT Debugging the cluster/pods Using cluster kubectl shell from the cluster web UI # Fetch namespaces kubectl get pods --all-namespaces kubectl get pods -n bqdev # Fetch logs on a pod/container kubectl logs postgres-564d9f79d5-z2sxl -n bqdev Mail server setup https://www.linuxbabe.com/mail-server/ubuntu-16-04-iredmail-server-installation Move individual workload/containers to rancher-kubernetes using rancher-cli (doesnt work with self-signed certificates) https://rancher.com/blog/2018/2018-08-02-journey-from-cattle-to-k8s/ Run Bisque on two nodes Load Balancers add in workloads /k8s-in-rancher/load-balancers-and-ingress Tried using built in Ingress for bisque-test.bqtest.192.168.1.112.xip.io but failed to work If you want to expose the workload container to outside world then use NodePort otherwise work with ClusterIp(Internal Only) port configuration https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/ Remove/Cleanup Rancher Stop Rancher Containers # Master: for the rancher server container docker stop $(docker ps -a -q --filter ancestor=rancher/rancher:stable --format= {{.ID}} ) # Workers: for all k8s containers docker stop $(docker ps -f name=k8s* --format= {{.ID}} ) Clean the container, images and volumes docker rm -f $(docker ps -a -f name=k8s* --format= {{.ID}} ) docker rmi -f $(docker images -q rancher/* ) docker volume rm $(docker volume ls -q) Unmount and remove data (/var/lib/kubelet/pods/XXX, /var/lib/kubelet, /var/lib/rancher) # Unmount directories for mount in $(mount | grep tmpfs | grep '/var/lib/kubelet' | awk '{ print $3 }') /var/lib/kubelet /var/lib/rancher; do sudo umount $mount; done # Clean the directories sudo rm -rf /etc/ceph \\ /etc/cni \\ /etc/kubernetes \\ /opt/cni \\ /opt/rke \\ /run/secrets/kubernetes.io \\ /run/calico \\ /run/flannel \\ /var/lib/calico \\ /var/lib/etcd \\ /var/lib/cni \\ /var/lib/kubelet \\ /var/lib/rancher/rke/log \\ /var/log/containers \\ /var/log/pods \\ /var/run/calico # Mounted host directories sudo rm -rf /host/rancher/ sudo rm -rf /var/log/rancher/auditlog Remove the existing network interface ip address show ip link delete interface_name Condor provisioning Condor Master - Image: biodev.ece.ucsb.edu:5000/condor - Ports: 9618, 9886 as HostPort - Environment CONDOR_DAEMONS = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT CONDOR_MANAGER_HOST = master Condor Worker - Same configuration as above - Ports: 9886 NodePort Random","title":"Bisque Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/bisque_rancher2/#bisque-rancher-20-setup-with-kubernetes-engine","text":"","title":"Bisque Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/bisque_rancher2/#nomenclature-kubernetes-like","text":"Name Kubernetes concepts Container Pod (Simplest Kubernetes object representing a set of containers on the cluster) Services Workload (Units of work that are running on the cluster, these can be pods or deployments) Load Balancer Ingress Stack Namespace (A virtual cluster) Environment Project (Administration)/Cluster (Compute machines that run containerized applications) Host Node (Physical or virtual machines making up the cluster) Catalog Helm","title":"Nomenclature (Kubernetes like)"},{"location":"guides/bisque_rancher2/#cluster-description","text":"","title":"Cluster Description"},{"location":"guides/bisque_rancher2/#lets-encrypt-on-ubuntu-1604","text":"Bind the hostname to the IP address by creating an A record in DNS Letsencrypt ACME challenge at TCP/80 on host Open up firewall for this \"sudo ufw allow 80 sudo ufw allow 80 443\" Verify the port 80 availability sudo netstat -peanut | grep :80 Setup certificates at /etc/letsencrypt sudo certbot certonly --standalone --dry-run \\ --cert-name loup.ece.ucsb.edu -d loup.ece.ucsb.edu","title":"Lets Encrypt on Ubuntu 16.04"},{"location":"guides/bisque_rancher2/#installation-rancher-20","text":"Install/Startup Rancher: https://rancher.com/docs/rancher/v2.x/en/installation/single-node/ - Rancher etcd data persisted at /var/lib/rancher - Since port 80 is occupied by rancher/rancher, a rancher/rancher-agent cannot be run on this node. docker run -d --restart=unless-stopped \\ -p 8080:80 -p 8443:443 \\ -v /var/log/rancher/auditlog:/var/log/auditlog \\ -v /host/rancher:/var/lib/rancher \\ -e AUDIT_LEVEL=1 \\ rancher/rancher:stable You will have rancher accessible at https://loup.ece.ucsb.edu:8443","title":"Installation Rancher 2.0"},{"location":"guides/bisque_rancher2/#create-the-yaml-configuration-based-on-docker-composeyaml-in-case-of-migration","text":"","title":"Create the YAML configuration based on docker-compose.yaml (In case of migration)"},{"location":"guides/bisque_rancher2/#create-an-access-key-for-rancher-cli-operations-doesnt-work-on-self-signed-certs","text":"endpoint : https://loup.ece.ucsb.edu/v3 access-key: token-xt47w secret-key: bearer-tok: token-xt47w:","title":"Create an access key for Rancher CLI operations (Doesnt work on self-signed certs)"},{"location":"guides/bisque_rancher2/#migration-cli","text":"Download the docker-compose and rancher-compose.yml files from existing rancher user interface for migration https://rancher.com/docs/rancher/v2.x/en/v1.6-migration Migration using CLI tools # migration-tools export --url RANCHER_URL --access-key RANCHER_ACCESS_KEY \\ # --secret-key RANCHER_SECRET_KEY --export-dir EXPORT_DIR ./migration-tools parse --docker-file compose/docker-compose.yml \\ --rancher-file compose/rancher-compose.yml","title":"Migration CLI"},{"location":"guides/bisque_rancher2/#setup-cluster-rke-clusterscustom-nodes","text":"","title":"Setup Cluster /rke-clusters/custom-nodes"},{"location":"guides/bisque_rancher2/#port-requirements","text":"Open up ports based on the CNI provider requirements - Use Canal as the provider in this case # API/UI Clients sudo ufw allow 22,80,443/tcp # Etcd Plane Nodes sudo ufw allow 2379,2380,9099,6443/tcp sudo ufw allow 8472/udp # Control Plane Nodes sudo ufw allow 2379,2380,10250,6443,9099,10254/tcp sudo ufw allow 8472/udp # Worker Plane Nodes sudo ufw allow 6443,9099,10254/tcp sudo ufw allow 8472/udp # Workload sudo ufw allow 30000:32767/tcp sudo ufw allow 30000:32767/udp # Others sudo ufw allow 2376/tcp","title":"Port requirements"},{"location":"guides/bisque_rancher2/#create-cluster","text":"Added a cluster in rancher-ui named \"bqdev-cluster\" and run the below command for running the rancher-agent sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 --server https://loup.ece.ucsb.edu:8443 \\ --token 7z2ncgjj4482m48fpsj7xjmc8lc9n6bsxh7qcjrsr6rcxrzhzl6prz \\ --ca-checksum d522680b13d7aabe4dc57bb2776e28759852c336d0cf0e0f9fed5d3fb7b495e8 \\ --etcd --controlplane --worker Add more nodes as worker only, by running this command on those nodes so that they register with the above main control node sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 --server https://arkady.ece.ucsb.edu \\ --token 298vqgm9fs6kfd8n5rbmwq6mx87g6vdbmr26c7xbf4bgb6t9z4bcjt \\ --worker","title":"Create cluster"},{"location":"guides/bisque_rancher2/#create-a-namespace-bqdev-within-this-cluster","text":"Bisque Test environment where workloads are deployed","title":"Create a namespace bqdev within this cluster"},{"location":"guides/bisque_rancher2/#setup-volume","text":"Mount the host directory for volume using NFS and setup the nfs client access for the cluster https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04 Setup folders # Create the path on host system sudo mkdir /opt/bisque/ -p \\ sudo mkdir /opt/bisque/data -p \\ sudo mkdir /opt/bisque/local/workdir -p # Allow other users to edit this sudo chown -R nobody:nogroup /opt/bisque/ Open up the ports used by NFS # Access from specific machines sudo ufw allow from 192.168.1.129 to any port nfs sudo ufw allow from 192.168.1.133 to any port nfs # Specific ports in case above doesnt work sudo ufw allow 32768:65535/tcp sudo ufw allow 32768:65535/udp sudo ufw allow 2049/tcp sudo ufw allow 2049/udp sudo ufw allow 111/tcp sudo ufw allow 111/udp Now add NFS host configuration at /etc/exports /opt/bisque 192.168.1.129(rw,sync,no_root_squash,no_subtree_check) /opt/bisque 192.168.1.133(rw,sync,no_root_squash,no_subtree_check) restart the nfs server sudo systemctl restart nfs-kernel-server Mount the NFS folder on the client sudo mount 192.168.1.123:/opt/bisque/ /run/bisque/ Verify the mount on a client system using df -h OR https://www.claudiokuenzler.com/blog/786/rancher-2.0-create-persistent-volume-from-nfs-share - Create a persistent volume in the cluster - Set local path option on the node as /run/bisque","title":"Setup Volume"},{"location":"guides/bisque_rancher2/#setup-workload-on-the-cluster","text":"Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc","title":"Setup Workload on the cluster"},{"location":"guides/bisque_rancher2/#workload-configuration","text":"Name: ucsb-bisque05-svc-wl Pods: 1 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 80-tcp-ClusterIP(Internal)-Same 27000-tcp-ClusterIP(Internal)-Same Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /tmp/bisque Scaling: No change Command: Entrypoint: /builder/run-bisque.sh Command: bootstrap start Working Dir: /source Console: Interactive TTY (-i -t) Networking: No Change Labels: No change Security: No change","title":"Workload configuration"},{"location":"guides/bisque_rancher2/#environment-configuration","text":"Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 Connoisseur GPU variables NVIDIA_REQUIRE_CUDA=cuda =8.0 NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility CUDA_PKG_VERSION=8-0=8.0.61-1 CAFFE_PKG_VERSION=0.15.13-1ubuntu16.04+cuda8.0 CAFFE_VERSION=0.15 CUDA_VERSION=8.0.61 IMGCNV=imgcnv_ubuntu16_2.4.3 CONDOR_MANAGER_HOST=master.condor CONDOR_DAEMONS=MASTER,SCHEDD,SHARED_PORT","title":"Environment Configuration"},{"location":"guides/bisque_rancher2/#debugging-the-clusterpods","text":"Using cluster kubectl shell from the cluster web UI # Fetch namespaces kubectl get pods --all-namespaces kubectl get pods -n bqdev # Fetch logs on a pod/container kubectl logs postgres-564d9f79d5-z2sxl -n bqdev","title":"Debugging the cluster/pods"},{"location":"guides/bisque_rancher2/#mail-server-setup","text":"https://www.linuxbabe.com/mail-server/ubuntu-16-04-iredmail-server-installation","title":"Mail server setup"},{"location":"guides/bisque_rancher2/#move-individual-workloadcontainers-to-rancher-kubernetes-using-rancher-cli-doesnt-work-with-self-signed-certificates","text":"https://rancher.com/blog/2018/2018-08-02-journey-from-cattle-to-k8s/","title":"Move individual workload/containers to rancher-kubernetes using rancher-cli (doesnt work with self-signed certificates)"},{"location":"guides/bisque_rancher2/#run-bisque-on-two-nodes","text":"Load Balancers add in workloads /k8s-in-rancher/load-balancers-and-ingress Tried using built in Ingress for bisque-test.bqtest.192.168.1.112.xip.io but failed to work If you want to expose the workload container to outside world then use NodePort otherwise work with ClusterIp(Internal Only) port configuration https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/","title":"Run Bisque on two nodes"},{"location":"guides/bisque_rancher2/#removecleanup-rancher","text":"Stop Rancher Containers # Master: for the rancher server container docker stop $(docker ps -a -q --filter ancestor=rancher/rancher:stable --format= {{.ID}} ) # Workers: for all k8s containers docker stop $(docker ps -f name=k8s* --format= {{.ID}} ) Clean the container, images and volumes docker rm -f $(docker ps -a -f name=k8s* --format= {{.ID}} ) docker rmi -f $(docker images -q rancher/* ) docker volume rm $(docker volume ls -q) Unmount and remove data (/var/lib/kubelet/pods/XXX, /var/lib/kubelet, /var/lib/rancher) # Unmount directories for mount in $(mount | grep tmpfs | grep '/var/lib/kubelet' | awk '{ print $3 }') /var/lib/kubelet /var/lib/rancher; do sudo umount $mount; done # Clean the directories sudo rm -rf /etc/ceph \\ /etc/cni \\ /etc/kubernetes \\ /opt/cni \\ /opt/rke \\ /run/secrets/kubernetes.io \\ /run/calico \\ /run/flannel \\ /var/lib/calico \\ /var/lib/etcd \\ /var/lib/cni \\ /var/lib/kubelet \\ /var/lib/rancher/rke/log \\ /var/log/containers \\ /var/log/pods \\ /var/run/calico # Mounted host directories sudo rm -rf /host/rancher/ sudo rm -rf /var/log/rancher/auditlog Remove the existing network interface ip address show ip link delete interface_name","title":"Remove/Cleanup Rancher"},{"location":"guides/bisque_rancher2/#condor-provisioning","text":"Condor Master - Image: biodev.ece.ucsb.edu:5000/condor - Ports: 9618, 9886 as HostPort - Environment CONDOR_DAEMONS = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT CONDOR_MANAGER_HOST = master Condor Worker - Same configuration as above - Ports: 9886 NodePort Random","title":"Condor provisioning"},{"location":"guides/connoisseur/","text":"Setup Connoisseur Information Connoisseur is the DL module for distributed compute. Condor is running on each node TODO Figure out the configurations !!! 1. OpenCV 3.4.1 Installation Guide: Link 1 # Verify your installation pkg-config --libs opencv 2. CUDA 8.0 and cuDNN v7.1.4 Installation Guide: Link1 , Link2 Download cuda_8.0.61 $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Sun_Sep__4_22:14:01_CDT_2016 Cuda compilation tools, release 8.0, V8.0.44 Download libcudnn7-dev_7.1.4 cuDNN v7.1.4 Runtime Library for Ubuntu16.04 (Deb) cuDNN v7.1.4 Developer Library for Ubuntu16.04 (Deb) 3. CAFFE Setup caffe 1.0 using the official guide Installation Overview mkdir build cd build cmake .. make all make install make runtest $ caffe --version caffe version 1.0.0 Make sure you configure the path env in ~/.bashrc export PATH=$PATH:/usr/local/cuda/bin:~/cv/caffe/build/tools export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/cv/caffe/.build_release/lib export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/:~/cv/caffe/.build_release/lib Also add the following in ~/[python_env_folder]/bin/activate script after the PATH variable setup export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/ Test it all out to ensure caffe and opencv are functional (bqenv) rahul@bqdev:~/ws/bisque$ python Python 2.7.12 (default, Dec 4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2 import cv2 import caffe Also, ensure Caffe runs on the system $ caffe --version caffe version 1.0.0 4. Install Connoisseur Pre-requisite: (caffe and opencv may be the cause) source ~/bisque/bqenv/bin/activate pip install lmdb pyvoro Installation In Bisque top directory unzip the contents of module/connoisseur.zip Activate Env source ~/bisque/bqenv/bin/activate Ensure that caffe and opencv is importable in python Install connoisseur cd ~/bisque/connoisseur pip install -e connoisseur Move the module to Bisque modules mv ~/bisque/connoisseur/modules/Connoisseur/ ~/bisque/modules/ Deactivate Env source ~/bisque/bqenv/bin/deactivate Re/Start the Bisque server $ paver setup bq-admin setup $ bq-admin server start $ bq-admin server stop Make sure the service is available at http://0.0.0.0:8080/services Verify the log in module manager has no errors Thereafter look for the loaded services at the URL http://0.0.0.0:8080/services Look for the service named connoisseur in this list 5. Load Connoisseur Now lets add the connoisseur user group for loading the module Create a connoisseur group Add this group to the user/admin Restart the services $ bq-admin server start bq-admin server stop Load the module as user/admin Open the module from navbar using Analyze- Classification- Connoisseur TODO: Configuration, load model and run a test on Connoisseur","title":"Connoisseur"},{"location":"guides/connoisseur/#setup-connoisseur","text":"","title":"Setup Connoisseur"},{"location":"guides/connoisseur/#information","text":"Connoisseur is the DL module for distributed compute. Condor is running on each node","title":"Information"},{"location":"guides/connoisseur/#todo","text":"Figure out the configurations !!!","title":"TODO"},{"location":"guides/connoisseur/#1-opencv-341","text":"Installation Guide: Link 1 # Verify your installation pkg-config --libs opencv","title":"1. OpenCV 3.4.1"},{"location":"guides/connoisseur/#2-cuda-80-and-cudnn-v714","text":"Installation Guide: Link1 , Link2 Download cuda_8.0.61 $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Sun_Sep__4_22:14:01_CDT_2016 Cuda compilation tools, release 8.0, V8.0.44 Download libcudnn7-dev_7.1.4 cuDNN v7.1.4 Runtime Library for Ubuntu16.04 (Deb) cuDNN v7.1.4 Developer Library for Ubuntu16.04 (Deb)","title":"2. CUDA 8.0 and cuDNN v7.1.4"},{"location":"guides/connoisseur/#3-caffe","text":"Setup caffe 1.0 using the official guide Installation Overview mkdir build cd build cmake .. make all make install make runtest $ caffe --version caffe version 1.0.0 Make sure you configure the path env in ~/.bashrc export PATH=$PATH:/usr/local/cuda/bin:~/cv/caffe/build/tools export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/cv/caffe/.build_release/lib export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/:~/cv/caffe/.build_release/lib Also add the following in ~/[python_env_folder]/bin/activate script after the PATH variable setup export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/ Test it all out to ensure caffe and opencv are functional (bqenv) rahul@bqdev:~/ws/bisque$ python Python 2.7.12 (default, Dec 4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2 import cv2 import caffe Also, ensure Caffe runs on the system $ caffe --version caffe version 1.0.0","title":"3. CAFFE"},{"location":"guides/connoisseur/#4-install-connoisseur","text":"Pre-requisite: (caffe and opencv may be the cause) source ~/bisque/bqenv/bin/activate pip install lmdb pyvoro Installation In Bisque top directory unzip the contents of module/connoisseur.zip Activate Env source ~/bisque/bqenv/bin/activate Ensure that caffe and opencv is importable in python Install connoisseur cd ~/bisque/connoisseur pip install -e connoisseur Move the module to Bisque modules mv ~/bisque/connoisseur/modules/Connoisseur/ ~/bisque/modules/ Deactivate Env source ~/bisque/bqenv/bin/deactivate Re/Start the Bisque server $ paver setup bq-admin setup $ bq-admin server start $ bq-admin server stop Make sure the service is available at http://0.0.0.0:8080/services Verify the log in module manager has no errors Thereafter look for the loaded services at the URL http://0.0.0.0:8080/services Look for the service named connoisseur in this list","title":"4. Install Connoisseur"},{"location":"guides/connoisseur/#5-load-connoisseur","text":"Now lets add the connoisseur user group for loading the module Create a connoisseur group Add this group to the user/admin Restart the services $ bq-admin server start bq-admin server stop Load the module as user/admin Open the module from navbar using Analyze- Classification- Connoisseur TODO: Configuration, load model and run a test on Connoisseur","title":"5. Load Connoisseur"},{"location":"guides/errors/","text":"Errors Solution to errors encountered during the setup 1. Port in use error If you see an error like below it may be due to port not being free File /opt/nagare-home/lib/python2.7/site-packages/Paste-1.7.5.1-py2.7.egg/paste/httpserver.py , line 1105, in server_close self.thread_pool.shutdown(60) AttributeError: 'WSGIThreadPoolServer' object has no attribute 'thread_pool' Solution is to kill the process holding the port sudo kill $(sudo lsof -t -i:8080) 2. Import Error on a library Issue: ImportError: libhdf5_serial.so.100: cannot open shared object file: No such file or directory Solution to fix the installed packages pip uninstall h5py pip uninstall tables pip install --no-cache-dir tables","title":"Errors"},{"location":"guides/errors/#errors","text":"Solution to errors encountered during the setup","title":"Errors"},{"location":"guides/errors/#1-port-in-use-error","text":"If you see an error like below it may be due to port not being free File /opt/nagare-home/lib/python2.7/site-packages/Paste-1.7.5.1-py2.7.egg/paste/httpserver.py , line 1105, in server_close self.thread_pool.shutdown(60) AttributeError: 'WSGIThreadPoolServer' object has no attribute 'thread_pool' Solution is to kill the process holding the port sudo kill $(sudo lsof -t -i:8080)","title":"1. Port in use error"},{"location":"guides/errors/#2-import-error-on-a-library","text":"Issue: ImportError: libhdf5_serial.so.100: cannot open shared object file: No such file or directory Solution to fix the installed packages pip uninstall h5py pip uninstall tables pip install --no-cache-dir tables","title":"2. Import Error on a library"},{"location":"guides/hggit/","text":"Version Control Instructions (HG-GIT) Maintain the HG- GIT code push using http://hg-git.github.io Current Release Version: 0.5.10 Instructions :https://til.secretgeek.net/mercurial/convert_hg_to_git.html Source HG :https://biodev.ece.ucsb.edu/hg/bisque-stable Target Git :https://github.com/UCSB-VRL/bisque.git hg- git push Install $ sudo easy_install hg-git $ hg clone --insecure https://biodev.ece.ucsb.edu/hg/bisque-stable Make sure the following is in your .hgrc file: vim ~/.hgrc [git] intree = 1 [extensions] hgext.bookmarks = hggit = [ui] ignore = ~/.hgignore Add a master branch name to the repo and export cd bisque-stable hg bookmark -r default master # so a ref gets created hg gexport --debug # exporting hg commits/objects to git Initialize and prepare the target git repo git init git remote add origin https://github.com/UCSB-VRL/bisque.git git remote -v # verify the remote repo track the large files using LFS https://git-lfs.github.com/ curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash git lfs install git lfs track \"*.ipch\" git lfs track \"*.sdf\" git lfs track \"*.sav\" git lfs track \"*.model\" git add .gitattributes Check the commits using 'git status' and prepare for github git pull origin master git add --all git commit -m \"hg-git from https://biodev.ece.ucsb.edu/hg/bisque-stable/\" git push origin master Current Advice: Stare and compare that the new git repo matches the old hg repo, until you are completely satisfied.","title":"Hggit"},{"location":"guides/hggit/#version-control-instructions-hg-git","text":"Maintain the HG- GIT code push using http://hg-git.github.io Current Release Version: 0.5.10 Instructions :https://til.secretgeek.net/mercurial/convert_hg_to_git.html Source HG :https://biodev.ece.ucsb.edu/hg/bisque-stable Target Git :https://github.com/UCSB-VRL/bisque.git hg- git push Install $ sudo easy_install hg-git $ hg clone --insecure https://biodev.ece.ucsb.edu/hg/bisque-stable Make sure the following is in your .hgrc file: vim ~/.hgrc [git] intree = 1 [extensions] hgext.bookmarks = hggit = [ui] ignore = ~/.hgignore Add a master branch name to the repo and export cd bisque-stable hg bookmark -r default master # so a ref gets created hg gexport --debug # exporting hg commits/objects to git Initialize and prepare the target git repo git init git remote add origin https://github.com/UCSB-VRL/bisque.git git remote -v # verify the remote repo track the large files using LFS https://git-lfs.github.com/ curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash git lfs install git lfs track \"*.ipch\" git lfs track \"*.sdf\" git lfs track \"*.sav\" git lfs track \"*.model\" git add .gitattributes Check the commits using 'git status' and prepare for github git pull origin master git add --all git commit -m \"hg-git from https://biodev.ece.ucsb.edu/hg/bisque-stable/\" git push origin master Current Advice: Stare and compare that the new git repo matches the old hg repo, until you are completely satisfied.","title":"Version Control Instructions (HG-GIT)"},{"location":"guides/jupyter_notebooks/","text":"Reference Video demonstration at Youtube Cyverse Webinar Reference Updated Bisque notebooks on CyVerse github account Data Sample data from AWS S3 Download public/shared data from Cyverse Bisque Download public/shared data from UCSB Biodev Bisque Notebooks Use the three samples from CBIucsb Bitbucket repository as described in the video demonstration 1_upload_images_group.ipynb 2_upload_image_graphical_annotations.ipynb 3_modify_annotations.ipynb General Instructions Install bisque on local and the relevant packages in the virtual env Modify the URL and credentials in the notebooks Download the tif/lsm data from the sample data share above and drop it in relevant folders Go through each step in the notebook to understand and execute them","title":"Jupyter"},{"location":"guides/jupyter_notebooks/#reference","text":"Video demonstration at Youtube Cyverse Webinar Reference Updated Bisque notebooks on CyVerse github account","title":"Reference"},{"location":"guides/jupyter_notebooks/#data","text":"Sample data from AWS S3 Download public/shared data from Cyverse Bisque Download public/shared data from UCSB Biodev Bisque","title":"Data"},{"location":"guides/jupyter_notebooks/#notebooks","text":"Use the three samples from CBIucsb Bitbucket repository as described in the video demonstration 1_upload_images_group.ipynb 2_upload_image_graphical_annotations.ipynb 3_modify_annotations.ipynb","title":"Notebooks"},{"location":"guides/jupyter_notebooks/#general-instructions","text":"Install bisque on local and the relevant packages in the virtual env Modify the URL and credentials in the notebooks Download the tif/lsm data from the sample data share above and drop it in relevant folders Go through each step in the notebook to understand and execute them","title":"General Instructions"},{"location":"guides/resources/module/connoisseur/","text":"Connoisseur Connoisseur is a machine learning service for BisQue which manages ML models and their creation/execution. Currently it only supports a deep learning framework Caffe. Issues to solve long running requests must run as celery tasks locking models is needed to protect binaries from parallel operations transaction during request is bad, need to update status of the resource in the middle of a long running request physical location for models store for templates need to capture output from long running caffe binary hierarchical models models loaded separately in each process/thread and thus will use more RAM and will be slower INFO API GET /connoisseur or GET /connoisseur/info CLASSIFICATION API GET /connoisseur/MODEL_ID/classify:IMAGE_ID GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points] # default, classify uniformly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points_random] # classify randomly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:segmentation] # segment image pixels producing image mask with class labels GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:regions] # partition image into regions producing low-res polygons GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient_uniform] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:image] # Parameters for all methods: points: number of output points, approximate for some methods goodness: % (0-100), minimum goodness for an output point border: % (0-100) of the image size, border in pixels will be estimated from image's dimensions parameters for point classification random and uniform GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:points[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf parameters for region partitioning GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:regions[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf parameters for segmentation GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:ids] GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:colors] formats: png MODEL INFO API GET /connoisseur/MODEL_ID/class:3/sample:1 RESTful API GET - request classification, preview or training Responses: 204 Empty results 400 Bad Request 401 Unauthorized 500 Internal Server Error * 501 Not Implemented MODEL DEFINITION See classifier_model.py CLASSIFIER OUTPUTS [table/gobs] gob type: str | gob label: str | gob vertices: [(x,y,z,...)] | goodness: float | accuracy: float | color: str [image/png] bytes... STATUS WORKFLOW new classes loaded ----- # needs to re-load all files from template and repeat all steps: 3-8 classes filtered | samples loaded ----- # needs to re-split the samples and other steps: 5-8 samples split | trained | validated | finished ------ TRAINING WORKFLOW The model resource is created by the JS UI and initially may only contain a user-given name and a dataset to train on. After the model resource is created in the system several operations may be requested following a typical training sequence. Define required model parameters a. Pick a model template to fine-tune or train from scratch b. This should define a \"mode\", currently point classification but later also fully convolutional Find classes for the model, this will lock the model for current classes and will not allow further changes Create training samples, using parameters defined in the model. This can be repeated to augment the sample list a. Absolute minimum number of samples per class b. Minimum number of samples to activate sample augmentation Train on available samples a. train the model b. validate the model and update classes performance Set classes that will be used for classification based on observed performance in the training step Model is ready for recognition Repeat step 3 and further to improve the model MODEL MODIFICATION API GET /connoisseur/create/template:CaffeNet[/dataset:UUID] GET /connoisseur/MODEL_ID/update[/classes:init][/classes:filter][/samples:init][/samples:update][/samples:split] GET /connoisseur/MODEL_ID/train[/method:finetune][/method:snapshot] GET /connoisseur/MODEL_ID/validate","title":"Connoisseur #"},{"location":"guides/resources/module/connoisseur/#connoisseur","text":"Connoisseur is a machine learning service for BisQue which manages ML models and their creation/execution. Currently it only supports a deep learning framework Caffe.","title":"Connoisseur"},{"location":"guides/resources/module/connoisseur/#issues-to-solve","text":"long running requests must run as celery tasks locking models is needed to protect binaries from parallel operations transaction during request is bad, need to update status of the resource in the middle of a long running request physical location for models store for templates need to capture output from long running caffe binary hierarchical models models loaded separately in each process/thread and thus will use more RAM and will be slower","title":"Issues to solve"},{"location":"guides/resources/module/connoisseur/#info-api","text":"GET /connoisseur or GET /connoisseur/info","title":"INFO API"},{"location":"guides/resources/module/connoisseur/#classification-api","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points] # default, classify uniformly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points_random] # classify randomly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:segmentation] # segment image pixels producing image mask with class labels GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:regions] # partition image into regions producing low-res polygons GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient_uniform] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:image] # Parameters for all methods: points: number of output points, approximate for some methods goodness: % (0-100), minimum goodness for an output point border: % (0-100) of the image size, border in pixels will be estimated from image's dimensions","title":"CLASSIFICATION API"},{"location":"guides/resources/module/connoisseur/#parameters-for-point-classification-random-and-uniform","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:points[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf","title":"parameters for point classification random and uniform"},{"location":"guides/resources/module/connoisseur/#parameters-for-region-partitioning","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:regions[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf","title":"parameters for region partitioning"},{"location":"guides/resources/module/connoisseur/#parameters-for-segmentation","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:ids] GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:colors] formats: png","title":"parameters for segmentation"},{"location":"guides/resources/module/connoisseur/#model-info-api","text":"GET /connoisseur/MODEL_ID/class:3/sample:1","title":"MODEL INFO API"},{"location":"guides/resources/module/connoisseur/#restful-api","text":"GET - request classification, preview or training Responses: 204 Empty results 400 Bad Request 401 Unauthorized 500 Internal Server Error * 501 Not Implemented","title":"RESTful API"},{"location":"guides/resources/module/connoisseur/#model-definition","text":"See classifier_model.py","title":"MODEL DEFINITION"},{"location":"guides/resources/module/connoisseur/#classifier-outputs","text":"[table/gobs] gob type: str | gob label: str | gob vertices: [(x,y,z,...)] | goodness: float | accuracy: float | color: str [image/png] bytes...","title":"CLASSIFIER OUTPUTS"},{"location":"guides/resources/module/connoisseur/#status-workflow","text":"new classes loaded ----- # needs to re-load all files from template and repeat all steps: 3-8 classes filtered | samples loaded ----- # needs to re-split the samples and other steps: 5-8 samples split | trained | validated | finished ------","title":"STATUS WORKFLOW"},{"location":"guides/resources/module/connoisseur/#training-workflow","text":"The model resource is created by the JS UI and initially may only contain a user-given name and a dataset to train on. After the model resource is created in the system several operations may be requested following a typical training sequence. Define required model parameters a. Pick a model template to fine-tune or train from scratch b. This should define a \"mode\", currently point classification but later also fully convolutional Find classes for the model, this will lock the model for current classes and will not allow further changes Create training samples, using parameters defined in the model. This can be repeated to augment the sample list a. Absolute minimum number of samples per class b. Minimum number of samples to activate sample augmentation Train on available samples a. train the model b. validate the model and update classes performance Set classes that will be used for classification based on observed performance in the training step Model is ready for recognition Repeat step 3 and further to improve the model","title":"TRAINING WORKFLOW"},{"location":"guides/resources/module/connoisseur/#model-modification-api","text":"GET /connoisseur/create/template:CaffeNet[/dataset:UUID] GET /connoisseur/MODEL_ID/update[/classes:init][/classes:filter][/samples:init][/samples:update][/samples:split] GET /connoisseur/MODEL_ID/train[/method:finetune][/method:snapshot] GET /connoisseur/MODEL_ID/validate","title":"MODEL MODIFICATION API"}]}