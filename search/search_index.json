{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BisQue Documentation Bisque (Bio-Image Semantic Query User Environment) : Store, visualize, organize and analyze images in the cloud. https://bioimage.ucsb.edu/bisque Project Source Github: https://github.com/UCSB-VRL/bisque Docs : https://UCSB-VRL.github.io/bisque-dev Legacy Bitbucket : https://bitbucket.org/CBIucsb/bisque Wiki : http://biodev.ece.ucsb.edu/projects/bisquik Clone BisQue Explore the source by checking out the bisque-stable branch from the current github repository git clone https://github.com/UCSB-VRL/bisque bisque-stable Bisque Development Environment Artifact and guides Bisque setup from source code Bisque setup in Docker Jupyter Notebooks Common Errors Bisque Module development Module Development Instructions: TODO Bisque Planteome Module Bisque Rancher Deployment Environment Rancher 2.0 with RKE Setup Postgresql Server setup for Bisque","title":"Home"},{"location":"#bisque-documentation","text":"Bisque (Bio-Image Semantic Query User Environment) : Store, visualize, organize and analyze images in the cloud. https://bioimage.ucsb.edu/bisque","title":"BisQue Documentation"},{"location":"#project-source","text":"Github: https://github.com/UCSB-VRL/bisque Docs : https://UCSB-VRL.github.io/bisque-dev Legacy Bitbucket : https://bitbucket.org/CBIucsb/bisque Wiki : http://biodev.ece.ucsb.edu/projects/bisquik","title":"Project Source"},{"location":"#clone-bisque","text":"Explore the source by checking out the bisque-stable branch from the current github repository git clone https://github.com/UCSB-VRL/bisque bisque-stable","title":"Clone BisQue"},{"location":"#bisque-development-environment","text":"Artifact and guides Bisque setup from source code Bisque setup in Docker Jupyter Notebooks Common Errors","title":"Bisque Development Environment"},{"location":"#bisque-module-development","text":"Module Development Instructions: TODO Bisque Planteome Module","title":"Bisque Module development"},{"location":"#bisque-rancher-deployment-environment","text":"Rancher 2.0 with RKE Setup Postgresql Server setup for Bisque","title":"Bisque Rancher Deployment Environment"},{"location":"guides/about/","text":"About Wiki: http://biodev.ece.ucsb.edu/projects/bisquik Source Bitbucket: https://bitbucket.org/CBIucsb/bisque","title":"About"},{"location":"guides/about/#about","text":"Wiki: http://biodev.ece.ucsb.edu/projects/bisquik","title":"About"},{"location":"guides/about/#source","text":"Bitbucket: https://bitbucket.org/CBIucsb/bisque","title":"Source"},{"location":"guides/bisque/","text":"Bisque Development Environment Setup Instructions This guide details the installation of a bisque server on an Ubuntu 16.04 LTS environment. Project Source Github: https://github.com/UCSB-VRL/bisque Developer Installation Bisque Github Pages: https://ucsb-vrl.github.io/bisque-dev/guides/bisque Reference Bique Bioimage Google Groups Instructions on installing bisque using docker Setup for Ubuntu 16.04 Pre-requisites sudo apt-get install -y python python-dev python-virtualenv python-numpy python-scipy sudo apt-get install -y libxml2-dev libxslt1-dev libhdf5-dev sudo apt-get install -y libmysqlclient-dev libpq-dev mercurial git cmake sudo apt-get install -y postgresql postgresql-client libsqlite3-dev sudo apt-get install -y python-paver python-setuptools sudo apt-get install -y graphviz libgraphviz-dev pkg-config sudo apt-get install -y openslide-tools python-openslide sudo apt-get install -y libfftw3-dev libbz2-dev libz-dev sudo apt-get install -y liblcms2-dev libtiff-dev libpng-dev sudo apt-get install -y libgdcm2.6 libopenslide-dev libopenslide0 Install Openjpeg git clone https://github.com/uclouvain/openjpeg cd openjpeg mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release sudo make -j4 install sudo ldconfig Install BioImageConvert BioImageConvert Source Repository Prebuilt Binaries Repository Setup for pre-built binaries (below) # Ubuntu 18 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu18_2.5.0.tar.gz # Ubuntu 16 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu16_2.4.3.tar.gz tar -xvzf imgcnv_ubuntu16_2.4.3.tar.gz sudo cp imgcnv_ubuntu16_2.4.3/imgcnv /usr/local/bin/ sudo cp imgcnv_ubuntu16_2.4.3/libimgcnv.so.2.4.3 /usr/local/lib/ sudo ln -s /usr/local/lib/libimgcnv.so.2.4.3 /usr/local/lib/libimgcnv.so.2.4 sudo ln -s /usr/local/lib/libimgcnv.so.2.4 /usr/local/lib/libimgcnv.so.2 sudo ln -s /usr/local/lib/libimgcnv.so.2 /usr/local/lib/libimgcnv.so sudo ldconfig Alternately, Compile by source and Install (You are on your own here) hg clone --insecure http://biodev.ece.ucsb.edu/hg/imgcnv cd imgcnv make -j6 sudo make install A. Clone the repository and Prepare Virtual Environment We will clone the stable repository and work inside a python virtual environment for setup requirements. git clone https://github.com/UCSB-VRL/bisque.git bisque-stable Its always a good practive to use a virtualenv to develop projects sudo pip install virtualenvwrapper Edit ~/.bashrc and export the following variables # virtualenv and virtualenvwrapper export PATH=/usr/local/bin:$PATH export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python export VIRTUALENVWRAPPER_VIRTUALENV=/usr/local/bin/virtualenv source /usr/local/bin/virtualenvwrapper.sh Now load this using \"source ~/.bashrc\" Create a virtual envrionment \"mkvirtualenv -p /usr/bin/python2 bqdev\" Change environment \"workon bqdev\" Deprecated Bootstrap Installer (Do Not Use this Script) $ mkdir bisque cd bisque $ wget http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable/contrib/bootstrap/bisque-bootstrap.py $ python bisque-bootstrap.py bqenv # Activate Virtualenv bisque$ source bqenv/bin/activate Now Install requirements pip install -i https://biodev.ece.ucsb.edu/py/bisque/xenial/+simple/ -r requirements.txt Alternate Index Url for Development: https://biodev.ece.ucsb.edu/py/bisque/dev/+simple Index Url Debian Stretch: https://biodev.ece.ucsb.edu/py/bisque/stretch/+simple/ Index Url Ubuntu xenial: https://biodev.ece.ucsb.edu/py/bisque/xenial/+simple/ Fix the requirements.txt (Only if the installation fails) #Fix the requirements.txt file using sed -i s/.*psycopg2==2.6.1.*/psycopg2==2.7.1./ requirements.txt psycopg2==2.7.1 Minimatic==1.0 Paste==1.7.5.1 httplib2==0.7.3 #tgext.registration==0.1dev # Install separately since packages may be deprecated in PyPi easy_install http://biodev.ece.ucsb.edu/binaries/depot/tgext.registration2/tgext.registration2-0.5.2.tar.gz B. Configure Bisque Environment Run the Paver setup $ paver setup Expected paver log tail Installing collected packages: bqengine Running setup.py develop for bqengine Successfully installed bqengine Now run: bq-admin setup Run the bq-admin standalone setup $ bq-admin setup Expected bq-admin setup log tail ... Found imgcnv version 2.4.3 Imgcnv is installed and no-precompiled version exists. Using installed version Top level site variables are: bisque.admin_email=YourEmail@YourOrganization bisque.admin_id=admin bisque.organization=Your Organization bisque.paths.root=. bisque.server=http://loup.ece.ucsb.edu:8088 bisque.title=Image Repository Change a site variable [Y]? N ... Running setup_config() from bq.websetup CALLING function install_preferences at 0x7f47f191f848 Initialize your database with: $ bq-admin setup createdb You can start bisque with: $ bq-admin server start then point your browser to: http://loup.ece.ucsb.edu:8088 If you need to shutdown the servers, then use: $ bq-admin server stop You can login as admin and change the default password. Send Installation/Registration report [Y]? N Add \"config/runtime-bisque.cfg\" for module configuration and docker image registry Edit and add run config from config-defaults/runtime-bisque.defaults to config/runtime-bisque.cfg Here are some local changes but requires docker to be setup runtime.platforms = command runtime.staging_base = staging/ [docker] docker.enabled = true # A hub where to push containers to docker.hub = biodev.ece.ucsb.edu:5000 C. Run Bisque Server Start/Stop the server $ bq-admin server start $ bq-admin server stop Overview of Installation (Just for review) workon bqdev paver setup [server|engine] bq-admin setup [server|engine] bq-admin deploy public Open in browser D. Upload Dataset Upload an image file from your local directory to Bisque Browse uploaded dataset in the file explorer Select Browse menu item in the navigation bar and click on dataset Thereafter click on the files tab and click through the local database folder in the left document tree section. Select the uploaded file to view This visualization is by default 2D in nature. The various slices can be stepped through or played using the scroll bar on righ bottom of the image view area A 3D visualization can be explored by clicking the cube icon towards the right of share and delete icons on the tool bar right below the navigation bar. E. Module Load/Install Module packages and dependencies Lets take module/MetaData code as an example. Ensure ~/bisque/config/runtime-bisque.cfg has the configuration as below runtime.staging_base = staging/ docker.hub = biodev.ece.ucsb.edu:5000 Make sure the runtime-module.cfg is as per below module_enabled = True runtime.platforms=command [command] environments = Staged executable = python MetaData.py files = MetaData.py Now build/compile this module cd ~/bisque/modules/MetaData python setup.py Open up the Bisque web interface at http://loup.ece.ucsb.edu:8088 Login using admin:admin Update the email address in the users context menu item (This is important) Open Module Manager from the admin user context menu Paste the following URL in the Module- Engine Module http://loup.ece.ucsb.edu:8088/engine_service/ List of engine modules could be seen Drag-n-Drop say MetaData module from the list to the left window and close the window Run Module Refresh the page, go to Analyse and select Metadata module there Select a single image for test Execute Run command and observe the updates in the results section F. Debug Module Every module run will generate a model execution identifier to track this execution. You can observe this after clicking the run command and you have a result. It could be success or fail in the Results section http://loup.ece.ucsb.edu:8088/module_service/MetaData/?wpublic=1#mex=http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF The mex identifier (==MEX_ID=00-ZmuAoE43wTxByycmaCnvBF==) observed from URL can be used to locate this execution code resulting logs in the staging folders change directory to this staging folder and observe the files there cd ~/bisque/staging/00-ZmuAoE43wTxByycmaCnvBF rahul@loup:~/bisque/staging/00-ZmuAoE43wTxByycmaCnvBF$ tree . \u251c\u2500\u2500 MetaData.log \u251c\u2500\u2500 MetaData.py \u251c\u2500\u2500 python.log \u2514\u2500\u2500 state00-ZmuAoE43wTxByycmaCnvBF.bq 0 directories, 4 files Here we can see that the MetaData.log and python.log files are generated In case of issues these are the log files that we need to look into for detailed error reporting main log file is the bisque_8088.log file at the ~/bisque home directory 13:45:11,996 INFO [bq.root] [admin] POST http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF?view=short 13:45:12,001 INFO [bq.module_server] MEX APPEND http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF?view=short 13:45:12,205 INFO [bq.engine_service.command_run] SUCCESS Command python MetaData.py http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF admin:00-ZmuAoE43wTxByycmaCnvBF with 0 13:45:12,206 INFO [bq.engine_service.command_run] All processes have returned [ finished ] 13:45:12,206 INFO [bq.engine_service.command_run] finishing 1 mexes - [{ files : [ MetaData.py ], status : finished , executable : [ python , MetaData.py , http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh , http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF , admin:00-ZmuAoE43wTxByycmaCnvBF ], initial_dir : /home/rahul/repository/github/bisque-dev , module_enabled : True , named_args : { bisque_token : admin:00-ZmuAoE43wTxByycmaCnvBF , image_url : http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh , mex_url : http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF }, bisque_token : u admin:00-ZmuAoE43wTxByycmaCnvBF , environments : Staged , runtime.staging_base : staging/ , mex_id : 00-ZmuAoE43wTxByycmaCnvBF , runtime.matlab_launcher : config-defaults/templates/matlab_launcher_SYS.tmpl , arguments : [], module_dir : /home/rahul/repository/github/bisque-dev , staging_path : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF , iterables : False, log_name : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF/python.log , runtime.matlab_home : , mex_url : http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF , rundir : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF , runtime.platforms : command }] This will show us the COMMAND that was run to execute this module with MEX_URL python MetaData.py \\ http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh \\ http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF \\ admin:00-ZmuAoE43wTxByycmaCnvBF This is the command that we can use to debug and rerun/replay this MEX in order to update the execution result. This is different from running the module again from the web interface since that will produce a new MEX_ID and create another staging folder corresponding to that. G. Module MEX/UI Click on Browse- mex to go to the Model execution page Here we will be able to see the various MEX sessions that were run for all modules under execution We will be able to obtain the MEX_ID and view its state from here by selecting one of the executions of interest. In case we select a module that has errors in it then it would turn out to show the status with messages in red Now let us go to Browse- dataset and select the file that we ran the module for. Here we will be able to view the Annotation that was added by the Metadata module to this image. Annotation can be visualized using the \"Annotations\" tab on the right The Model execution runs can be seen in the \"Analysis\" tab on the right Further Docker Condor based modules require additional setup NEXT Install Docker (https://docs.docker.com/install/linux/docker-ce/ubuntu/) Also go through the post-installation steps Also install the Nvidia-docker in case you are planning to use GPU based modules or connoisseur services. Install Condor (https://research.cs.wisc.edu/htcondor/ubuntu/) Understand the configurations as well for master slave setup Bisque Condor Setup Instructions rahul@bqdev:~$ /etc/init.d/condor restart [ ok ] Restarting condor (via systemctl): condor.service. rahul@bqdev:~$ ps -ef | grep condor","title":"BisQue"},{"location":"guides/bisque/#bisque-development-environment-setup-instructions","text":"This guide details the installation of a bisque server on an Ubuntu 16.04 LTS environment. Project Source Github: https://github.com/UCSB-VRL/bisque Developer Installation Bisque Github Pages: https://ucsb-vrl.github.io/bisque-dev/guides/bisque Reference Bique Bioimage Google Groups Instructions on installing bisque using docker","title":"Bisque Development Environment Setup Instructions"},{"location":"guides/bisque/#setup-for-ubuntu-1604","text":"","title":"Setup for Ubuntu 16.04"},{"location":"guides/bisque/#pre-requisites","text":"sudo apt-get install -y python python-dev python-virtualenv python-numpy python-scipy sudo apt-get install -y libxml2-dev libxslt1-dev libhdf5-dev sudo apt-get install -y libmysqlclient-dev libpq-dev mercurial git cmake sudo apt-get install -y postgresql postgresql-client libsqlite3-dev sudo apt-get install -y python-paver python-setuptools sudo apt-get install -y graphviz libgraphviz-dev pkg-config sudo apt-get install -y openslide-tools python-openslide sudo apt-get install -y libfftw3-dev libbz2-dev libz-dev sudo apt-get install -y liblcms2-dev libtiff-dev libpng-dev sudo apt-get install -y libgdcm2.6 libopenslide-dev libopenslide0 Install Openjpeg git clone https://github.com/uclouvain/openjpeg cd openjpeg mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release sudo make -j4 install sudo ldconfig Install BioImageConvert BioImageConvert Source Repository Prebuilt Binaries Repository Setup for pre-built binaries (below) # Ubuntu 18 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu18_2.5.0.tar.gz # Ubuntu 16 version wget https://bitbucket.org/dimin/bioimageconvert/downloads/imgcnv_ubuntu16_2.4.3.tar.gz tar -xvzf imgcnv_ubuntu16_2.4.3.tar.gz sudo cp imgcnv_ubuntu16_2.4.3/imgcnv /usr/local/bin/ sudo cp imgcnv_ubuntu16_2.4.3/libimgcnv.so.2.4.3 /usr/local/lib/ sudo ln -s /usr/local/lib/libimgcnv.so.2.4.3 /usr/local/lib/libimgcnv.so.2.4 sudo ln -s /usr/local/lib/libimgcnv.so.2.4 /usr/local/lib/libimgcnv.so.2 sudo ln -s /usr/local/lib/libimgcnv.so.2 /usr/local/lib/libimgcnv.so sudo ldconfig Alternately, Compile by source and Install (You are on your own here) hg clone --insecure http://biodev.ece.ucsb.edu/hg/imgcnv cd imgcnv make -j6 sudo make install","title":"Pre-requisites"},{"location":"guides/bisque/#a-clone-the-repository-and-prepare-virtual-environment","text":"We will clone the stable repository and work inside a python virtual environment for setup requirements. git clone https://github.com/UCSB-VRL/bisque.git bisque-stable Its always a good practive to use a virtualenv to develop projects sudo pip install virtualenvwrapper Edit ~/.bashrc and export the following variables # virtualenv and virtualenvwrapper export PATH=/usr/local/bin:$PATH export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python export VIRTUALENVWRAPPER_VIRTUALENV=/usr/local/bin/virtualenv source /usr/local/bin/virtualenvwrapper.sh Now load this using \"source ~/.bashrc\" Create a virtual envrionment \"mkvirtualenv -p /usr/bin/python2 bqdev\" Change environment \"workon bqdev\"","title":"A. Clone the repository and Prepare Virtual Environment"},{"location":"guides/bisque/#deprecated-bootstrap-installer-do-not-use-this-script","text":"$ mkdir bisque cd bisque $ wget http://biodev.ece.ucsb.edu/projects/bisquik/export/tip/bisque-stable/contrib/bootstrap/bisque-bootstrap.py $ python bisque-bootstrap.py bqenv # Activate Virtualenv bisque$ source bqenv/bin/activate Now Install requirements pip install -i https://biodev.ece.ucsb.edu/py/bisque/xenial/+simple/ -r requirements.txt Alternate Index Url for Development: https://biodev.ece.ucsb.edu/py/bisque/dev/+simple Index Url Debian Stretch: https://biodev.ece.ucsb.edu/py/bisque/stretch/+simple/ Index Url Ubuntu xenial: https://biodev.ece.ucsb.edu/py/bisque/xenial/+simple/ Fix the requirements.txt (Only if the installation fails) #Fix the requirements.txt file using sed -i s/.*psycopg2==2.6.1.*/psycopg2==2.7.1./ requirements.txt psycopg2==2.7.1 Minimatic==1.0 Paste==1.7.5.1 httplib2==0.7.3 #tgext.registration==0.1dev # Install separately since packages may be deprecated in PyPi easy_install http://biodev.ece.ucsb.edu/binaries/depot/tgext.registration2/tgext.registration2-0.5.2.tar.gz","title":"Deprecated Bootstrap Installer (Do Not Use this Script)"},{"location":"guides/bisque/#b-configure-bisque-environment","text":"Run the Paver setup $ paver setup Expected paver log tail Installing collected packages: bqengine Running setup.py develop for bqengine Successfully installed bqengine Now run: bq-admin setup Run the bq-admin standalone setup $ bq-admin setup Expected bq-admin setup log tail ... Found imgcnv version 2.4.3 Imgcnv is installed and no-precompiled version exists. Using installed version Top level site variables are: bisque.admin_email=YourEmail@YourOrganization bisque.admin_id=admin bisque.organization=Your Organization bisque.paths.root=. bisque.server=http://loup.ece.ucsb.edu:8088 bisque.title=Image Repository Change a site variable [Y]? N ... Running setup_config() from bq.websetup CALLING function install_preferences at 0x7f47f191f848 Initialize your database with: $ bq-admin setup createdb You can start bisque with: $ bq-admin server start then point your browser to: http://loup.ece.ucsb.edu:8088 If you need to shutdown the servers, then use: $ bq-admin server stop You can login as admin and change the default password. Send Installation/Registration report [Y]? N Add \"config/runtime-bisque.cfg\" for module configuration and docker image registry Edit and add run config from config-defaults/runtime-bisque.defaults to config/runtime-bisque.cfg Here are some local changes but requires docker to be setup runtime.platforms = command runtime.staging_base = staging/ [docker] docker.enabled = true # A hub where to push containers to docker.hub = biodev.ece.ucsb.edu:5000","title":"B. Configure Bisque Environment"},{"location":"guides/bisque/#c-run-bisque-server","text":"Start/Stop the server $ bq-admin server start $ bq-admin server stop Overview of Installation (Just for review) workon bqdev paver setup [server|engine] bq-admin setup [server|engine] bq-admin deploy public Open in browser","title":"C. Run Bisque Server"},{"location":"guides/bisque/#d-upload-dataset","text":"Upload an image file from your local directory to Bisque Browse uploaded dataset in the file explorer Select Browse menu item in the navigation bar and click on dataset Thereafter click on the files tab and click through the local database folder in the left document tree section. Select the uploaded file to view This visualization is by default 2D in nature. The various slices can be stepped through or played using the scroll bar on righ bottom of the image view area A 3D visualization can be explored by clicking the cube icon towards the right of share and delete icons on the tool bar right below the navigation bar.","title":"D. Upload Dataset"},{"location":"guides/bisque/#e-module","text":"","title":"E. Module"},{"location":"guides/bisque/#loadinstall-module-packages-and-dependencies","text":"Lets take module/MetaData code as an example. Ensure ~/bisque/config/runtime-bisque.cfg has the configuration as below runtime.staging_base = staging/ docker.hub = biodev.ece.ucsb.edu:5000 Make sure the runtime-module.cfg is as per below module_enabled = True runtime.platforms=command [command] environments = Staged executable = python MetaData.py files = MetaData.py Now build/compile this module cd ~/bisque/modules/MetaData python setup.py Open up the Bisque web interface at http://loup.ece.ucsb.edu:8088 Login using admin:admin Update the email address in the users context menu item (This is important) Open Module Manager from the admin user context menu Paste the following URL in the Module- Engine Module http://loup.ece.ucsb.edu:8088/engine_service/ List of engine modules could be seen Drag-n-Drop say MetaData module from the list to the left window and close the window","title":"Load/Install Module packages and dependencies"},{"location":"guides/bisque/#run-module","text":"Refresh the page, go to Analyse and select Metadata module there Select a single image for test Execute Run command and observe the updates in the results section","title":"Run Module"},{"location":"guides/bisque/#f-debug-module","text":"Every module run will generate a model execution identifier to track this execution. You can observe this after clicking the run command and you have a result. It could be success or fail in the Results section http://loup.ece.ucsb.edu:8088/module_service/MetaData/?wpublic=1#mex=http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF The mex identifier (==MEX_ID=00-ZmuAoE43wTxByycmaCnvBF==) observed from URL can be used to locate this execution code resulting logs in the staging folders change directory to this staging folder and observe the files there cd ~/bisque/staging/00-ZmuAoE43wTxByycmaCnvBF rahul@loup:~/bisque/staging/00-ZmuAoE43wTxByycmaCnvBF$ tree . \u251c\u2500\u2500 MetaData.log \u251c\u2500\u2500 MetaData.py \u251c\u2500\u2500 python.log \u2514\u2500\u2500 state00-ZmuAoE43wTxByycmaCnvBF.bq 0 directories, 4 files Here we can see that the MetaData.log and python.log files are generated In case of issues these are the log files that we need to look into for detailed error reporting main log file is the bisque_8088.log file at the ~/bisque home directory 13:45:11,996 INFO [bq.root] [admin] POST http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF?view=short 13:45:12,001 INFO [bq.module_server] MEX APPEND http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF?view=short 13:45:12,205 INFO [bq.engine_service.command_run] SUCCESS Command python MetaData.py http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF admin:00-ZmuAoE43wTxByycmaCnvBF with 0 13:45:12,206 INFO [bq.engine_service.command_run] All processes have returned [ finished ] 13:45:12,206 INFO [bq.engine_service.command_run] finishing 1 mexes - [{ files : [ MetaData.py ], status : finished , executable : [ python , MetaData.py , http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh , http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF , admin:00-ZmuAoE43wTxByycmaCnvBF ], initial_dir : /home/rahul/repository/github/bisque-dev , module_enabled : True , named_args : { bisque_token : admin:00-ZmuAoE43wTxByycmaCnvBF , image_url : http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh , mex_url : http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF }, bisque_token : u admin:00-ZmuAoE43wTxByycmaCnvBF , environments : Staged , runtime.staging_base : staging/ , mex_id : 00-ZmuAoE43wTxByycmaCnvBF , runtime.matlab_launcher : config-defaults/templates/matlab_launcher_SYS.tmpl , arguments : [], module_dir : /home/rahul/repository/github/bisque-dev , staging_path : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF , iterables : False, log_name : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF/python.log , runtime.matlab_home : , mex_url : http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF , rundir : /home/rahul/repository/github/bisque-dev/staging/00-ZmuAoE43wTxByycmaCnvBF , runtime.platforms : command }] This will show us the COMMAND that was run to execute this module with MEX_URL python MetaData.py \\ http://loup.ece.ucsb.edu:8088/data_service/00-6dpWgEAue2iz8YZcKPHejh \\ http://loup.ece.ucsb.edu:8088/module_service/mex/00-ZmuAoE43wTxByycmaCnvBF \\ admin:00-ZmuAoE43wTxByycmaCnvBF This is the command that we can use to debug and rerun/replay this MEX in order to update the execution result. This is different from running the module again from the web interface since that will produce a new MEX_ID and create another staging folder corresponding to that.","title":"F. Debug Module"},{"location":"guides/bisque/#g-module-mexui","text":"Click on Browse- mex to go to the Model execution page Here we will be able to see the various MEX sessions that were run for all modules under execution We will be able to obtain the MEX_ID and view its state from here by selecting one of the executions of interest. In case we select a module that has errors in it then it would turn out to show the status with messages in red Now let us go to Browse- dataset and select the file that we ran the module for. Here we will be able to view the Annotation that was added by the Metadata module to this image. Annotation can be visualized using the \"Annotations\" tab on the right The Model execution runs can be seen in the \"Analysis\" tab on the right","title":"G. Module MEX/UI"},{"location":"guides/bisque/#further","text":"","title":"Further"},{"location":"guides/bisque/#docker-condor-based-modules-require-additional-setup","text":"NEXT Install Docker (https://docs.docker.com/install/linux/docker-ce/ubuntu/) Also go through the post-installation steps Also install the Nvidia-docker in case you are planning to use GPU based modules or connoisseur services. Install Condor (https://research.cs.wisc.edu/htcondor/ubuntu/) Understand the configurations as well for master slave setup Bisque Condor Setup Instructions rahul@bqdev:~$ /etc/init.d/condor restart [ ok ] Restarting condor (via systemctl): condor.service. rahul@bqdev:~$ ps -ef | grep condor","title":"Docker &amp; Condor based modules require additional setup"},{"location":"guides/bisque_docker/","text":"Bisque Docker Environment Setup Instructions Docker/Project Source Docker Hub Bisque Dev Image UCSB-VRL/bisque-stable Github Bique Bioimage Google Groups Installation Pre-requisite Install docker (light weight VM) on you laptop: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Instructions on installing bisque using docker https://github.com/UCSB-VRL/bisque/README.md Create the Docker Image Run Environment Setup folders pull code mkdir ws cd ws git clone https://github.com/UCSB-VRL/bisque mkdir container-modules container-data container-config cp -r bisque/modules/* container-modules/ Start Docker and mount directories for run # Docker Run docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ vishwakarmarhl/ucsb-bisque05-svc:dev Check Container state bisque@ubuntu:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a1162677d66a vishwakarmarhl/ucsb-bisque05-svc:dev /builder/run-bisque\u2026 13 minutes ago Up 13 minutes 0.0.0.0:8080- 8080/tcp, 0.0.0.0:27000- 27000/tcp bisque-dev Stop Docker docker stop $(docker ps -a -q --filter ancestor=vishwakarmarhl/ucsb-bisque05-svc:dev --format= {{.ID}} ) Advanced environment for working with database URI as, sqlite:///data/bisque.db postgresql://rahul:rahul@localhost/bqmurks58 or postgresql://dbhost:5432/bisque docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ -e BISQUE_DBURL=postgresql://rahul:rahul@localhost/bqmurks58 \\ vishwakarmarhl/ucsb-bisque05-svc:dev This will start the bisque docker/server Client Service http://0.0.0.0:8080 Engine Service http://0.0.0.0:8080/engine_service Module Service http://0.0.0.0:8080/module_service Features http://0.0.0.0:8080/features Use \"admin:admin\" to authenticate into this local environment To manipulate and check logs, connect to docker bash bisque-host$ docker exec -t -i bisque-dev /bin/bash Develop in Docker (TODO !!) Use the dev docker image docker vishwakarmarhl/ucsb-bisque05-svc Create the workspace and prepare the development setup cd ~/ws git clone https://github.com/UCSB-VRL/bisque mkdir container-modules container-data container-config cp -r bisque/modules/* container-modules/ Create the container docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ vishwakarmarhl/ucsb-bisque05-svc:dev Bash into the docker container and develop docker exec -it bisque-dev /bin/bash Activate environment and Test/Build a Module (TODO !! python-dev) root@9362d1f8bf12:/source# cd modules/MetaData root@9362d1f8bf12:/source/modules/MetaData# source /usr/lib/bisque/bin/activate root@9362d1f8bf12:/source/modules/MetaData# python setup.py Verify Bisque Services Bisque Client interface at http://0.0.0.0:8080/client_service Bisque Engine service at http://0.0.0.0:8080/engine_service Now you can add the Metadata module from the manager interface and run a test","title":"Docker"},{"location":"guides/bisque_docker/#bisque-docker-environment-setup-instructions","text":"","title":"Bisque Docker Environment Setup Instructions"},{"location":"guides/bisque_docker/#dockerproject-source","text":"Docker Hub Bisque Dev Image UCSB-VRL/bisque-stable Github Bique Bioimage Google Groups","title":"Docker/Project Source"},{"location":"guides/bisque_docker/#installation","text":"","title":"Installation"},{"location":"guides/bisque_docker/#pre-requisite","text":"Install docker (light weight VM) on you laptop: https://docs.docker.com/install/linux/docker-ce/ubuntu/ Instructions on installing bisque using docker https://github.com/UCSB-VRL/bisque/README.md","title":"Pre-requisite"},{"location":"guides/bisque_docker/#create-the-docker-image","text":"","title":"Create the Docker Image"},{"location":"guides/bisque_docker/#run-environment","text":"Setup folders pull code mkdir ws cd ws git clone https://github.com/UCSB-VRL/bisque mkdir container-modules container-data container-config cp -r bisque/modules/* container-modules/ Start Docker and mount directories for run # Docker Run docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ vishwakarmarhl/ucsb-bisque05-svc:dev Check Container state bisque@ubuntu:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a1162677d66a vishwakarmarhl/ucsb-bisque05-svc:dev /builder/run-bisque\u2026 13 minutes ago Up 13 minutes 0.0.0.0:8080- 8080/tcp, 0.0.0.0:27000- 27000/tcp bisque-dev Stop Docker docker stop $(docker ps -a -q --filter ancestor=vishwakarmarhl/ucsb-bisque05-svc:dev --format= {{.ID}} ) Advanced environment for working with database URI as, sqlite:///data/bisque.db postgresql://rahul:rahul@localhost/bqmurks58 or postgresql://dbhost:5432/bisque docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ -e BISQUE_DBURL=postgresql://rahul:rahul@localhost/bqmurks58 \\ vishwakarmarhl/ucsb-bisque05-svc:dev This will start the bisque docker/server Client Service http://0.0.0.0:8080 Engine Service http://0.0.0.0:8080/engine_service Module Service http://0.0.0.0:8080/module_service Features http://0.0.0.0:8080/features Use \"admin:admin\" to authenticate into this local environment To manipulate and check logs, connect to docker bash bisque-host$ docker exec -t -i bisque-dev /bin/bash","title":"Run Environment"},{"location":"guides/bisque_docker/#develop-in-docker-todo","text":"Use the dev docker image docker vishwakarmarhl/ucsb-bisque05-svc Create the workspace and prepare the development setup cd ~/ws git clone https://github.com/UCSB-VRL/bisque mkdir container-modules container-data container-config cp -r bisque/modules/* container-modules/ Create the container docker run --name bisque-dev --rm -p 8080:8080 -p 27000:27000 \\ -v $(pwd)/container-modules:/source/modules \\ -v $(pwd)/container-data:/source/data \\ -v $(pwd)/container-config:/source/config \\ vishwakarmarhl/ucsb-bisque05-svc:dev Bash into the docker container and develop docker exec -it bisque-dev /bin/bash Activate environment and Test/Build a Module (TODO !! python-dev) root@9362d1f8bf12:/source# cd modules/MetaData root@9362d1f8bf12:/source/modules/MetaData# source /usr/lib/bisque/bin/activate root@9362d1f8bf12:/source/modules/MetaData# python setup.py Verify Bisque Services Bisque Client interface at http://0.0.0.0:8080/client_service Bisque Engine service at http://0.0.0.0:8080/engine_service Now you can add the Metadata module from the manager interface and run a test","title":"Develop in Docker (TODO !!)"},{"location":"guides/bisque_module/","text":"Module Development Guide/Resources Developer Reference: https://biodev.ece.ucsb.edu/projects/bisquik/wiki/Developer Module Reference: https://biodev.ece.ucsb.edu/projects/bisquik/wiki/Developer/ModuleSystem Container organization and plan for rancher setup Bisque Production Rancher: http://saw.ece.ucsb.edu:8080 Server Containers Instance Name Host or IP Image Name Remarks BisQue Server http://dough.ece.ucsb.edu biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Main BisQue service with Connoisseur condor-nodes condor.master biodev.ece.ucsb.edu:5000/condor One master and 4 worker nodes Module Containers Instance Name Host or IP Image Name Remarks Dream3D IP ADDR biodev.ece.ucsb.edu:5000/bisque_dream3d Dream3D Module Predict Strength IP ADDR biodev.ece.ucsb.edu:5000/predict_strength Predict Strength Module Cell Segment 3D Unet IP ADDR biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2 3D Cell Segmentation There is another load balancer or haproxy to route the traffic based on IP rules LetsEncrypt certificates are used for encrypted traffic A.) Develop Module CellSegment3DUnet (PyTorch) We describe the module which takes a 3D cell image in TIFF format as input and generates segmentation mask for it. Overall Module code root@karma:/module# tree -L 1 . |-- CellSegment3DUnet.xml |-- Dockerfile |-- PythonScript.log |-- PythonScriptWrapper |-- PythonScriptWrapper.py |-- PythonScriptWrapper.spec |-- README.md |-- build/ |-- module.log |-- public/ |-- pydist/ |-- runtime-module.cfg |-- scriptrun.log |-- setup.py `-- source/ Describe each section of the module Developing the Module in Docker This module is used for segmenting the 3D image using UNet Pytorch based model. We will build a container to test, develop and deploy the module. Build Docker Image docker build -t biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest . -f Dockerfile docker tag $(docker images -q \"biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest\") biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest docker push biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest Run container and bash nvidia-docker run -it --ipc=host -v $(pwd):/module biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest bash The docker run, creates a container and then connect to its bash B.) Module Deploy/Execution Extracts from the ~/staging/**/docker_run.log execution log Updated engine service to include --ipc=host parameter in the launcher template (DOCKER_RUN) at bq.engine.controllers/docker_env.py docker create --ipc=host biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2 \\ python PythonScriptWrapper.py \\ http://drishti.ece.ucsb.edu:8080/data_service/00-GFmjehgjqfqQi5CdXsAbiC \\ 15 0.05 \\ http://drishti.ece.ucsb.edu:8080/module_service/mex/00-ZRwn68oz8CRhf2n9oXA9za \\ admin:00-ZRwn68oz8CRhf2n9oXA9za 9eb7d1be403ca77b6cdc5c2140d289c2ee1692e736fe39abc0bf1fd798a530f9 (Returns an identifier for this instance) Eventually this is the code that is running inside the container at runtime python PythonScriptWrapper.py \\ http://drishti.ece.ucsb.edu:8080/data_service/00-kDwj3vQq83vJA6SvVvVVh8 \\ 15 0.05 \\ http://drishti.ece.ucsb.edu:8080/module_service/mex/00-RNeG4KEKJUQPXboPEbt63S \\ admin:00-RNeG4KEKJUQPXboPEbt63S tail -f PythonScript.log Run based on the identifier for that instance docker start 9eb7d1be403ca77b6cdc5c2140d289c2ee1692e736fe39abc0bf1fd798a530f9 docker wait 9eb7d1be403ca77b6cdc5c2140d289c2ee1692e736fe39abc0bf1fd798a530f9 C.) Integrate with Python 3 codebase/module or external clients Use a Python 3 wheels build of the bisque-api==0.5.9 package BQAPI https://setuptools.readthedocs.io/en/latest/setuptools.html#distributing-a-setuptools-based-project cd ~/bisque/bqapi python3 -m pip install --user --upgrade setuptools wheel python3 setup.py sdist bdist_wheel This will create the whl file in dist folder which can be installed using python3 -m pip install dist/bisque_api-0.5.9-py2.py3-none-any.whl In case the bqapi code is not portable to Python 3 easily. We can use the 2to3.5 CLI for migration. This will update the files with Python 3 syntax in place and move the legacy code to a *.bak file upon update 2to3.5 -w *.py Ideally we should be able to create a new bisque-api-py3 and push it to the packages respository at https://biodev.ece.ucsb.edu/py/bisque/prod","title":"Bisque module"},{"location":"guides/bisque_module/#module-development-guideresources","text":"Developer Reference: https://biodev.ece.ucsb.edu/projects/bisquik/wiki/Developer Module Reference: https://biodev.ece.ucsb.edu/projects/bisquik/wiki/Developer/ModuleSystem","title":"Module Development Guide/Resources"},{"location":"guides/bisque_module/#container-organization-and-plan-for-rancher-setup","text":"Bisque Production Rancher: http://saw.ece.ucsb.edu:8080","title":"Container organization and plan for rancher setup"},{"location":"guides/bisque_module/#server-containers","text":"Instance Name Host or IP Image Name Remarks BisQue Server http://dough.ece.ucsb.edu biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Main BisQue service with Connoisseur condor-nodes condor.master biodev.ece.ucsb.edu:5000/condor One master and 4 worker nodes","title":"Server Containers"},{"location":"guides/bisque_module/#module-containers","text":"Instance Name Host or IP Image Name Remarks Dream3D IP ADDR biodev.ece.ucsb.edu:5000/bisque_dream3d Dream3D Module Predict Strength IP ADDR biodev.ece.ucsb.edu:5000/predict_strength Predict Strength Module Cell Segment 3D Unet IP ADDR biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2 3D Cell Segmentation There is another load balancer or haproxy to route the traffic based on IP rules LetsEncrypt certificates are used for encrypted traffic","title":"Module Containers"},{"location":"guides/bisque_module/#a-develop-module-cellsegment3dunet-pytorch","text":"We describe the module which takes a 3D cell image in TIFF format as input and generates segmentation mask for it.","title":"A.) Develop Module CellSegment3DUnet (PyTorch)"},{"location":"guides/bisque_module/#overall-module-code","text":"root@karma:/module# tree -L 1 . |-- CellSegment3DUnet.xml |-- Dockerfile |-- PythonScript.log |-- PythonScriptWrapper |-- PythonScriptWrapper.py |-- PythonScriptWrapper.spec |-- README.md |-- build/ |-- module.log |-- public/ |-- pydist/ |-- runtime-module.cfg |-- scriptrun.log |-- setup.py `-- source/ Describe each section of the module","title":"Overall Module code"},{"location":"guides/bisque_module/#developing-the-module-in-docker","text":"This module is used for segmenting the 3D image using UNet Pytorch based model. We will build a container to test, develop and deploy the module.","title":"Developing the Module in Docker"},{"location":"guides/bisque_module/#build-docker-image","text":"docker build -t biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest . -f Dockerfile docker tag $(docker images -q \"biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest\") biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest docker push biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest","title":"Build Docker Image"},{"location":"guides/bisque_module/#run-container-and-bash","text":"nvidia-docker run -it --ipc=host -v $(pwd):/module biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2:latest bash The docker run, creates a container and then connect to its bash","title":"Run container and bash"},{"location":"guides/bisque_module/#b-module-deployexecution","text":"Extracts from the ~/staging/**/docker_run.log execution log Updated engine service to include --ipc=host parameter in the launcher template (DOCKER_RUN) at bq.engine.controllers/docker_env.py docker create --ipc=host biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2 \\ python PythonScriptWrapper.py \\ http://drishti.ece.ucsb.edu:8080/data_service/00-GFmjehgjqfqQi5CdXsAbiC \\ 15 0.05 \\ http://drishti.ece.ucsb.edu:8080/module_service/mex/00-ZRwn68oz8CRhf2n9oXA9za \\ admin:00-ZRwn68oz8CRhf2n9oXA9za 9eb7d1be403ca77b6cdc5c2140d289c2ee1692e736fe39abc0bf1fd798a530f9 (Returns an identifier for this instance) Eventually this is the code that is running inside the container at runtime python PythonScriptWrapper.py \\ http://drishti.ece.ucsb.edu:8080/data_service/00-kDwj3vQq83vJA6SvVvVVh8 \\ 15 0.05 \\ http://drishti.ece.ucsb.edu:8080/module_service/mex/00-RNeG4KEKJUQPXboPEbt63S \\ admin:00-RNeG4KEKJUQPXboPEbt63S tail -f PythonScript.log","title":"B.) Module Deploy/Execution"},{"location":"guides/bisque_module/#run-based-on-the-identifier-for-that-instance","text":"docker start 9eb7d1be403ca77b6cdc5c2140d289c2ee1692e736fe39abc0bf1fd798a530f9 docker wait 9eb7d1be403ca77b6cdc5c2140d289c2ee1692e736fe39abc0bf1fd798a530f9","title":"Run based on the identifier for that instance"},{"location":"guides/bisque_module/#c-integrate-with-python-3-codebasemodule-or-external-clients","text":"Use a Python 3 wheels build of the bisque-api==0.5.9 package BQAPI https://setuptools.readthedocs.io/en/latest/setuptools.html#distributing-a-setuptools-based-project cd ~/bisque/bqapi python3 -m pip install --user --upgrade setuptools wheel python3 setup.py sdist bdist_wheel This will create the whl file in dist folder which can be installed using python3 -m pip install dist/bisque_api-0.5.9-py2.py3-none-any.whl In case the bqapi code is not portable to Python 3 easily. We can use the 2to3.5 CLI for migration. This will update the files with Python 3 syntax in place and move the legacy code to a *.bak file upon update 2to3.5 -w *.py Ideally we should be able to create a new bisque-api-py3 and push it to the packages respository at https://biodev.ece.ucsb.edu/py/bisque/prod","title":"C.) Integrate with Python 3 codebase/module or external clients"},{"location":"guides/bisque_module_planteome/","text":"PLANTEOME DEEP SEGMENT ANALYSIS MODULE This module segments a marked object (creating a graphical object) within an input image. Then the module will classify either the entire original image or the segment created in the first step. This uses PyTorch in order to do this deep segmentation. Reference Module Development Guide: https://github.com/pndaly/BisQue_Platform_Guide Sample Deep Learning Module: Planteome Deep Segment Analysis Pre-requisite: link Working bisque environment at http://loup.ece.ucsb.edu:8088/ Docker should be enabled and setup on this environment The bisque deployment server should have access to good CPU RAM since Torch Scikit-image is used Access to module folder for deploying this module (Say at ~/bisque/module) Add/Deploy the Planteome module Identify the bisque module folder at path ~/bisque/module Login to http://loup.ece.ucsb.edu:8088/ with credentials admin:admin Download the Planteome module to the bisque module folder Register the module to Bisque by opening the Module Manager Provide the engine URL(http://loup.ece.ucsb.edu:8088/engine_service) in the Engine service section and click load. This will list the modules by querying the engine service. Select the module named \"Planteome\", drag this module to the left pane and drop it to register. Build/Configure the module Edit the runtime-module.cfg with relevant configuration Changed the docker.hub configuration to biodev.ece.ucsb.edu:5000 View/Edit the Dockerfile for the relevant packages Edited versions for numpy\\==1.16.1 scikit-image\\==0.14.2 Source to your bisque python environment \"workon bqenv\" for installation pip install -r requirements.txt python setup.py This will install all the dependency in the modules requirements.txt file, build modules code, and create a docker image for running it. Based on your \"docker.hub\" configuration the setup will push the docker image to registry as [biodev.ece.ucsb.edu:5000/bisque_uplanteome] Steps to run the algorithm from Bisque Client Select an image to be analyzed. The foreground/background annotation tooltip can be found on the top-right of the module image viewer. Mark the part of the image to be segmented with foreground line(s) annotation(s). Mark the image with background annotations around the object to be segmented. Select which deep classifier to use, whether to segment the image, the segmentation quality, and whether to classify the entire image or the segmented object instead. Press the 'RUN' button. Analysis may take some time depending on the image and segmentation quality. Results are given in visual and table formats, depending on whether the segmentation and classification functionalities respectively were enabled in the options. Isolated/Development test setup Make sure you have annotated the image and have an a mex identifier availablefor manual test/run. This can be done by, - Opening the module and select image - Annotate the image as per the directions above - Configure and run the module once - Make note of the mex URL for this run by looking at the docker_run.log in the staging folder. This will be used for replaying the test run from the modules folder. Additional module execution information - When we annotate the image and click RUN on the module user interface - The Planteome module is created from biodev.ece.ucsb.edu:5000/bisque_uplanteome:latest ``` docker create biodev.ece.ucsb.edu:5000/bisque_uplanteome \\ python PlanteomeDeepSegment.py \\ http://loup.ece.ucsb.edu:8088/module_service/mex/00-NKpU4CWiHfupgckuXBNFDd \\ admin:00-NKpU4CWiHfupgckuXBNFDd Simple True 3 False ``` This module is run with the hash id of the docker create Setup/Run the docker container for test Build the docker container docker build --no-cache -t bisque_uplanteome -f Dockerfile . docker build -t biodev.ece.ucsb.edu:5000/bisque_uplanteome . Run the container and bash into it docker run -it --net=host biodev.ece.ucsb.edu:5000/bisque_uplanteome bash test run the code on mex identifier python PlanteomeDeepSegment.py http://loup.ece.ucsb.edu:8088/module_service/mex/00-NKpU4CWiHfupgckuXBNFDd admin:00-NKpU4CWiHfupgckuXBNFDd Simple True 3 False Version: 0.3 Author(s): Dimitrios Trigkakis, Justin Preece","title":"Bisque module planteome"},{"location":"guides/bisque_module_planteome/#planteome-deep-segment-analysis-module","text":"This module segments a marked object (creating a graphical object) within an input image. Then the module will classify either the entire original image or the segment created in the first step. This uses PyTorch in order to do this deep segmentation.","title":"PLANTEOME DEEP SEGMENT ANALYSIS MODULE"},{"location":"guides/bisque_module_planteome/#reference","text":"Module Development Guide: https://github.com/pndaly/BisQue_Platform_Guide Sample Deep Learning Module: Planteome Deep Segment Analysis","title":"Reference"},{"location":"guides/bisque_module_planteome/#pre-requisite-link","text":"Working bisque environment at http://loup.ece.ucsb.edu:8088/ Docker should be enabled and setup on this environment The bisque deployment server should have access to good CPU RAM since Torch Scikit-image is used Access to module folder for deploying this module (Say at ~/bisque/module)","title":"Pre-requisite: link"},{"location":"guides/bisque_module_planteome/#adddeploy-the-planteome-module","text":"Identify the bisque module folder at path ~/bisque/module Login to http://loup.ece.ucsb.edu:8088/ with credentials admin:admin Download the Planteome module to the bisque module folder Register the module to Bisque by opening the Module Manager Provide the engine URL(http://loup.ece.ucsb.edu:8088/engine_service) in the Engine service section and click load. This will list the modules by querying the engine service. Select the module named \"Planteome\", drag this module to the left pane and drop it to register.","title":"Add/Deploy the Planteome module"},{"location":"guides/bisque_module_planteome/#buildconfigure-the-module","text":"Edit the runtime-module.cfg with relevant configuration Changed the docker.hub configuration to biodev.ece.ucsb.edu:5000 View/Edit the Dockerfile for the relevant packages Edited versions for numpy\\==1.16.1 scikit-image\\==0.14.2 Source to your bisque python environment \"workon bqenv\" for installation pip install -r requirements.txt python setup.py This will install all the dependency in the modules requirements.txt file, build modules code, and create a docker image for running it. Based on your \"docker.hub\" configuration the setup will push the docker image to registry as [biodev.ece.ucsb.edu:5000/bisque_uplanteome]","title":"Build/Configure the module"},{"location":"guides/bisque_module_planteome/#steps-to-run-the-algorithm-from-bisque-client","text":"Select an image to be analyzed. The foreground/background annotation tooltip can be found on the top-right of the module image viewer. Mark the part of the image to be segmented with foreground line(s) annotation(s). Mark the image with background annotations around the object to be segmented. Select which deep classifier to use, whether to segment the image, the segmentation quality, and whether to classify the entire image or the segmented object instead. Press the 'RUN' button. Analysis may take some time depending on the image and segmentation quality. Results are given in visual and table formats, depending on whether the segmentation and classification functionalities respectively were enabled in the options.","title":"Steps to run the algorithm from Bisque Client"},{"location":"guides/bisque_module_planteome/#isolateddevelopment-test-setup","text":"Make sure you have annotated the image and have an a mex identifier availablefor manual test/run. This can be done by, - Opening the module and select image - Annotate the image as per the directions above - Configure and run the module once - Make note of the mex URL for this run by looking at the docker_run.log in the staging folder. This will be used for replaying the test run from the modules folder. Additional module execution information - When we annotate the image and click RUN on the module user interface - The Planteome module is created from biodev.ece.ucsb.edu:5000/bisque_uplanteome:latest ``` docker create biodev.ece.ucsb.edu:5000/bisque_uplanteome \\ python PlanteomeDeepSegment.py \\ http://loup.ece.ucsb.edu:8088/module_service/mex/00-NKpU4CWiHfupgckuXBNFDd \\ admin:00-NKpU4CWiHfupgckuXBNFDd Simple True 3 False ``` This module is run with the hash id of the docker create","title":"Isolated/Development test setup"},{"location":"guides/bisque_module_planteome/#setuprun-the-docker-container-for-test","text":"Build the docker container docker build --no-cache -t bisque_uplanteome -f Dockerfile . docker build -t biodev.ece.ucsb.edu:5000/bisque_uplanteome . Run the container and bash into it docker run -it --net=host biodev.ece.ucsb.edu:5000/bisque_uplanteome bash test run the code on mex identifier python PlanteomeDeepSegment.py http://loup.ece.ucsb.edu:8088/module_service/mex/00-NKpU4CWiHfupgckuXBNFDd admin:00-NKpU4CWiHfupgckuXBNFDd Simple True 3 False Version: 0.3 Author(s): Dimitrios Trigkakis, Justin Preece","title":"Setup/Run the docker container for test"},{"location":"guides/connoisseur/","text":"Setup Connoisseur Information Connoisseur is the DL module for distributed compute. Condor is running on each node TODO Figure out the configurations !!! 1. OpenCV 3.4.1 Installation Guide: Link 1 # Verify your installation pkg-config --libs opencv 2. CUDA 8.0 and cuDNN v7.1.4 Installation Guide: Link1 , Link2 Download cuda_8.0.61 $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Sun_Sep__4_22:14:01_CDT_2016 Cuda compilation tools, release 8.0, V8.0.44 Download libcudnn7-dev_7.1.4 cuDNN v7.1.4 Runtime Library for Ubuntu16.04 (Deb) cuDNN v7.1.4 Developer Library for Ubuntu16.04 (Deb) 3. CAFFE Setup caffe 1.0 using the official guide Installation Overview mkdir build cd build cmake .. make all make install make runtest $ caffe --version caffe version 1.0.0 Make sure you configure the path env in ~/.bashrc export PATH=$PATH:/usr/local/cuda/bin:~/cv/caffe/build/tools export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/cv/caffe/.build_release/lib export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/:~/cv/caffe/.build_release/lib Also add the following in ~/[python_env_folder]/bin/activate script after the PATH variable setup export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/ Test it all out to ensure caffe and opencv are functional (bqenv) rahul@bqdev:~/ws/bisque$ python Python 2.7.12 (default, Dec 4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2 import cv2 import caffe Also, ensure Caffe runs on the system $ caffe --version caffe version 1.0.0 4. Install Connoisseur Pre-requisite: (caffe and opencv may be the cause) source ~/bisque/bqenv/bin/activate pip install lmdb pyvoro Installation In Bisque top directory unzip the contents of module/connoisseur.zip Activate Env source ~/bisque/bqenv/bin/activate Ensure that caffe and opencv is importable in python Install connoisseur cd ~/bisque/connoisseur pip install -e connoisseur Move the module to Bisque modules mv ~/bisque/connoisseur/modules/Connoisseur/ ~/bisque/modules/ Deactivate Env source ~/bisque/bqenv/bin/deactivate Re/Start the Bisque server $ paver setup bq-admin setup $ bq-admin server start $ bq-admin server stop Make sure the service is available at http://0.0.0.0:8080/services Verify the log in module manager has no errors Thereafter look for the loaded services at the URL http://0.0.0.0:8080/services Look for the service named connoisseur in this list 5. Load Connoisseur Now lets add the connoisseur user group for loading the module Create a connoisseur group Add this group to the user/admin Restart the services $ bq-admin server start bq-admin server stop Load the module as user/admin Open the module from navbar using Analyze- Classification- Connoisseur TODO: Configuration, load model and run a test on Connoisseur","title":"Connoisseur"},{"location":"guides/connoisseur/#setup-connoisseur","text":"","title":"Setup Connoisseur"},{"location":"guides/connoisseur/#information","text":"Connoisseur is the DL module for distributed compute. Condor is running on each node","title":"Information"},{"location":"guides/connoisseur/#todo","text":"Figure out the configurations !!!","title":"TODO"},{"location":"guides/connoisseur/#1-opencv-341","text":"Installation Guide: Link 1 # Verify your installation pkg-config --libs opencv","title":"1. OpenCV 3.4.1"},{"location":"guides/connoisseur/#2-cuda-80-and-cudnn-v714","text":"Installation Guide: Link1 , Link2 Download cuda_8.0.61 $ nvcc --version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2016 NVIDIA Corporation Built on Sun_Sep__4_22:14:01_CDT_2016 Cuda compilation tools, release 8.0, V8.0.44 Download libcudnn7-dev_7.1.4 cuDNN v7.1.4 Runtime Library for Ubuntu16.04 (Deb) cuDNN v7.1.4 Developer Library for Ubuntu16.04 (Deb)","title":"2. CUDA 8.0 and cuDNN v7.1.4"},{"location":"guides/connoisseur/#3-caffe","text":"Setup caffe 1.0 using the official guide Installation Overview mkdir build cd build cmake .. make all make install make runtest $ caffe --version caffe version 1.0.0 Make sure you configure the path env in ~/.bashrc export PATH=$PATH:/usr/local/cuda/bin:~/cv/caffe/build/tools export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/cv/caffe/.build_release/lib export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/:~/cv/caffe/.build_release/lib Also add the following in ~/[python_env_folder]/bin/activate script after the PATH variable setup export PYTHONPATH=$PYTHONPATH:~/cv/caffe/python/ Test it all out to ensure caffe and opencv are functional (bqenv) rahul@bqdev:~/ws/bisque$ python Python 2.7.12 (default, Dec 4 2017, 14:50:18) [GCC 5.4.0 20160609] on linux2 import cv2 import caffe Also, ensure Caffe runs on the system $ caffe --version caffe version 1.0.0","title":"3. CAFFE"},{"location":"guides/connoisseur/#4-install-connoisseur","text":"Pre-requisite: (caffe and opencv may be the cause) source ~/bisque/bqenv/bin/activate pip install lmdb pyvoro Installation In Bisque top directory unzip the contents of module/connoisseur.zip Activate Env source ~/bisque/bqenv/bin/activate Ensure that caffe and opencv is importable in python Install connoisseur cd ~/bisque/connoisseur pip install -e connoisseur Move the module to Bisque modules mv ~/bisque/connoisseur/modules/Connoisseur/ ~/bisque/modules/ Deactivate Env source ~/bisque/bqenv/bin/deactivate Re/Start the Bisque server $ paver setup bq-admin setup $ bq-admin server start $ bq-admin server stop Make sure the service is available at http://0.0.0.0:8080/services Verify the log in module manager has no errors Thereafter look for the loaded services at the URL http://0.0.0.0:8080/services Look for the service named connoisseur in this list","title":"4. Install Connoisseur"},{"location":"guides/connoisseur/#5-load-connoisseur","text":"Now lets add the connoisseur user group for loading the module Create a connoisseur group Add this group to the user/admin Restart the services $ bq-admin server start bq-admin server stop Load the module as user/admin Open the module from navbar using Analyze- Classification- Connoisseur TODO: Configuration, load model and run a test on Connoisseur","title":"5. Load Connoisseur"},{"location":"guides/errors/","text":"Errors Solution to errors encountered during the setup 1. Port in use error If you see an error like below it may be due to port not being free File /opt/nagare-home/lib/python2.7/site-packages/Paste-1.7.5.1-py2.7.egg/paste/httpserver.py , line 1105, in server_close self.thread_pool.shutdown(60) AttributeError: WSGIThreadPoolServer object has no attribute thread_pool Solution is to kill the process holding the port sudo kill $(sudo lsof -t -i:8080) 2. Import Error on a library Issue: ImportError: libhdf5_serial.so.100: cannot open shared object file: No such file or directory Shows pytables related warning/issues when you start server Solution is to fix the installed packages pip uninstall h5py tables pip install --no-cache-dir tables 3. Container Errors RunContainerError: failed to start container 9c51eea9f13d6a4cedb61591ca953914ce6396f8e8849116441388605d9f4320 : Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused process_linux.go:424: container init caused \\ process_linux.go:407: running prestart hook 0 caused \\\\\\ error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda =9.0 --pid=2959531 /var/lib/docker/overlay2/bb9185b9c4dc070c25cac10ae0aad2b991f6dddf8c70f8979e28ec0609c14a21/merged]\\\\\\\\nnvidia-container-cli: initialization error: cuda error: no cuda-capable device is detected\\\\\\\\n\\\\\\ \\ : unknown 4. Fix for the network issues in container. \"cannot reach/connect to external/host address\" Error: Network requests.exceptions.ConnectionError: HTTPConnectionPool (host='loup.ece.ucsb.edu', port=8088): Max retries exceeded with url Error: PyTorch ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm). File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 274, in handler _error_if_any_worker_fails() RuntimeError: DataLoader worker (pid 277) is killed by signal: Bus error. Common Fix : mount the docker container using --ipc=host flag docker create --ipc=host biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2 \\ python PythonScriptWrapper.py \\ http://bisque-dev-gpu-01.cyverse.org:8080/data_service/00-ZeBjryEbutgnpKFWvFDx38 \\ 15 0.05 \\ http://bisque-dev-gpu-01.cyverse.org:8080/module_service/mex/00-g5rHg7NyujuUmPzLLb2M78 \\ admin:00-g5rHg7NyujuUmPzLLb2M78","title":"Errors"},{"location":"guides/errors/#errors","text":"Solution to errors encountered during the setup","title":"Errors"},{"location":"guides/errors/#1-port-in-use-error","text":"If you see an error like below it may be due to port not being free File /opt/nagare-home/lib/python2.7/site-packages/Paste-1.7.5.1-py2.7.egg/paste/httpserver.py , line 1105, in server_close self.thread_pool.shutdown(60) AttributeError: WSGIThreadPoolServer object has no attribute thread_pool Solution is to kill the process holding the port sudo kill $(sudo lsof -t -i:8080)","title":"1. Port in use error"},{"location":"guides/errors/#2-import-error-on-a-library","text":"Issue: ImportError: libhdf5_serial.so.100: cannot open shared object file: No such file or directory Shows pytables related warning/issues when you start server Solution is to fix the installed packages pip uninstall h5py tables pip install --no-cache-dir tables","title":"2. Import Error on a library"},{"location":"guides/errors/#3-container-errors","text":"RunContainerError: failed to start container 9c51eea9f13d6a4cedb61591ca953914ce6396f8e8849116441388605d9f4320 : Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused process_linux.go:424: container init caused \\ process_linux.go:407: running prestart hook 0 caused \\\\\\ error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --compute --utility --require=cuda =9.0 --pid=2959531 /var/lib/docker/overlay2/bb9185b9c4dc070c25cac10ae0aad2b991f6dddf8c70f8979e28ec0609c14a21/merged]\\\\\\\\nnvidia-container-cli: initialization error: cuda error: no cuda-capable device is detected\\\\\\\\n\\\\\\ \\ : unknown","title":"3. Container Errors"},{"location":"guides/errors/#4-fix-for-the-network-issues-in-container-cannot-reachconnect-to-externalhost-address","text":"Error: Network requests.exceptions.ConnectionError: HTTPConnectionPool (host='loup.ece.ucsb.edu', port=8088): Max retries exceeded with url Error: PyTorch ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm). File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 274, in handler _error_if_any_worker_fails() RuntimeError: DataLoader worker (pid 277) is killed by signal: Bus error. Common Fix : mount the docker container using --ipc=host flag docker create --ipc=host biodev.ece.ucsb.edu:5000/torch-cellseg-3dunet-v2 \\ python PythonScriptWrapper.py \\ http://bisque-dev-gpu-01.cyverse.org:8080/data_service/00-ZeBjryEbutgnpKFWvFDx38 \\ 15 0.05 \\ http://bisque-dev-gpu-01.cyverse.org:8080/module_service/mex/00-g5rHg7NyujuUmPzLLb2M78 \\ admin:00-g5rHg7NyujuUmPzLLb2M78","title":"4. Fix for the network issues in container. \"cannot reach/connect to external/host address\""},{"location":"guides/hggit/","text":"Version Control Instructions (HG-GIT) Maintain the HG- GIT code push using http://hg-git.github.io Current Release Version: 0.5.10 Instructions :https://til.secretgeek.net/mercurial/convert_hg_to_git.html Source HG :https://biodev.ece.ucsb.edu/hg/bisque-stable Target Git :https://github.com/UCSB-VRL/bisque.git hg- git push Install $ sudo easy_install hg-git $ hg clone --insecure https://biodev.ece.ucsb.edu/hg/bisque-stable Make sure the following is in your .hgrc file: vim ~/.hgrc [git] intree = 1 [extensions] hgext.bookmarks = hggit = [ui] ignore = ~/.hgignore Add a master branch name to the repo and export cd bisque-stable hg bookmark -r default master # so a ref gets created hg gexport --debug # exporting hg commits/objects to git Initialize and prepare the target git repo git init git remote add origin https://github.com/UCSB-VRL/bisque.git git remote -v # verify the remote repo track the large files using LFS https://git-lfs.github.com/ curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash git lfs install git lfs track \"*.ipch\" git lfs track \"*.sdf\" git lfs track \"*.sav\" git lfs track \"*.model\" git add .gitattributes Check the commits using 'git status' and prepare for github git pull origin master git add --all git commit -m \"hg-git from https://biodev.ece.ucsb.edu/hg/bisque-stable/\" git push origin master Current Advice: Stare and compare that the new git repo matches the old hg repo, until you are completely satisfied.","title":"Hggit"},{"location":"guides/hggit/#version-control-instructions-hg-git","text":"Maintain the HG- GIT code push using http://hg-git.github.io Current Release Version: 0.5.10 Instructions :https://til.secretgeek.net/mercurial/convert_hg_to_git.html Source HG :https://biodev.ece.ucsb.edu/hg/bisque-stable Target Git :https://github.com/UCSB-VRL/bisque.git hg- git push Install $ sudo easy_install hg-git $ hg clone --insecure https://biodev.ece.ucsb.edu/hg/bisque-stable Make sure the following is in your .hgrc file: vim ~/.hgrc [git] intree = 1 [extensions] hgext.bookmarks = hggit = [ui] ignore = ~/.hgignore Add a master branch name to the repo and export cd bisque-stable hg bookmark -r default master # so a ref gets created hg gexport --debug # exporting hg commits/objects to git Initialize and prepare the target git repo git init git remote add origin https://github.com/UCSB-VRL/bisque.git git remote -v # verify the remote repo track the large files using LFS https://git-lfs.github.com/ curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash git lfs install git lfs track \"*.ipch\" git lfs track \"*.sdf\" git lfs track \"*.sav\" git lfs track \"*.model\" git add .gitattributes Check the commits using 'git status' and prepare for github git pull origin master git add --all git commit -m \"hg-git from https://biodev.ece.ucsb.edu/hg/bisque-stable/\" git push origin master Current Advice: Stare and compare that the new git repo matches the old hg repo, until you are completely satisfied.","title":"Version Control Instructions (HG-GIT)"},{"location":"guides/jupyter_notebooks/","text":"Reference Video demonstration at Youtube Cyverse Webinar Reference Updated Bisque notebooks on CyVerse github account Data Sample data from AWS S3 Download public/shared data from Cyverse Bisque Download public/shared data from UCSB Biodev Bisque Notebooks Use the three samples from CBIucsb Bitbucket repository as described in the video demonstration 1_upload_images_group.ipynb 2_upload_image_graphical_annotations.ipynb 3_modify_annotations.ipynb General Instructions Install bisque on local and the relevant packages in the virtual env Modify the URL and credentials in the notebooks Download the tif/lsm data from the sample data share above and drop it in relevant folders Go through each step in the notebook to understand and execute them","title":"Jupyter"},{"location":"guides/jupyter_notebooks/#reference","text":"Video demonstration at Youtube Cyverse Webinar Reference Updated Bisque notebooks on CyVerse github account","title":"Reference"},{"location":"guides/jupyter_notebooks/#data","text":"Sample data from AWS S3 Download public/shared data from Cyverse Bisque Download public/shared data from UCSB Biodev Bisque","title":"Data"},{"location":"guides/jupyter_notebooks/#notebooks","text":"Use the three samples from CBIucsb Bitbucket repository as described in the video demonstration 1_upload_images_group.ipynb 2_upload_image_graphical_annotations.ipynb 3_modify_annotations.ipynb","title":"Notebooks"},{"location":"guides/jupyter_notebooks/#general-instructions","text":"Install bisque on local and the relevant packages in the virtual env Modify the URL and credentials in the notebooks Download the tif/lsm data from the sample data share above and drop it in relevant folders Go through each step in the notebook to understand and execute them","title":"General Instructions"},{"location":"guides/openstack/","text":"Openstack @UCSB We should login to it using our Ucsb NetId credentials(the way you use at gauchospace). Link: https://mimic.aristotle.ucsb.edu/ Authenticate using: Keystone Credentials Domain: ucsb user: pass: Notes from Jeff Oakes Create/Launch compute instance Use auto_allocated network (10.0.0.x network) Create the instance (Ubuntu) Associate the floating IP Change the deafult security group for access over SSH port 22 Create a SSH key and access instance using \"ssh -i *.pem ubuntu@169.*.*.*\" Create/Use storage(Ceph based) as attached volume. Format the partition in the instance mount it (Use GPT label or XFS for TB's of volumes) sudo -i dmesg | grep vd mount /dev/vdb1 /mnt df -h help: http://www.darwinbiler.com/openstack-creating-and-attaching-a-volume-into-an-instance/ Ansible Playbook for instance configuraiton (Rather than Golden Image or a bash script for installation) Note: Openstack uses 3 KVM instances and try not to launch 3 instances with 32GB RAM at once Openstack @CyVerse Login to Cyverse Openstack using the account created by Chris Martin or Andy Edmond in University of Arizona Link: https://tombstone-cloud.cyverse.org Domain: cso user: vishwakarma pass: Note: Similar instance and volume creation process as described earlier.","title":"Openstack"},{"location":"guides/openstack/#openstack-ucsb","text":"We should login to it using our Ucsb NetId credentials(the way you use at gauchospace). Link: https://mimic.aristotle.ucsb.edu/ Authenticate using: Keystone Credentials Domain: ucsb user: pass:","title":"Openstack @UCSB"},{"location":"guides/openstack/#notes-from-jeff-oakes-amp111amp97amp107amp101amp115amp106amp64amp117amp99amp115amp98amp46amp101amp100amp117","text":"Create/Launch compute instance Use auto_allocated network (10.0.0.x network) Create the instance (Ubuntu) Associate the floating IP Change the deafult security group for access over SSH port 22 Create a SSH key and access instance using \"ssh -i *.pem ubuntu@169.*.*.*\" Create/Use storage(Ceph based) as attached volume. Format the partition in the instance mount it (Use GPT label or XFS for TB's of volumes) sudo -i dmesg | grep vd mount /dev/vdb1 /mnt df -h help: http://www.darwinbiler.com/openstack-creating-and-attaching-a-volume-into-an-instance/ Ansible Playbook for instance configuraiton (Rather than Golden Image or a bash script for installation)","title":"Notes from Jeff Oakes &#111;&#97;&#107;&#101;&#115;&#106;&#64;&#117;&#99;&#115;&#98;&#46;&#101;&#100;&#117;"},{"location":"guides/openstack/#note-openstack-uses-3-kvm-instances-and-try-not-to-launch-3-instances-with-32gb-ram-at-once","text":"","title":"Note: Openstack uses 3 KVM instances and try not to launch 3 instances with 32GB RAM at once"},{"location":"guides/openstack/#openstack-cyverse","text":"Login to Cyverse Openstack using the account created by Chris Martin or Andy Edmond in University of Arizona Link: https://tombstone-cloud.cyverse.org Domain: cso user: vishwakarma pass: Note: Similar instance and volume creation process as described earlier.","title":"Openstack @CyVerse"},{"location":"guides/rancher2_bisque/","text":"Rancher 2.0 Setup (with Kubernetes engine) Introduction Why Docker Container ?? A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Container vs. Virtual Machine Why Kubernetes ?? K8s ?? Kubernetes provides a container-centric management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads. Why Rancher ?? Rancher combines everything an organization needs to run containers in production and centrally manage multiple Kubernetes clusters Rancher includes a full Kubernetes distribution, but adds value around Kubernetes in three key areas: Cluster Operations and Management, Intuitive Workload Management, and Enterprise Support. Nomenclature (Kubernetes like) Rancher 2.0 Guide to provisioning a Kubernetes cluster which uses the Kubernetes container-orchestration system to Name Kubernetes concepts Container Pod (Simplest Kubernetes object representing a set of containers on the cluster) Services Workload (Units of work that are running on the cluster, these can be pods or deployments) Load Balancer Ingress Stack Namespace (A virtual cluster) Environment Project (Administration)/Cluster (Compute machines that run containerized applications) Host Node (Physical or virtual machines making up the cluster) Catalog Helm A. Cluster Description Deployment Google Slides B. Lets Encrypt on Ubuntu 16.04 Bind the hostname to the IP address by creating an A record in DNS Letsencrypt ACME challenge at TCP/80 on host Open up firewall for this \"sudo ufw allow 80 sudo ufw allow 80 443\" Verify the port 80 availability sudo netstat -peanut | grep :80 Setup certificates at /etc/letsencrypt sudo certbot certonly --standalone --dry-run \\ --cert-name loup.ece.ucsb.edu -d loup.ece.ucsb.edu C. Master Rancher 2.0 Install/Startup Rancher: https://rancher.com/docs/rancher/v2.x/en/installation/single-node/ - Rancher etcd data persisted at /var/lib/rancher - Since port 80 is occupied by rancher/rancher, a rancher/rancher-agent cannot be run on this node. docker run -d --restart=unless-stopped \\ -p 8080:80 -p 8443:443 \\ -v /var/log/rancher/auditlog:/var/log/auditlog \\ -v /host/rancher:/var/lib/rancher \\ -e AUDIT_LEVEL=1 \\ rancher/rancher:stable You will have rancher accessible at https://loup.ece.ucsb.edu:8443 if everything goes fine Create the YAML configuration based on docker-compose.yaml (In case of migration) Create an access key for Rancher CLI operations (Doesnt work on self-signed certs) endpoint : https://loup.ece.ucsb.edu:8443/v3 access-key: token-xt47w secret-key: bearer-tok: token-xt47w: Migration CLI Download the docker-compose and rancher-compose.yml files from existing rancher user interface for migration https://rancher.com/docs/rancher/v2.x/en/v1.6-migration Migration using CLI tools migration-tools export --url RANCHER_URL --access-key RANCHER_ACCESS_KEY \\ --secret-key RANCHER_SECRET_KEY --export-dir EXPORT_DIR ./migration-tools parse --docker-file compose/docker-compose.yml \\ --rancher-file compose/rancher-compose.yml D. Setup Cluster RKE/custom-nodes Port requirements Open up ports based on the CNI provider requirements - Use Canal as the provider in this case # API/UI Clients sudo ufw allow 22,80,443/tcp # Etcd Plane Nodes sudo ufw allow 2379,2380,9099,6443/tcp sudo ufw allow 8472/udp # Control Plane Nodes sudo ufw allow 2379,2380,10250,6443,9099,10254/tcp sudo ufw allow 8472/udp # Worker Plane Nodes sudo ufw allow 6443,9099,10254/tcp sudo ufw allow 8472/udp # Workload sudo ufw allow 30000:32767/tcp sudo ufw allow 30000:32767/udp # Others sudo ufw allow 2376/tcp Create cluster Create a cluster in rancher-ui named \"bq-cluster\" Select \"custom\" local/remote nodes option to create this cluster Run the below command for running the rancher-agent/workers sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 --server https://loup.ece.ucsb.edu:8443 \\ --token 7z2ncgjj4482m48fpsj7xjmc8lc9n6bsxh7qcjrsr6rcxrzhzl6prz \\ --ca-checksum d522680b13d7aabe4dc57bb2776e28759852c336d0cf0e0f9fed5d3fb7b495e8 \\ --etcd --controlplane --worker The final \"bq-cluster\" state can be visualized upon creation A docker ps on a node of the cluster (as created above) would look like below screenshot Add more nodes as worker, by running above command on those nodes so that they register with the rancher2 and become part of this cluster. The nodes on a cluster can be visualized in rancher cluster - nodes menu. Create a namespace bqdev within this cluster Bisque Development environment where workloads are deployed and tested E. Setup Volume Mount the host directory for volume using NFS and setup the nfs client access for the cluster https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04 Setup folders # Create the path on host system sudo mkdir /opt/bisque/ -p \\ sudo mkdir /opt/bisque/data -p \\ sudo mkdir /opt/bisque/local/workdir -p # Allow other users to edit this sudo chown -R nobody:nogroup /opt/bisque/ Open up the ports used by NFS # Access from specific machines sudo ufw allow from 192.168.1.129 to any port nfs sudo ufw allow from 192.168.1.133 to any port nfs # Specific ports in case above doesnt work sudo ufw allow 32768:65535/tcp sudo ufw allow 32768:65535/udp sudo ufw allow 2049/tcp sudo ufw allow 2049/udp sudo ufw allow 111/tcp sudo ufw allow 111/udp Now add NFS host configuration at /etc/exports /opt/bisque 192.168.1.129(rw,sync,no_root_squash,no_subtree_check) /opt/bisque 192.168.1.133(rw,sync,no_root_squash,no_subtree_check) restart the nfs server on the NFS host machine sudo systemctl restart nfs-kernel-server Mount the NFS folder on the client machine sudo mount 192.168.1.123:/opt/bisque/ /run/bisque/ Verify the mount on a client system using df -h AND https://www.claudiokuenzler.com/blog/786/rancher-2.0-create-persistent-volume-from-nfs-share Create a persistent volume in the cluster Set local path option on the node as /run/bisque We can see all the volumes that are created in the Volumes section of the \"bq-cluster\" workload F. Setup Workload (on the cluster) Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at custom registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc or we can use a publicly deployed image at https://hub.docker.com Test workload configuration Name: ucsb-bisque05-svc Pods: 2 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 8080-tcp-NodePort-Random 27000-tcp-NodePort-Random Alternately, we can use 8080-tcp-ClusterIP(Internal)-Same 27000-tcp-ClusterIP(Internal)-Same Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /tmp/bisque Scaling: No change Command: (Only, in case needed. Not used with the current Image) Entrypoint: /builder/run-bisque.sh Command: bootstrap start Working Dir: /source Console: Interactive TTY (-i -t) Networking: No Change Labels: No change Security: No change Finally we can see the overall state of pods in the workload within the clusters Environment Configuration Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 We should see the overview of workloads deployed as below G. Load Balancing (using L7 Ingress) Add Ingress configuration for load balancing with name \"bq-website\" Configure the target(ucsb-bisque05-svc) pods so that the port 8080 is exposed through the ingress controller Load Balancing section of the workload will showcase the list of ingress controllers along with the mapping H. Monitoring/Debugging Using cluster kubectl shell from the cluster web UI # Fetch namespaces kubectl get pods --all-namespaces kubectl get pods -n bqdev # Fetch logs on a pod/container kubectl logs postgres-564d9f79d5-z2sxl -n bqdev Use Cluster dashboard for all cluster monitoring and configuration I. Uninstall Rancher Stop Rancher Containers # Master: for the rancher server container docker stop $(docker ps -a -q --filter ancestor=rancher/rancher:stable --format= {{.ID}} ) # Workers: for all k8s containers docker stop $(docker ps -f name=k* --format= {{.ID}} ) Clean the container, images and volumes docker rm -f $(docker ps -a -f name=k* --format= {{.ID}} ) docker rmi -f $(docker images -q rancher/* ) docker volume rm $(docker volume ls -q) Unmount and remove data (/var/lib/kubelet/pods/XXX, /var/lib/kubelet, /var/lib/rancher) # Unmount directories for mount in $(mount | grep tmpfs | grep /var/lib/kubelet | awk { print $3 } ) /var/lib/kubelet /var/lib/rancher; do sudo umount $mount; done # Clean the directories sudo rm -rf /etc/ceph \\ /etc/cni \\ /etc/kubernetes \\ /opt/cni \\ /opt/rke \\ /run/secrets/kubernetes.io \\ /run/calico \\ /run/flannel \\ /var/lib/calico \\ /var/lib/etcd \\ /var/lib/cni \\ /var/lib/kubelet \\ /var/lib/rancher/rke/log \\ /var/log/containers \\ /var/log/pods \\ /var/run/calico # Mounted host directories sudo rm -rf /host/rancher/ sudo rm -rf /var/log/rancher/auditlog Remove the existing network interface ip address show ip link delete interface_name Additional References ==TODO== 1.) Mail server setup https://www.linuxbabe.com/mail-server/ubuntu-16-04-iredmail-server-installation 2.) Migration from Rancher 1.x to 2.x individual workload/containers to rancher-kubernetes using rancher-cli (doesnt work with self-signed certificates) https://rancher.com/blog/2018/2018-08-02-journey-from-cattle-to-k8s/ 3.) Reference on Ingress Controllers Load Balancers add in workloads /k8s-in-rancher/load-balancers-and-ingress Tried using built in Ingress for bq-website.bqdev.192.168.1.129.xip.io but failed to work for engine service If you want to expose the workload container to outside world then use NodePort otherwise work with ClusterIp(Internal Only) port configuration https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/ 4.) PostgreSQL server Setup PostgreSql 10.4 on Rancher workload This is used in the Bisque configuration as environment variable BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres","title":"Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/rancher2_bisque/#rancher-20-setup-with-kubernetes-engine","text":"","title":"Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/rancher2_bisque/#introduction","text":"Why Docker Container ?? A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Container vs. Virtual Machine Why Kubernetes ?? K8s ?? Kubernetes provides a container-centric management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads. Why Rancher ?? Rancher combines everything an organization needs to run containers in production and centrally manage multiple Kubernetes clusters Rancher includes a full Kubernetes distribution, but adds value around Kubernetes in three key areas: Cluster Operations and Management, Intuitive Workload Management, and Enterprise Support.","title":"Introduction"},{"location":"guides/rancher2_bisque/#nomenclature-kubernetes-like","text":"Rancher 2.0 Guide to provisioning a Kubernetes cluster which uses the Kubernetes container-orchestration system to Name Kubernetes concepts Container Pod (Simplest Kubernetes object representing a set of containers on the cluster) Services Workload (Units of work that are running on the cluster, these can be pods or deployments) Load Balancer Ingress Stack Namespace (A virtual cluster) Environment Project (Administration)/Cluster (Compute machines that run containerized applications) Host Node (Physical or virtual machines making up the cluster) Catalog Helm","title":"Nomenclature (Kubernetes like)"},{"location":"guides/rancher2_bisque/#a-cluster-description","text":"Deployment Google Slides","title":"A. Cluster Description"},{"location":"guides/rancher2_bisque/#b-lets-encrypt-on-ubuntu-1604","text":"Bind the hostname to the IP address by creating an A record in DNS Letsencrypt ACME challenge at TCP/80 on host Open up firewall for this \"sudo ufw allow 80 sudo ufw allow 80 443\" Verify the port 80 availability sudo netstat -peanut | grep :80 Setup certificates at /etc/letsencrypt sudo certbot certonly --standalone --dry-run \\ --cert-name loup.ece.ucsb.edu -d loup.ece.ucsb.edu","title":"B. Lets Encrypt on Ubuntu 16.04"},{"location":"guides/rancher2_bisque/#c-master-rancher-20","text":"Install/Startup Rancher: https://rancher.com/docs/rancher/v2.x/en/installation/single-node/ - Rancher etcd data persisted at /var/lib/rancher - Since port 80 is occupied by rancher/rancher, a rancher/rancher-agent cannot be run on this node. docker run -d --restart=unless-stopped \\ -p 8080:80 -p 8443:443 \\ -v /var/log/rancher/auditlog:/var/log/auditlog \\ -v /host/rancher:/var/lib/rancher \\ -e AUDIT_LEVEL=1 \\ rancher/rancher:stable You will have rancher accessible at https://loup.ece.ucsb.edu:8443 if everything goes fine Create the YAML configuration based on docker-compose.yaml (In case of migration) Create an access key for Rancher CLI operations (Doesnt work on self-signed certs) endpoint : https://loup.ece.ucsb.edu:8443/v3 access-key: token-xt47w secret-key: bearer-tok: token-xt47w:","title":"C. Master Rancher 2.0"},{"location":"guides/rancher2_bisque/#migration-cli","text":"Download the docker-compose and rancher-compose.yml files from existing rancher user interface for migration https://rancher.com/docs/rancher/v2.x/en/v1.6-migration Migration using CLI tools migration-tools export --url RANCHER_URL --access-key RANCHER_ACCESS_KEY \\ --secret-key RANCHER_SECRET_KEY --export-dir EXPORT_DIR ./migration-tools parse --docker-file compose/docker-compose.yml \\ --rancher-file compose/rancher-compose.yml","title":"Migration CLI"},{"location":"guides/rancher2_bisque/#d-setup-cluster-rkecustom-nodes","text":"","title":"D. Setup Cluster RKE/custom-nodes"},{"location":"guides/rancher2_bisque/#port-requirements","text":"Open up ports based on the CNI provider requirements - Use Canal as the provider in this case # API/UI Clients sudo ufw allow 22,80,443/tcp # Etcd Plane Nodes sudo ufw allow 2379,2380,9099,6443/tcp sudo ufw allow 8472/udp # Control Plane Nodes sudo ufw allow 2379,2380,10250,6443,9099,10254/tcp sudo ufw allow 8472/udp # Worker Plane Nodes sudo ufw allow 6443,9099,10254/tcp sudo ufw allow 8472/udp # Workload sudo ufw allow 30000:32767/tcp sudo ufw allow 30000:32767/udp # Others sudo ufw allow 2376/tcp","title":"Port requirements"},{"location":"guides/rancher2_bisque/#create-cluster","text":"Create a cluster in rancher-ui named \"bq-cluster\" Select \"custom\" local/remote nodes option to create this cluster Run the below command for running the rancher-agent/workers sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 --server https://loup.ece.ucsb.edu:8443 \\ --token 7z2ncgjj4482m48fpsj7xjmc8lc9n6bsxh7qcjrsr6rcxrzhzl6prz \\ --ca-checksum d522680b13d7aabe4dc57bb2776e28759852c336d0cf0e0f9fed5d3fb7b495e8 \\ --etcd --controlplane --worker The final \"bq-cluster\" state can be visualized upon creation A docker ps on a node of the cluster (as created above) would look like below screenshot Add more nodes as worker, by running above command on those nodes so that they register with the rancher2 and become part of this cluster. The nodes on a cluster can be visualized in rancher cluster - nodes menu.","title":"Create cluster"},{"location":"guides/rancher2_bisque/#create-a-namespace-bqdev-within-this-cluster","text":"Bisque Development environment where workloads are deployed and tested","title":"Create a namespace bqdev within this cluster"},{"location":"guides/rancher2_bisque/#e-setup-volume","text":"Mount the host directory for volume using NFS and setup the nfs client access for the cluster https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04 Setup folders # Create the path on host system sudo mkdir /opt/bisque/ -p \\ sudo mkdir /opt/bisque/data -p \\ sudo mkdir /opt/bisque/local/workdir -p # Allow other users to edit this sudo chown -R nobody:nogroup /opt/bisque/ Open up the ports used by NFS # Access from specific machines sudo ufw allow from 192.168.1.129 to any port nfs sudo ufw allow from 192.168.1.133 to any port nfs # Specific ports in case above doesnt work sudo ufw allow 32768:65535/tcp sudo ufw allow 32768:65535/udp sudo ufw allow 2049/tcp sudo ufw allow 2049/udp sudo ufw allow 111/tcp sudo ufw allow 111/udp Now add NFS host configuration at /etc/exports /opt/bisque 192.168.1.129(rw,sync,no_root_squash,no_subtree_check) /opt/bisque 192.168.1.133(rw,sync,no_root_squash,no_subtree_check) restart the nfs server on the NFS host machine sudo systemctl restart nfs-kernel-server Mount the NFS folder on the client machine sudo mount 192.168.1.123:/opt/bisque/ /run/bisque/ Verify the mount on a client system using df -h AND https://www.claudiokuenzler.com/blog/786/rancher-2.0-create-persistent-volume-from-nfs-share Create a persistent volume in the cluster Set local path option on the node as /run/bisque We can see all the volumes that are created in the Volumes section of the \"bq-cluster\" workload","title":"E. Setup Volume"},{"location":"guides/rancher2_bisque/#f-setup-workload-on-the-cluster","text":"Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at custom registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc or we can use a publicly deployed image at https://hub.docker.com","title":"F. Setup Workload (on the cluster)"},{"location":"guides/rancher2_bisque/#test-workload-configuration","text":"Name: ucsb-bisque05-svc Pods: 2 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 8080-tcp-NodePort-Random 27000-tcp-NodePort-Random Alternately, we can use 8080-tcp-ClusterIP(Internal)-Same 27000-tcp-ClusterIP(Internal)-Same Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /tmp/bisque Scaling: No change Command: (Only, in case needed. Not used with the current Image) Entrypoint: /builder/run-bisque.sh Command: bootstrap start Working Dir: /source Console: Interactive TTY (-i -t) Networking: No Change Labels: No change Security: No change Finally we can see the overall state of pods in the workload within the clusters","title":"Test workload configuration"},{"location":"guides/rancher2_bisque/#environment-configuration","text":"Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 We should see the overview of workloads deployed as below","title":"Environment Configuration"},{"location":"guides/rancher2_bisque/#g-load-balancing-using-l7-ingress","text":"Add Ingress configuration for load balancing with name \"bq-website\" Configure the target(ucsb-bisque05-svc) pods so that the port 8080 is exposed through the ingress controller Load Balancing section of the workload will showcase the list of ingress controllers along with the mapping","title":"G. Load Balancing (using L7 Ingress)"},{"location":"guides/rancher2_bisque/#h-monitoringdebugging","text":"Using cluster kubectl shell from the cluster web UI # Fetch namespaces kubectl get pods --all-namespaces kubectl get pods -n bqdev # Fetch logs on a pod/container kubectl logs postgres-564d9f79d5-z2sxl -n bqdev Use Cluster dashboard for all cluster monitoring and configuration","title":"H. Monitoring/Debugging"},{"location":"guides/rancher2_bisque/#i-uninstall-rancher","text":"Stop Rancher Containers # Master: for the rancher server container docker stop $(docker ps -a -q --filter ancestor=rancher/rancher:stable --format= {{.ID}} ) # Workers: for all k8s containers docker stop $(docker ps -f name=k* --format= {{.ID}} ) Clean the container, images and volumes docker rm -f $(docker ps -a -f name=k* --format= {{.ID}} ) docker rmi -f $(docker images -q rancher/* ) docker volume rm $(docker volume ls -q) Unmount and remove data (/var/lib/kubelet/pods/XXX, /var/lib/kubelet, /var/lib/rancher) # Unmount directories for mount in $(mount | grep tmpfs | grep /var/lib/kubelet | awk { print $3 } ) /var/lib/kubelet /var/lib/rancher; do sudo umount $mount; done # Clean the directories sudo rm -rf /etc/ceph \\ /etc/cni \\ /etc/kubernetes \\ /opt/cni \\ /opt/rke \\ /run/secrets/kubernetes.io \\ /run/calico \\ /run/flannel \\ /var/lib/calico \\ /var/lib/etcd \\ /var/lib/cni \\ /var/lib/kubelet \\ /var/lib/rancher/rke/log \\ /var/log/containers \\ /var/log/pods \\ /var/run/calico # Mounted host directories sudo rm -rf /host/rancher/ sudo rm -rf /var/log/rancher/auditlog Remove the existing network interface ip address show ip link delete interface_name","title":"I. Uninstall Rancher"},{"location":"guides/rancher2_bisque/#additional-references","text":"==TODO==","title":"Additional References"},{"location":"guides/rancher2_bisque/#1-mail-server-setup","text":"https://www.linuxbabe.com/mail-server/ubuntu-16-04-iredmail-server-installation","title":"1.) Mail server setup"},{"location":"guides/rancher2_bisque/#2-migration-from-rancher-1x-to-2x","text":"individual workload/containers to rancher-kubernetes using rancher-cli (doesnt work with self-signed certificates) https://rancher.com/blog/2018/2018-08-02-journey-from-cattle-to-k8s/","title":"2.) Migration from Rancher 1.x to 2.x"},{"location":"guides/rancher2_bisque/#3-reference-on-ingress-controllers","text":"Load Balancers add in workloads /k8s-in-rancher/load-balancers-and-ingress Tried using built in Ingress for bq-website.bqdev.192.168.1.129.xip.io but failed to work for engine service If you want to expose the workload container to outside world then use NodePort otherwise work with ClusterIp(Internal Only) port configuration https://rancher.com/blog/2018/2018-08-14-expose-and-monitor-workloads/","title":"3.) Reference on Ingress Controllers"},{"location":"guides/rancher2_bisque/#4-postgresql-server","text":"Setup PostgreSql 10.4 on Rancher workload This is used in the Bisque configuration as environment variable BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres","title":"4.) PostgreSQL server"},{"location":"guides/rancher2_bisque_gpu/","text":"Bisque Workload on Rancher 2.0 Setup (with Kubernetes engine) Pre-requisite Rancher 2.0 with bq-cluster workload instructions Rancher 2.0 with PostgreSql workload instructions General instructions Setup Cluster with /rke-clusters/custom-nodes Note: Assuming we have bq-cluster with postgres and persistent volumes Setup Workload on the bq-cluster Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at custom registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc or another available at vishwakarmarhl/ucsb-bisque05-svc:dev Bisque Service Workload configuration Name: ucsb-bisque05-svc Pods: 2 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 8080-tcp-NodePort-Random 27000-tcp-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /run/bisque Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change Environment Configuration Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 Condor provisioning Condor Master - Image: biodev.ece.ucsb.edu:5000/condor - Ports: 9618, 9886 as HostPort - Environment CONDOR_DAEMONS = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT CONDOR_MANAGER_HOST = master Condor Worker - Same configuration as above - Ports: 9886 NodePort Random Workload (bq-cluster) Dashboard Connoisseur/GPU Workload provisioning Here lets take a look at connoissuer service in Bisque. We will run a Bisque H1 server disabling the engine_service and connoisseur in part (A) and later another service in part (B) with Connoisseur and engine service for the actual compute. Pre-requisites Create a namespace \"connoisseur\" and deploy everything isolated from the existing bisque-svc Create a postgres database for this deployment psql -h 10.42.0.15 -U postgres --password -p 5432 postgres \"create database connoissuer;\" \"grant all privileges on database connoisseur to postgres;\" Create a volume bqcon-vol mounted at 192.168.1.123:/opt/bisque/connoisseur over NFS A.) Bisque Client Service Workload configuration Name: bq-connoisseur-client-svc Pods: 1 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Port Mapping: 80-tcp-NodePort-Random 27000-tcp-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host that has GPU, Say \"arkady\" Health Check: No change Volumes: Persistent Volume claim of bqcon-vol and set the mount point as /run/bisque/ Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change Environment Configuration Bisque client service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/connoisseur BISQUE_SERVERS_H1_SERVICES_DISABLED = engine_service,connoisseur B.) Bisque Engine Service Workload configuration All the same configuration as above but skipping one environment variable Skip the disable variable -- BISQUE_SERVERS_H1_SERVICES_DISABLED = engine_service,connoisseur Verify connoisseur GPU variables NVIDIA_REQUIRE_CUDA=cuda =8.0 NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility CUDA_PKG_VERSION=8-0=8.0.61-1 CAFFE_PKG_VERSION=0.15.13-1ubuntu16.04+cuda8.0 CAFFE_VERSION=0.15 CUDA_VERSION=8.0.61 CONDOR_MANAGER_HOST=master.condor CONDOR_DAEMONS=MASTER,SCHEDD,SHARED_PORT Workload (bq-cluster) with Namespace Connoissuer TODO: - Fix Caffe and CUDA within the image - Probably write your own Docker file for a new CUDA/GPU enabled container bisque@bq-connoisseur-engine-svc-74755f798b-6lbgk:/source$ caffe deveice_query -gpu all E0213 00:36:24.855478 1798 caffe.cpp:77] Available caffe actions: E0213 00:36:24.856573 1798 caffe.cpp:80] device_query E0213 00:36:24.856690 1798 caffe.cpp:80] test E0213 00:36:24.856793 1798 caffe.cpp:80] time E0213 00:36:24.856899 1798 caffe.cpp:80] train F0213 00:36:24.857019 1798 caffe.cpp:82] Unknown action: deveice_query *** Check failure stack trace: *** @ 0x7fafbc7c65cd google::LogMessage::Fail() @ 0x7fafbc7c8433 google::LogMessage::SendToLog() @ 0x7fafbc7c615b google::LogMessage::Flush() @ 0x7fafbc7c8e1e google::LogMessageFatal::~LogMessageFatal() @ 0x40863a main @ 0x7fafbb218830 __libc_start_main @ 0x408dd9 _start @ (nil) (unknown) Aborted (core dumped)","title":"Bisque Workload on Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/rancher2_bisque_gpu/#bisque-workload-on-rancher-20-setup-with-kubernetes-engine","text":"","title":"Bisque Workload on Rancher 2.0 Setup (with Kubernetes engine)"},{"location":"guides/rancher2_bisque_gpu/#pre-requisite","text":"Rancher 2.0 with bq-cluster workload instructions Rancher 2.0 with PostgreSql workload instructions","title":"Pre-requisite"},{"location":"guides/rancher2_bisque_gpu/#general-instructions","text":"Setup Cluster with /rke-clusters/custom-nodes","title":"General instructions"},{"location":"guides/rancher2_bisque_gpu/#note-assuming-we-have-bq-cluster-with-postgres-and-persistent-volumes","text":"","title":"Note: Assuming we have bq-cluster with postgres and persistent volumes"},{"location":"guides/rancher2_bisque_gpu/#setup-workload-on-the-bq-cluster","text":"Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ We will be using the image at custom registry biodev.ece.ucsb.edu:5000/ucsb-bisque05-svc or another available at vishwakarmarhl/ucsb-bisque05-svc:dev","title":"Setup Workload on the bq-cluster"},{"location":"guides/rancher2_bisque_gpu/#bisque-service-workload-configuration","text":"Name: ucsb-bisque05-svc Pods: 2 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev or vishwakarmarhl/ucsb-bisque05-svc:dev Port Mapping: 8080-tcp-NodePort-Random 27000-tcp-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host Health Check: No change Volumes: Persistent Volume claim and set the mount point as /run/bisque Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change","title":"Bisque Service Workload configuration"},{"location":"guides/rancher2_bisque_gpu/#environment-configuration","text":"Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/postgres DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3","title":"Environment Configuration"},{"location":"guides/rancher2_bisque_gpu/#condor-provisioning","text":"Condor Master - Image: biodev.ece.ucsb.edu:5000/condor - Ports: 9618, 9886 as HostPort - Environment CONDOR_DAEMONS = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT CONDOR_MANAGER_HOST = master Condor Worker - Same configuration as above - Ports: 9886 NodePort Random","title":"Condor provisioning"},{"location":"guides/rancher2_bisque_gpu/#workload-bq-cluster-dashboard","text":"","title":"Workload (bq-cluster) Dashboard"},{"location":"guides/rancher2_bisque_gpu/#connoisseurgpu-workload-provisioning","text":"Here lets take a look at connoissuer service in Bisque. We will run a Bisque H1 server disabling the engine_service and connoisseur in part (A) and later another service in part (B) with Connoisseur and engine service for the actual compute.","title":"Connoisseur/GPU Workload provisioning"},{"location":"guides/rancher2_bisque_gpu/#pre-requisites","text":"Create a namespace \"connoisseur\" and deploy everything isolated from the existing bisque-svc Create a postgres database for this deployment psql -h 10.42.0.15 -U postgres --password -p 5432 postgres \"create database connoissuer;\" \"grant all privileges on database connoisseur to postgres;\" Create a volume bqcon-vol mounted at 192.168.1.123:/opt/bisque/connoisseur over NFS","title":"Pre-requisites"},{"location":"guides/rancher2_bisque_gpu/#a-bisque-client-service-workload-configuration","text":"Name: bq-connoisseur-client-svc Pods: 1 Docker Image : biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Port Mapping: 80-tcp-NodePort-Random 27000-tcp-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host that has GPU, Say \"arkady\" Health Check: No change Volumes: Persistent Volume claim of bqcon-vol and set the mount point as /run/bisque/ Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change","title":"A.) Bisque Client Service Workload configuration"},{"location":"guides/rancher2_bisque_gpu/#environment-configuration_1","text":"Bisque client service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@loup.ece.ucsb.edu BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 BISQUE_DBURL=postgresql://postgres:postgres@10.42.0.15:5432/connoisseur BISQUE_SERVERS_H1_SERVICES_DISABLED = engine_service,connoisseur","title":"Environment Configuration"},{"location":"guides/rancher2_bisque_gpu/#b-bisque-engine-service-workload-configuration","text":"All the same configuration as above but skipping one environment variable Skip the disable variable -- BISQUE_SERVERS_H1_SERVICES_DISABLED = engine_service,connoisseur Verify connoisseur GPU variables NVIDIA_REQUIRE_CUDA=cuda =8.0 NVIDIA_VISIBLE_DEVICES=all NVIDIA_DRIVER_CAPABILITIES=compute,utility CUDA_PKG_VERSION=8-0=8.0.61-1 CAFFE_PKG_VERSION=0.15.13-1ubuntu16.04+cuda8.0 CAFFE_VERSION=0.15 CUDA_VERSION=8.0.61 CONDOR_MANAGER_HOST=master.condor CONDOR_DAEMONS=MASTER,SCHEDD,SHARED_PORT","title":"B.) Bisque Engine Service Workload configuration"},{"location":"guides/rancher2_bisque_gpu/#workload-bq-cluster-with-namespace-connoissuer","text":"TODO: - Fix Caffe and CUDA within the image - Probably write your own Docker file for a new CUDA/GPU enabled container bisque@bq-connoisseur-engine-svc-74755f798b-6lbgk:/source$ caffe deveice_query -gpu all E0213 00:36:24.855478 1798 caffe.cpp:77] Available caffe actions: E0213 00:36:24.856573 1798 caffe.cpp:80] device_query E0213 00:36:24.856690 1798 caffe.cpp:80] test E0213 00:36:24.856793 1798 caffe.cpp:80] time E0213 00:36:24.856899 1798 caffe.cpp:80] train F0213 00:36:24.857019 1798 caffe.cpp:82] Unknown action: deveice_query *** Check failure stack trace: *** @ 0x7fafbc7c65cd google::LogMessage::Fail() @ 0x7fafbc7c8433 google::LogMessage::SendToLog() @ 0x7fafbc7c615b google::LogMessage::Flush() @ 0x7fafbc7c8e1e google::LogMessageFatal::~LogMessageFatal() @ 0x40863a main @ 0x7fafbb218830 __libc_start_main @ 0x408dd9 _start @ (nil) (unknown) Aborted (core dumped)","title":"Workload (bq-cluster) with Namespace Connoissuer"},{"location":"guides/rancher2_bisque_openstack/","text":"Rancher 2.0 (with Kubernetes engine) Openstack Pre-requisite Packages list on bisquesvc production (xenial-caffe) env Custom Registry is available and active at https://biodev.ece.ucsb.edu:5000/v2/_catalog Dev Pi Packages are acccessible Openstack environment as per the screenshot Add Openstack storage volume as per the Bisque requirement and volume available in openstack. This will later be NFS mounted for this tutorial. Nvidia-docker Since there is a GPU requirement in Bisque Connoisseur. - Install Docker - Use a specific version of docker for compatibility with nvidia-docker2 on Xenial or Bionic based on your system ``` sudo apt-get install docker-ce=5:18.09.1~3-0~ubuntu-xenial \\ docker-ce-cli=5:18.09.1~3-0~ubuntu-xenial containerd.io OR sudo apt-get install docker-ce=5:18.09.2~3-0~ubuntu-bionic \\ docker-ce-cli=5:18.09.2~3-0~ubuntu-bionic containerd.io ``` Install Nvidia-Docker , Configure default nvidia-runtime: https://github.com/NVIDIA/k8s-device-plugin#preparing-your-gpu-nodes # Run the following to insert the configuration sudo echo { \\ default-runtime\\ : \\ nvidia\\ , \\ runtimes\\ : { \\ nvidia\\ : { \\ path\\ : \\ nvidia-container-runtime\\ , \\ runtimeArgs\\ : [] }}} \\ /etc/docker/daemon.json # Now verify the config and it should look like below sudo echo /etc/docker/daemon.json { default-runtime : nvidia , runtimes : { nvidia : { path : nvidia-container-runtime , runtimeArgs : [] } } } (Not Prefferred, use only if you know what you are doing) Configure the docker to use a particular storage path or even a NFS mounted path # Edit where images are stored at sudo vim /etc/default/docker DOCKER_OPTS= -dns 8.8.8.8 -dns 8.8.4.4 -g /run/bisque/docker Check whether you can execute nvidia-smi inside a container without the runtime flag sudo service docker restart docker run --rm nvidia/cuda:9.0-base nvidia-smi PostgreSQL server Setup PostgreSql 10.4 on Rancher workload Verify the connectivity to this database using the below command psql -h postgres.prod -U postgres --password -p 5432 postgres This will be used in the Bisque configuration as environment variable later BISQUE_DBURL=postgresql://postgres:postgres@postgres.prod:5432/postgres A. Cluster Description Deployment Google Slides ==Cluster== - bisque-dev-01.cyverse.org , ubuntu@128.196.65.71 - bisque-dev-02.cyverse.org , ubuntu@128.196.65.100 - bisque-dev-gpu-01.cyverse.org , ubuntu@128.196.65.142 B. Lets Encrypt on Ubuntu 16.04 Note: Only works on port 80 - Bind the hostname to the IP address by creating an A record in DNS - Letsencrypt ACME challenge at TCP/80 on host - Open up firewall for this \"sudo ufw allow 80 sudo ufw allow 80 443\" - Verify the port 80 availability sudo netstat -peanut | grep :80 Setup certificates at /etc/letsencrypt on each node sudo certbot certonly --standalone --dry-run \\ --agree-tos -m vishwakarma@ucsb.edu \\ --cert-name bisque-dev-01.cyverse.org \\ -d bisque-dev-01.cyverse.org sudo certbot certonly --standalone --dry-run \\ --agree-tos -m vishwakarma@ucsb.edu \\ --cert-name bisque-dev-02.cyverse.org \\ -d bisque-dev-02.cyverse.org sudo certbot certonly --standalone --dry-run \\ --agree-tos -m vishwakarma@ucsb.edu \\ --cert-name bisque-dev-gpu-01.cyverse.org \\ -d bisque-dev-gpu-01.cyverse.org C. Master Rancher 2.0 Install/Startup Rancher: https://rancher.com/docs/rancher/v2.x/en/installation/single-node/ - Rancher etcd data persisted at /var/lib/rancher - Since port 80 is occupied by rancher/rancher, a rancher/rancher-agent cannot be run on this node. docker run -d --restart=unless-stopped \\ -p 8080:80 -p 8443:443 \\ -v /var/log/rancher/auditlog:/var/log/auditlog \\ -v /host/rancher:/var/lib/rancher \\ -e AUDIT_LEVEL=1 \\ rancher/rancher:stable You will have rancher accessible at https://bisque-dev-01.cyverse.org:8443 if everything goes fine D. Setup Cluster RKE/custom-nodes Create a cluster and name it \"bq-cluster\" Use Calico for the CNI network layer Choose custom cloud provider leveraging k8s Make this server as etcd, controlplane and worker sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes \\ -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 \\ --server https://bisque-dev-01.cyverse.org:8443 \\ --token 2s7gbd8gj9vx4pv4vgsq899gkkxdfrxc8j8wh8njgcgwdmtfkt7gd8 \\ --ca-checksum cf2f54c1837a5d9e1f8cc7db35efdb9a497c16b4ea9022b52b877a7cc94be505 \\ --node-name bisque-dev-gpu-01.cyverse.org \\ --address 128.196.65.142 \\ --etcd --controlplane --worker \\ --label type=gpu You will see a green bar below saying a particular node has registered. You can add more nodes to the cluster by editing the cluster and running another command as below sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes \\ -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 \\ --server https://bisque-dev-01.cyverse.org:8443 \\ --token 2sw5pb6nv2vqhrb8hrkbc6dgbn9k9lmw9kv6h8qzgmncxg9bfz6qpl \\ --ca-checksum cf2f54c1837a5d9e1f8cc7db35efdb9a497c16b4ea9022b52b877a7cc94be505 \\ --node-name bisque-dev-02.cyverse.org \\ --address 128.196.65.100 \\ --controlplane --worker \\ --label type=cpu Port requirements Open up ports based on the CNI provider requirements - See requirements for Calico as the provider in this case Service Discovery This Service Discovery entry enables DNS resolution for the workload\u2019s pods using the following naming convention: workload . namespace .svc.cluster.local E. Setup Volume Mount the host directory for volume using NFS and setup the nfs client access for the cluster https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04 Setup folders # Create the path on host system sudo mkdir /etc/letsencrypt/ -p \\ sudo mkdir /run/bisque/ -p \\ sudo mkdir /run/bisque/data -p \\ sudo mkdir /run/bisque/local/workdir -p # Allow other users to edit this sudo chown -R nobody:nogroup /run/bisque/ sudo chmod 0777 -R /run/bisque Now add NFS host configuration at /etc/exports /run/bisque 128.196.65.100(rw,sync,no_root_squash,no_subtree_check) /run/bisque 128.196.65.142(rw,sync,no_root_squash,no_subtree_check) /run/postgres 128.196.65.100(rw,sync,no_root_squash,no_subtree_check) /run/postgres 128.196.65.142(rw,sync,no_root_squash,no_subtree_check) restart the nfs server on the NFS host machine sudo systemctl restart nfs-kernel-server Mount the NFS folder on the client machine sudo apt-get install nfs-common sudo mount 128.196.65.71:/run/bisque/ /run/bisque/ sudo mount 128.196.65.71:/run/postgres/ /run/postgres/ Verify the mount on a client system using df -h AND https://www.claudiokuenzler.com/blog/786/rancher-2.0-create-persistent-volume-from-nfs-share Create a persistent volume in the cluster Set NFS-Share option on the node as /run/bisque We can see all the volumes that are created in the Volumes section of the \"bq-cluster\" workload F. Setup Workload (on the cluster) Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/ Condor Setup Installation Configuration - HT Condor install instructions Condor Master Node Deployment. This service will be discoverable at master.condor.svc.cluster.local Name: master Namespace: condor Pods: 1 Docker Image: biodev.ece.ucsb.edu:5000/condor Port Mapping: 9618 TCP HostPort 9618 9886 TCP HostPort 9886 Environment CONDOR_MANAGER_HOST = master.condor CONDOR_DAEMONS = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT Also, verify the /etc/condor/condor_config.local has property DAEMON_LIST = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT Condor Worker Node Deployment Name: worker Namespace: condor Pods: 2 Docker Image: biodev.ece.ucsb.edu:5000/condor Port Mapping: 9886 TCP ClusterIP(Internal) 9886 Volumes Persistent Volume from Node path /var/run/docker.sock (enables docker run) Node Scheduling: Run all the pods on a particular host (GPU based bisque-dev-gpu-01.cyverse.org due to caffe engine requirements) Environment CONDOR_MANAGER_HOST = master.condor CONDOR_DAEMONS = MASTER,SCHEDD,STARTD,SHARED_PORT Condor Configuration: Advanced Setup Instructions Configuration is needed to make sure the pool is discoverable, functional and collecting jobs. Bisque workload configuration We will be using the image at custom registry biodev.ece.ucsb.edu:5000 or we can use any publicly deployed image at https://hub.docker.com . The GPU enabled development environment is available at vishwakarmarhl/ucsb-bisque05-c9r:latest and the production image is at biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Name: bisquesvc Pods: 1 Docker Image: biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Port Mapping: 80-TCP-NodePort-Random 8080-TCP-NodePort-Random 27000-TCP-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host (GPU based bisque-dev-gpu-01.cyverse.org due to caffe engine) Health Check: No change Volumes Persistent Volume claim and set the mount point as /run/bisque Persistent Volume from Node path /etc/letsencrypt Persistent Volume from Node path /var/run/docker.sock Scaling: No change Command: (Only, in case needed. Not used with the current Image) Entrypoint: /builder/run-bisque.sh Command: bootstrap start Working Dir: /source Console: Interactive TTY (-i -t) Networking: Cluster first with host network Labels: No change Security Host: Privileged is True Finally we can see the overall state of pods in the workload within the clusters. Also view logs to track the Bisque initialization Environment Configuration Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@bisque-dev-gpu-01.cyverse.org BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@postgres.prod:5432/postgres CONDOR_DAEMONS= MASTER,SCHEDD,SHARED_PORT CONDOR_MANAGER_HOST= master.condor DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 We should see the overview of workloads deployed as below Service should be running at http://bisque-dev-gpu-01.cyverse.org:31274 Bisque GPU Verification (nvidia-docker should be default runtime) Connect into the container and verify \"nvidia-smi\" client Nvidia-Docker can be tested with docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi Test whether nvidia-runtime is default docker run --rm nvidia/cuda:9.0-base nvidia-smi You should be able to tail the log on the node where you have deployed the container. sudo tail -f /var/log/containers/prod_bisquecon-*.log G. Load Balancing (using L7 Ingress) Add Ingress configuration for load balancing with name \"bq-website\" Configure the target(bisquesvc) pods so that the port 80 is used","title":"Rancher 2.0 (with Kubernetes engine) Openstack"},{"location":"guides/rancher2_bisque_openstack/#rancher-20-with-kubernetes-engine-openstack","text":"","title":"Rancher 2.0 (with Kubernetes engine) Openstack"},{"location":"guides/rancher2_bisque_openstack/#pre-requisite","text":"Packages list on bisquesvc production (xenial-caffe) env Custom Registry is available and active at https://biodev.ece.ucsb.edu:5000/v2/_catalog Dev Pi Packages are acccessible Openstack environment as per the screenshot Add Openstack storage volume as per the Bisque requirement and volume available in openstack. This will later be NFS mounted for this tutorial.","title":"Pre-requisite"},{"location":"guides/rancher2_bisque_openstack/#nvidia-docker","text":"Since there is a GPU requirement in Bisque Connoisseur. - Install Docker - Use a specific version of docker for compatibility with nvidia-docker2 on Xenial or Bionic based on your system ``` sudo apt-get install docker-ce=5:18.09.1~3-0~ubuntu-xenial \\ docker-ce-cli=5:18.09.1~3-0~ubuntu-xenial containerd.io OR sudo apt-get install docker-ce=5:18.09.2~3-0~ubuntu-bionic \\ docker-ce-cli=5:18.09.2~3-0~ubuntu-bionic containerd.io ``` Install Nvidia-Docker , Configure default nvidia-runtime: https://github.com/NVIDIA/k8s-device-plugin#preparing-your-gpu-nodes # Run the following to insert the configuration sudo echo { \\ default-runtime\\ : \\ nvidia\\ , \\ runtimes\\ : { \\ nvidia\\ : { \\ path\\ : \\ nvidia-container-runtime\\ , \\ runtimeArgs\\ : [] }}} \\ /etc/docker/daemon.json # Now verify the config and it should look like below sudo echo /etc/docker/daemon.json { default-runtime : nvidia , runtimes : { nvidia : { path : nvidia-container-runtime , runtimeArgs : [] } } } (Not Prefferred, use only if you know what you are doing) Configure the docker to use a particular storage path or even a NFS mounted path # Edit where images are stored at sudo vim /etc/default/docker DOCKER_OPTS= -dns 8.8.8.8 -dns 8.8.4.4 -g /run/bisque/docker Check whether you can execute nvidia-smi inside a container without the runtime flag sudo service docker restart docker run --rm nvidia/cuda:9.0-base nvidia-smi","title":"Nvidia-docker"},{"location":"guides/rancher2_bisque_openstack/#postgresql-server","text":"Setup PostgreSql 10.4 on Rancher workload Verify the connectivity to this database using the below command psql -h postgres.prod -U postgres --password -p 5432 postgres This will be used in the Bisque configuration as environment variable later BISQUE_DBURL=postgresql://postgres:postgres@postgres.prod:5432/postgres","title":"PostgreSQL server"},{"location":"guides/rancher2_bisque_openstack/#a-cluster-description","text":"Deployment Google Slides ==Cluster== - bisque-dev-01.cyverse.org , ubuntu@128.196.65.71 - bisque-dev-02.cyverse.org , ubuntu@128.196.65.100 - bisque-dev-gpu-01.cyverse.org , ubuntu@128.196.65.142","title":"A. Cluster Description"},{"location":"guides/rancher2_bisque_openstack/#b-lets-encrypt-on-ubuntu-1604","text":"Note: Only works on port 80 - Bind the hostname to the IP address by creating an A record in DNS - Letsencrypt ACME challenge at TCP/80 on host - Open up firewall for this \"sudo ufw allow 80 sudo ufw allow 80 443\" - Verify the port 80 availability sudo netstat -peanut | grep :80 Setup certificates at /etc/letsencrypt on each node sudo certbot certonly --standalone --dry-run \\ --agree-tos -m vishwakarma@ucsb.edu \\ --cert-name bisque-dev-01.cyverse.org \\ -d bisque-dev-01.cyverse.org sudo certbot certonly --standalone --dry-run \\ --agree-tos -m vishwakarma@ucsb.edu \\ --cert-name bisque-dev-02.cyverse.org \\ -d bisque-dev-02.cyverse.org sudo certbot certonly --standalone --dry-run \\ --agree-tos -m vishwakarma@ucsb.edu \\ --cert-name bisque-dev-gpu-01.cyverse.org \\ -d bisque-dev-gpu-01.cyverse.org","title":"B. Lets Encrypt on Ubuntu 16.04"},{"location":"guides/rancher2_bisque_openstack/#c-master-rancher-20","text":"Install/Startup Rancher: https://rancher.com/docs/rancher/v2.x/en/installation/single-node/ - Rancher etcd data persisted at /var/lib/rancher - Since port 80 is occupied by rancher/rancher, a rancher/rancher-agent cannot be run on this node. docker run -d --restart=unless-stopped \\ -p 8080:80 -p 8443:443 \\ -v /var/log/rancher/auditlog:/var/log/auditlog \\ -v /host/rancher:/var/lib/rancher \\ -e AUDIT_LEVEL=1 \\ rancher/rancher:stable You will have rancher accessible at https://bisque-dev-01.cyverse.org:8443 if everything goes fine","title":"C. Master Rancher 2.0"},{"location":"guides/rancher2_bisque_openstack/#d-setup-cluster-rkecustom-nodes","text":"Create a cluster and name it \"bq-cluster\" Use Calico for the CNI network layer Choose custom cloud provider leveraging k8s Make this server as etcd, controlplane and worker sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes \\ -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 \\ --server https://bisque-dev-01.cyverse.org:8443 \\ --token 2s7gbd8gj9vx4pv4vgsq899gkkxdfrxc8j8wh8njgcgwdmtfkt7gd8 \\ --ca-checksum cf2f54c1837a5d9e1f8cc7db35efdb9a497c16b4ea9022b52b877a7cc94be505 \\ --node-name bisque-dev-gpu-01.cyverse.org \\ --address 128.196.65.142 \\ --etcd --controlplane --worker \\ --label type=gpu You will see a green bar below saying a particular node has registered. You can add more nodes to the cluster by editing the cluster and running another command as below sudo docker run -d --privileged --restart=unless-stopped --net=host \\ -v /etc/kubernetes:/etc/kubernetes \\ -v /var/run:/var/run \\ rancher/rancher-agent:v2.1.6 \\ --server https://bisque-dev-01.cyverse.org:8443 \\ --token 2sw5pb6nv2vqhrb8hrkbc6dgbn9k9lmw9kv6h8qzgmncxg9bfz6qpl \\ --ca-checksum cf2f54c1837a5d9e1f8cc7db35efdb9a497c16b4ea9022b52b877a7cc94be505 \\ --node-name bisque-dev-02.cyverse.org \\ --address 128.196.65.100 \\ --controlplane --worker \\ --label type=cpu","title":"D. Setup Cluster RKE/custom-nodes"},{"location":"guides/rancher2_bisque_openstack/#port-requirements","text":"Open up ports based on the CNI provider requirements - See requirements for Calico as the provider in this case","title":"Port requirements"},{"location":"guides/rancher2_bisque_openstack/#service-discovery","text":"This Service Discovery entry enables DNS resolution for the workload\u2019s pods using the following naming convention: workload . namespace .svc.cluster.local","title":"Service Discovery"},{"location":"guides/rancher2_bisque_openstack/#e-setup-volume","text":"Mount the host directory for volume using NFS and setup the nfs client access for the cluster https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04 Setup folders # Create the path on host system sudo mkdir /etc/letsencrypt/ -p \\ sudo mkdir /run/bisque/ -p \\ sudo mkdir /run/bisque/data -p \\ sudo mkdir /run/bisque/local/workdir -p # Allow other users to edit this sudo chown -R nobody:nogroup /run/bisque/ sudo chmod 0777 -R /run/bisque Now add NFS host configuration at /etc/exports /run/bisque 128.196.65.100(rw,sync,no_root_squash,no_subtree_check) /run/bisque 128.196.65.142(rw,sync,no_root_squash,no_subtree_check) /run/postgres 128.196.65.100(rw,sync,no_root_squash,no_subtree_check) /run/postgres 128.196.65.142(rw,sync,no_root_squash,no_subtree_check) restart the nfs server on the NFS host machine sudo systemctl restart nfs-kernel-server Mount the NFS folder on the client machine sudo apt-get install nfs-common sudo mount 128.196.65.71:/run/bisque/ /run/bisque/ sudo mount 128.196.65.71:/run/postgres/ /run/postgres/ Verify the mount on a client system using df -h AND https://www.claudiokuenzler.com/blog/786/rancher-2.0-create-persistent-volume-from-nfs-share Create a persistent volume in the cluster Set NFS-Share option on the node as /run/bisque We can see all the volumes that are created in the Volumes section of the \"bq-cluster\" workload","title":"E. Setup Volume"},{"location":"guides/rancher2_bisque_openstack/#f-setup-workload-on-the-cluster","text":"Bisque Test environment where workloads are deployed with open NodePort https://rancher.com/managing-kubernetes-workloads-with-rancher-2-0/","title":"F. Setup Workload (on the cluster)"},{"location":"guides/rancher2_bisque_openstack/#condor-setup","text":"Installation Configuration - HT Condor install instructions","title":"Condor Setup"},{"location":"guides/rancher2_bisque_openstack/#condor-master-node-deployment-this-service-will-be-discoverable-at-mastercondorsvcclusterlocal","text":"Name: master Namespace: condor Pods: 1 Docker Image: biodev.ece.ucsb.edu:5000/condor Port Mapping: 9618 TCP HostPort 9618 9886 TCP HostPort 9886 Environment CONDOR_MANAGER_HOST = master.condor CONDOR_DAEMONS = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT Also, verify the /etc/condor/condor_config.local has property DAEMON_LIST = COLLECTOR,MASTER,NEGOTIATOR,SCHEDD,SHARED_PORT","title":"Condor Master Node Deployment. This service will be discoverable at master.condor.svc.cluster.local"},{"location":"guides/rancher2_bisque_openstack/#condor-worker-node-deployment","text":"Name: worker Namespace: condor Pods: 2 Docker Image: biodev.ece.ucsb.edu:5000/condor Port Mapping: 9886 TCP ClusterIP(Internal) 9886 Volumes Persistent Volume from Node path /var/run/docker.sock (enables docker run) Node Scheduling: Run all the pods on a particular host (GPU based bisque-dev-gpu-01.cyverse.org due to caffe engine requirements) Environment CONDOR_MANAGER_HOST = master.condor CONDOR_DAEMONS = MASTER,SCHEDD,STARTD,SHARED_PORT","title":"Condor Worker Node Deployment"},{"location":"guides/rancher2_bisque_openstack/#condor-configuration-advanced-setup-instructions","text":"Configuration is needed to make sure the pool is discoverable, functional and collecting jobs.","title":"Condor Configuration: Advanced Setup Instructions"},{"location":"guides/rancher2_bisque_openstack/#bisque-workload-configuration","text":"We will be using the image at custom registry biodev.ece.ucsb.edu:5000 or we can use any publicly deployed image at https://hub.docker.com . The GPU enabled development environment is available at vishwakarmarhl/ucsb-bisque05-c9r:latest and the production image is at biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Name: bisquesvc Pods: 1 Docker Image: biodev.ece.ucsb.edu:5000/bisque-caffe-xenial:dev Port Mapping: 80-TCP-NodePort-Random 8080-TCP-NodePort-Random 27000-TCP-NodePort-Random Environment Variables: Copy paste the \"Environment Configuration\" section Node Scheduling: Run all the pods on a particular host (GPU based bisque-dev-gpu-01.cyverse.org due to caffe engine) Health Check: No change Volumes Persistent Volume claim and set the mount point as /run/bisque Persistent Volume from Node path /etc/letsencrypt Persistent Volume from Node path /var/run/docker.sock Scaling: No change Command: (Only, in case needed. Not used with the current Image) Entrypoint: /builder/run-bisque.sh Command: bootstrap start Working Dir: /source Console: Interactive TTY (-i -t) Networking: Cluster first with host network Labels: No change Security Host: Privileged is True Finally we can see the overall state of pods in the workload within the clusters. Also view logs to track the Bisque initialization","title":"Bisque workload configuration"},{"location":"guides/rancher2_bisque_openstack/#environment-configuration","text":"Bisque service variables BISQUE_USER= bisque BISQUE_BISQUE_ADMIN_EMAIL= admin@bisque-dev-gpu-01.cyverse.org BISQUE_BISQUE_BLOB_SERVICE_STORES= blobs,local BISQUE_BISQUE_STORES_BLOBS_MOUNTURL= file://$$datadir/blobdir/$$user/ BISQUE_BISQUE_STORES_BLOBS_TOP= file://$$datadir/blobdir/ BISQUE_BISQUE_STORES_LOCAL_MOUNTURL= file://$$datadir/imagedir/$$user/ BISQUE_BISQUE_STORES_LOCAL_READONLY= true BISQUE_BISQUE_STORES_LOCAL_TOP= file://$$datadir/imagedir/ BISQUE_DOCKER_DOCKER_HUB= biodev.ece.ucsb.edu:5000 BISQUE_SECRET= bq123 BISQUE_UID= 12027 BISQUE_RUNTIME_STAGING_BASE= /run/bisque/data/staging BQ__BISQUE__IMAGE_SERVICE__WORK_DIR= /run/bisque/local/workdir BQ__BISQUE__PATHS__DATA= /run/bisque/data MAIL_SERVER= dough.ece.ucsb.edu BISQUE_DBURL=postgresql://postgres:postgres@postgres.prod:5432/postgres CONDOR_DAEMONS= MASTER,SCHEDD,SHARED_PORT CONDOR_MANAGER_HOST= master.condor DEBIAN_FRONTEND=noninteractive IMGCNV=imgcnv_ubuntu16_2.4.3 We should see the overview of workloads deployed as below Service should be running at http://bisque-dev-gpu-01.cyverse.org:31274","title":"Environment Configuration"},{"location":"guides/rancher2_bisque_openstack/#bisque-gpu-verification-nvidia-docker-should-be-default-runtime","text":"Connect into the container and verify \"nvidia-smi\" client Nvidia-Docker can be tested with docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi Test whether nvidia-runtime is default docker run --rm nvidia/cuda:9.0-base nvidia-smi You should be able to tail the log on the node where you have deployed the container. sudo tail -f /var/log/containers/prod_bisquecon-*.log","title":"Bisque GPU Verification (nvidia-docker should be default runtime)"},{"location":"guides/rancher2_bisque_openstack/#g-load-balancing-using-l7-ingress","text":"Add Ingress configuration for load balancing with name \"bq-website\" Configure the target(bisquesvc) pods so that the port 80 is used","title":"G. Load Balancing (using L7 Ingress)"},{"location":"guides/rancher2_condor/","text":"The Condor instructions are based out of custom image(biodev.ece.ucsb.edu:5000/condor) Official Docs: https://research.cs.wisc.edu/htcondor/manual/v8.8 References: Condor Submit Jobs (Official Docs) Multi Node Condor Pool (Blog) Simple Condor Cluster (Blog) Condor Examples of Security Configurations (Official Docs) Install Docker Community Edition https://docs.docker.com/install/linux/docker-ce/ubuntu/ Topology The condor image available at the registry has htcondor==8.4.2~dfsg.1-1build1 pre-installed. Submit Node (host = bisquesvc.prod) Master node (host = master.condor) Worker Nodes (host = worker*.condor) Docker typically runs at worker and on the Bisque submit node for cases where we dont use condor at all Test this using docker ps on the worker nodes Now on each node we should start Condor and also make sure that the hostname \"master.condor\" is reachable from all the nodes in the pool, including the bisque submit node. $ ping master.condor PING master.condor.svc.cluster.local (10.43.55.33) 56(84) bytes of data. $ service condor start Master/Worker Condor Config Initiate startd at master and workers condor_startd Condor config vim /etc/condor/condor_config.local Add the following in the section where you see ALLOW_READ/WRITE keys ALLOW_ADMINISTRATOR = $(CONDOR_HOST) ALLOW_OWNER = $(FULL_HOSTNAME), $(ALLOW_ADMINISTRATOR) ALLOW_READ = * ALLOW_WRITE = * ALLOW_NEGOTIATOR = * ALLOW_NEGOTIATOR_SCHEDD = * ALLOW_WRITE_COLLECTOR = $(ALLOW_WRITE), $(FLOCK_FROM) ALLOW_WRITE_STARTD = $(ALLOW_WRITE), $(FLOCK_FROM) ALLOW_READ_COLLECTOR = $(ALLOW_READ), $(FLOCK_FROM) ALLOW_READ_STARTD = $(ALLOW_READ), $(FLOCK_FROM) ALLOW_CLIENT = * ALLOW_ADVERTISE_STARTD = * SEC_DEFAULT_NEGOTIATION = NEVER SEC_DEFAULT_AUTHENTICATION = NEVER Reconfig Restart Condor condor_reconfig service condor restart BisqueSvc Submit Node Condor Config Here is contents of the production configuration /etc/condor/condor_config.local file CONDOR_HOST = master.condor COLLECTOR_NAME = CBIUCSB DAEMON_LIST = MASTER,SCHEDD,SHARED_PORT CONDOR_ADMIN = admin@biodev.ece.ucsb.edu ## Do you want to use NFS for file access instead of remote system calls ALLOW_READ = $(ALLOW_READ), 172.*, 10.*, 128.111.*, *.ece.ucsb.edu, *.cs.ucsb.edu ALLOW_WRITE = $(ALLOW_WRITE), 172.*, 10.*, 128.111.*, *.ece.ucsb.edu, *.cs.ucsb.edu ALLOW_NEGOTIATOR = 172.*, 10.*, 128.111.* #https://lists.cs.wisc.edu/archive/htcondor-users/2016-December/msg00046.shtml DISCARD_SESSION_KEYRING_ON_STARTUP = false # Use CCB with shared port so outside units can talk to USE_SHARED_PORT = TRUE SHARED_PORT_ARGS = -p 9886 UPDATE_COLLECTOR_WITH_TCP = TRUE BIND_ALL_INTERFACES = TRUE # Slots for multi-cpu machines NUM_SLOTS = 1 NUM_SLOTS_TYPE_1 = 1 SLOT_TYPE_1 = 100% SLOT_TYPE_1_PARTITIONABLE = true START = True PREEMPT = False SUSPEND = False KILL = False WANT_SUSPEND = False WANT_VACATE= False CONTINUE= True State logs condor_status for the state of the pool root@bisquevc:/source# condor_status Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@master-7b988 LINUX X86_64 Unclaimed Idle -1.000 64423 0+13:07:28 slot1@worker-6c6cc LINUX X86_64 Unclaimed Idle -1.000 64423 0+13:04:24 Total Owner Claimed Unclaimed Matched Preempting Backfill X86_64/LINUX 2 0 0 2 0 0 0 Total 2 0 0 2 0 0 0 condor_q for the schedule of the queue. You can use \"-analyse\" to get additional details on the jobs. root@bisquesvc:/source# condor_q -- Schedd: bisquesvc : 10.42.0.15:40007 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Test Condor Condor is configured to be run as a user and not root. So we will change user (su bisque) to bisque and operate with a regular user level privileges for job submittion purposes. Create a file dock.sh with the following commands 1 2 3 4 #!/bin/bash echo Hello HTCondor from Job $1 running on `whoami`@`hostname` docker --version sleep 10s Create a dock.submit file with the following paramaters executable = dock.sh arguments = $(Process) universe = vanilla output = dock_$(Cluster)_$(Process).out error= dock_$(Cluster)_$(Process).error log = dock_$(Cluster)_$(Process).log should_transfer_files = YES when_to_transfer_output = ON_EXIT queue 2 Now execute bisque@bisquesvc:~/condor_dock_test$ condor_submit dock.submit Submitting job(s).. 2 job(s) submitted to cluster 10. Here we also observe the status of condor queue which should have the jobs running bisque@bisquesvc:~/condor_dock_test$ condor_q -- Schedd: bisquesvc : 10.42.0.15:9886?... ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 11.0 bisque 3/5 22:37 0+00:00:00 I 0 0.0 dock.sh 0 11.1 bisque 3/5 22:37 0+00:00:00 I 0 0.0 dock.sh 1 2 jobs; 0 completed, 0 removed, 2 idle, 0 running, 0 held, 0 suspended When the Master/worker nodes executes the job we see the following state bisque@bisquesvc:~/condor_dock_test$ condor_status Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@master-7b988 LINUX X86_64 Unclaimed Idle 0.000 64295 0+00:00:04 slot1_1@master-7b9 LINUX X86_64 Claimed Busy -1.000 128 0+00:00:04 slot1@worker-6c6cc LINUX X86_64 Unclaimed Idle 0.000 64295 0+00:00:04 slot1_1@worker-6c6 LINUX X86_64 Claimed Busy -1.000 128 0+00:00:04 Total Owner Claimed Unclaimed Matched Preempting Backfill X86_64/LINUX 4 0 2 2 0 0 0 Total 4 0 2 2 0 0 0 In about 10 seconds this execution will terminate and dump the results in corresponding log files bisque@bisquesvc:~/condor_dock_test$ condor_status Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@master-7b988 LINUX X86_64 Unclaimed Idle 0.000 64295 0+00:00:04 slot1@worker-6c6cc LINUX X86_64 Unclaimed Idle 0.000 64295 0+00:00:04 Total Owner Claimed Unclaimed Matched Preempting Backfill X86_64/LINUX 2 0 0 2 0 0 0 Total 2 0 0 2 0 0 0 bisque@bisquesvc:~/condor_dock_test$ condor_q -- Schedd: bisquesvc : 10.42.0.15:9886?... ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended To verify the execution at worker/master we can take a look at the /var/log/condor/StartLog # Logs at the master condor node 03/05/19 22:37:54 slot1_1: Request accepted. 03/05/19 22:37:54 slot1_1: Remote owner is bisque@bisquesvc 03/05/19 22:37:54 slot1_1: State change: claiming protocol successful 03/05/19 22:37:54 slot1_1: Changing state: Owner - Claimed 03/05/19 22:37:54 slot1_1: Got activate_claim request from shadow (10.42.0.15) 03/05/19 22:37:54 /proc format unknown for kernel version 4.15.0 03/05/19 22:37:54 slot1_1: Remote job ID is 11.0 03/05/19 22:37:54 slot1_1: Got universe VANILLA (5) from request classad 03/05/19 22:37:54 slot1_1: State change: claim-activation protocol successful 03/05/19 22:37:54 slot1_1: Changing activity: Idle - Busy 03/05/19 22:37:59 /proc format unknown for kernel version 4.15.0 03/05/19 22:38:04 /proc format unknown for kernel version 4.15.0 03/05/19 22:38:09 /proc format unknown for kernel version 4.15.0 03/05/19 22:38:09 slot1_1: Called deactivate_claim_forcibly() 03/05/19 22:38:09 Starter pid 1141 exited with status 0 03/05/19 22:38:09 slot1_1: State change: starter exited 03/05/19 22:38:09 slot1_1: Changing activity: Busy - Idle 03/05/19 22:38:09 slot1_1: State change: received RELEASE_CLAIM command 03/05/19 22:38:09 slot1_1: Changing state and activity: Claimed/Idle - Preempting/Vacating 03/05/19 22:38:09 slot1_1: State change: No preempting claim, returning to owner 03/05/19 22:38:09 slot1_1: Changing state and activity: Preempting/Vacating - Owner/Idle 03/05/19 22:38:09 slot1_1: State change: IS_OWNER is false 03/05/19 22:38:09 slot1_1: Changing state: Owner - Unclaimed 03/05/19 22:38:09 slot1_1: Changing state: Unclaimed - Delete 03/05/19 22:38:09 slot1_1: Resource no longer needed, deleting Final results of the output could be seen in the dock_11_*.out result files which represents the Job Id = 11 bisque@bisquesvc:~/condor_dock_test$ ll total 36 drwxrwxr-x 2 bisque bisque 4096 Mar 5 22:37 ./ drwxr-xr-x 1 bisque bisque 4096 Mar 5 22:34 ../ -rw-r--r-- 1 bisque bisque 0 Mar 5 22:37 dock_11_0.error -rw-rw-r-- 1 bisque bisque 1021 Mar 5 22:38 dock_11_0.log -rw-r--r-- 1 bisque bisque 132 Mar 5 22:38 dock_11_0.out -rw-r--r-- 1 bisque bisque 0 Mar 5 22:37 dock_11_1.error -rw-rw-r-- 1 bisque bisque 1021 Mar 5 22:38 dock_11_1.log -rw-r--r-- 1 bisque bisque 132 Mar 5 22:38 dock_11_1.out -rw-r--r-- 1 bisque bisque 163 Mar 5 22:34 dock.sh -rw-r--r-- 1 bisque bisque 253 Mar 5 09:07 dock.submit Output Result bisque@bisquesvc:~/condor_dock_test$ cat dock_11_1.out Hello HTCondor from Job 1 running on nobody@master-7b988ddb7d-hnspj Docker version 17.03.0-ce, build 60ccb22 Completed my first job Output Log bisque@bisquesvc:~/condor_dock_test$ cat dock_11_1.log 000 (011.001.000) 03/05 22:37:44 Job submitted from host: 10.42.0.15:9886?addrs=10.42.0.15-9886 noUDP sock=6207_60d7_3 ... 001 (011.001.000) 03/05 22:37:57 Job executing on host: 10.42.0.8:9886?sock=30363_868c ... 006 (011.001.000) 03/05 22:38:07 Image size of job updated: 1 8 - MemoryUsage of job (MB) 7560 - ResidentSetSize of job (KB) ... 005 (011.001.000) 03/05 22:38:09 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 132 - Run Bytes Sent By Job 163 - Run Bytes Received By Job 132 - Total Bytes Sent By Job 163 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 9 1 893677 Memory (MB) : 8 1 1 ...","title":"Rancher2 condor"},{"location":"guides/rancher2_condor/#references","text":"Condor Submit Jobs (Official Docs) Multi Node Condor Pool (Blog) Simple Condor Cluster (Blog) Condor Examples of Security Configurations (Official Docs) Install Docker Community Edition https://docs.docker.com/install/linux/docker-ce/ubuntu/","title":"References:"},{"location":"guides/rancher2_condor/#topology","text":"The condor image available at the registry has htcondor==8.4.2~dfsg.1-1build1 pre-installed. Submit Node (host = bisquesvc.prod) Master node (host = master.condor) Worker Nodes (host = worker*.condor) Docker typically runs at worker and on the Bisque submit node for cases where we dont use condor at all Test this using docker ps on the worker nodes Now on each node we should start Condor and also make sure that the hostname \"master.condor\" is reachable from all the nodes in the pool, including the bisque submit node. $ ping master.condor PING master.condor.svc.cluster.local (10.43.55.33) 56(84) bytes of data. $ service condor start","title":"Topology"},{"location":"guides/rancher2_condor/#masterworker-condor-config","text":"","title":"Master/Worker Condor Config"},{"location":"guides/rancher2_condor/#initiate-startd-at-master-and-workers","text":"condor_startd","title":"Initiate startd at master and workers"},{"location":"guides/rancher2_condor/#condor-config-vim-etccondorcondor_configlocal","text":"Add the following in the section where you see ALLOW_READ/WRITE keys ALLOW_ADMINISTRATOR = $(CONDOR_HOST) ALLOW_OWNER = $(FULL_HOSTNAME), $(ALLOW_ADMINISTRATOR) ALLOW_READ = * ALLOW_WRITE = * ALLOW_NEGOTIATOR = * ALLOW_NEGOTIATOR_SCHEDD = * ALLOW_WRITE_COLLECTOR = $(ALLOW_WRITE), $(FLOCK_FROM) ALLOW_WRITE_STARTD = $(ALLOW_WRITE), $(FLOCK_FROM) ALLOW_READ_COLLECTOR = $(ALLOW_READ), $(FLOCK_FROM) ALLOW_READ_STARTD = $(ALLOW_READ), $(FLOCK_FROM) ALLOW_CLIENT = * ALLOW_ADVERTISE_STARTD = * SEC_DEFAULT_NEGOTIATION = NEVER SEC_DEFAULT_AUTHENTICATION = NEVER","title":"Condor config vim /etc/condor/condor_config.local"},{"location":"guides/rancher2_condor/#reconfig-restart-condor","text":"condor_reconfig service condor restart","title":"Reconfig &amp; Restart Condor"},{"location":"guides/rancher2_condor/#bisquesvc-submit-node-condor-config","text":"Here is contents of the production configuration /etc/condor/condor_config.local file CONDOR_HOST = master.condor COLLECTOR_NAME = CBIUCSB DAEMON_LIST = MASTER,SCHEDD,SHARED_PORT CONDOR_ADMIN = admin@biodev.ece.ucsb.edu ## Do you want to use NFS for file access instead of remote system calls ALLOW_READ = $(ALLOW_READ), 172.*, 10.*, 128.111.*, *.ece.ucsb.edu, *.cs.ucsb.edu ALLOW_WRITE = $(ALLOW_WRITE), 172.*, 10.*, 128.111.*, *.ece.ucsb.edu, *.cs.ucsb.edu ALLOW_NEGOTIATOR = 172.*, 10.*, 128.111.* #https://lists.cs.wisc.edu/archive/htcondor-users/2016-December/msg00046.shtml DISCARD_SESSION_KEYRING_ON_STARTUP = false # Use CCB with shared port so outside units can talk to USE_SHARED_PORT = TRUE SHARED_PORT_ARGS = -p 9886 UPDATE_COLLECTOR_WITH_TCP = TRUE BIND_ALL_INTERFACES = TRUE # Slots for multi-cpu machines NUM_SLOTS = 1 NUM_SLOTS_TYPE_1 = 1 SLOT_TYPE_1 = 100% SLOT_TYPE_1_PARTITIONABLE = true START = True PREEMPT = False SUSPEND = False KILL = False WANT_SUSPEND = False WANT_VACATE= False CONTINUE= True","title":"BisqueSvc Submit Node Condor Config"},{"location":"guides/rancher2_condor/#state-logs","text":"condor_status for the state of the pool root@bisquevc:/source# condor_status Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@master-7b988 LINUX X86_64 Unclaimed Idle -1.000 64423 0+13:07:28 slot1@worker-6c6cc LINUX X86_64 Unclaimed Idle -1.000 64423 0+13:04:24 Total Owner Claimed Unclaimed Matched Preempting Backfill X86_64/LINUX 2 0 0 2 0 0 0 Total 2 0 0 2 0 0 0 condor_q for the schedule of the queue. You can use \"-analyse\" to get additional details on the jobs. root@bisquesvc:/source# condor_q -- Schedd: bisquesvc : 10.42.0.15:40007 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended","title":"State &amp; logs"},{"location":"guides/rancher2_condor/#test-condor","text":"Condor is configured to be run as a user and not root. So we will change user (su bisque) to bisque and operate with a regular user level privileges for job submittion purposes.","title":"Test Condor"},{"location":"guides/rancher2_condor/#create-a-file-docksh-with-the-following-commands","text":"1 2 3 4 #!/bin/bash echo Hello HTCondor from Job $1 running on `whoami`@`hostname` docker --version sleep 10s","title":"Create a file dock.sh with the following commands"},{"location":"guides/rancher2_condor/#create-a-docksubmit-file-with-the-following-paramaters","text":"executable = dock.sh arguments = $(Process) universe = vanilla output = dock_$(Cluster)_$(Process).out error= dock_$(Cluster)_$(Process).error log = dock_$(Cluster)_$(Process).log should_transfer_files = YES when_to_transfer_output = ON_EXIT queue 2","title":"Create a dock.submit file with the following paramaters"},{"location":"guides/rancher2_condor/#now-execute","text":"bisque@bisquesvc:~/condor_dock_test$ condor_submit dock.submit Submitting job(s).. 2 job(s) submitted to cluster 10. Here we also observe the status of condor queue which should have the jobs running bisque@bisquesvc:~/condor_dock_test$ condor_q -- Schedd: bisquesvc : 10.42.0.15:9886?... ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 11.0 bisque 3/5 22:37 0+00:00:00 I 0 0.0 dock.sh 0 11.1 bisque 3/5 22:37 0+00:00:00 I 0 0.0 dock.sh 1 2 jobs; 0 completed, 0 removed, 2 idle, 0 running, 0 held, 0 suspended When the Master/worker nodes executes the job we see the following state bisque@bisquesvc:~/condor_dock_test$ condor_status Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@master-7b988 LINUX X86_64 Unclaimed Idle 0.000 64295 0+00:00:04 slot1_1@master-7b9 LINUX X86_64 Claimed Busy -1.000 128 0+00:00:04 slot1@worker-6c6cc LINUX X86_64 Unclaimed Idle 0.000 64295 0+00:00:04 slot1_1@worker-6c6 LINUX X86_64 Claimed Busy -1.000 128 0+00:00:04 Total Owner Claimed Unclaimed Matched Preempting Backfill X86_64/LINUX 4 0 2 2 0 0 0 Total 4 0 2 2 0 0 0 In about 10 seconds this execution will terminate and dump the results in corresponding log files bisque@bisquesvc:~/condor_dock_test$ condor_status Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@master-7b988 LINUX X86_64 Unclaimed Idle 0.000 64295 0+00:00:04 slot1@worker-6c6cc LINUX X86_64 Unclaimed Idle 0.000 64295 0+00:00:04 Total Owner Claimed Unclaimed Matched Preempting Backfill X86_64/LINUX 2 0 0 2 0 0 0 Total 2 0 0 2 0 0 0 bisque@bisquesvc:~/condor_dock_test$ condor_q -- Schedd: bisquesvc : 10.42.0.15:9886?... ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended To verify the execution at worker/master we can take a look at the /var/log/condor/StartLog # Logs at the master condor node 03/05/19 22:37:54 slot1_1: Request accepted. 03/05/19 22:37:54 slot1_1: Remote owner is bisque@bisquesvc 03/05/19 22:37:54 slot1_1: State change: claiming protocol successful 03/05/19 22:37:54 slot1_1: Changing state: Owner - Claimed 03/05/19 22:37:54 slot1_1: Got activate_claim request from shadow (10.42.0.15) 03/05/19 22:37:54 /proc format unknown for kernel version 4.15.0 03/05/19 22:37:54 slot1_1: Remote job ID is 11.0 03/05/19 22:37:54 slot1_1: Got universe VANILLA (5) from request classad 03/05/19 22:37:54 slot1_1: State change: claim-activation protocol successful 03/05/19 22:37:54 slot1_1: Changing activity: Idle - Busy 03/05/19 22:37:59 /proc format unknown for kernel version 4.15.0 03/05/19 22:38:04 /proc format unknown for kernel version 4.15.0 03/05/19 22:38:09 /proc format unknown for kernel version 4.15.0 03/05/19 22:38:09 slot1_1: Called deactivate_claim_forcibly() 03/05/19 22:38:09 Starter pid 1141 exited with status 0 03/05/19 22:38:09 slot1_1: State change: starter exited 03/05/19 22:38:09 slot1_1: Changing activity: Busy - Idle 03/05/19 22:38:09 slot1_1: State change: received RELEASE_CLAIM command 03/05/19 22:38:09 slot1_1: Changing state and activity: Claimed/Idle - Preempting/Vacating 03/05/19 22:38:09 slot1_1: State change: No preempting claim, returning to owner 03/05/19 22:38:09 slot1_1: Changing state and activity: Preempting/Vacating - Owner/Idle 03/05/19 22:38:09 slot1_1: State change: IS_OWNER is false 03/05/19 22:38:09 slot1_1: Changing state: Owner - Unclaimed 03/05/19 22:38:09 slot1_1: Changing state: Unclaimed - Delete 03/05/19 22:38:09 slot1_1: Resource no longer needed, deleting Final results of the output could be seen in the dock_11_*.out result files which represents the Job Id = 11 bisque@bisquesvc:~/condor_dock_test$ ll total 36 drwxrwxr-x 2 bisque bisque 4096 Mar 5 22:37 ./ drwxr-xr-x 1 bisque bisque 4096 Mar 5 22:34 ../ -rw-r--r-- 1 bisque bisque 0 Mar 5 22:37 dock_11_0.error -rw-rw-r-- 1 bisque bisque 1021 Mar 5 22:38 dock_11_0.log -rw-r--r-- 1 bisque bisque 132 Mar 5 22:38 dock_11_0.out -rw-r--r-- 1 bisque bisque 0 Mar 5 22:37 dock_11_1.error -rw-rw-r-- 1 bisque bisque 1021 Mar 5 22:38 dock_11_1.log -rw-r--r-- 1 bisque bisque 132 Mar 5 22:38 dock_11_1.out -rw-r--r-- 1 bisque bisque 163 Mar 5 22:34 dock.sh -rw-r--r-- 1 bisque bisque 253 Mar 5 09:07 dock.submit Output Result bisque@bisquesvc:~/condor_dock_test$ cat dock_11_1.out Hello HTCondor from Job 1 running on nobody@master-7b988ddb7d-hnspj Docker version 17.03.0-ce, build 60ccb22 Completed my first job Output Log bisque@bisquesvc:~/condor_dock_test$ cat dock_11_1.log 000 (011.001.000) 03/05 22:37:44 Job submitted from host: 10.42.0.15:9886?addrs=10.42.0.15-9886 noUDP sock=6207_60d7_3 ... 001 (011.001.000) 03/05 22:37:57 Job executing on host: 10.42.0.8:9886?sock=30363_868c ... 006 (011.001.000) 03/05 22:38:07 Image size of job updated: 1 8 - MemoryUsage of job (MB) 7560 - ResidentSetSize of job (KB) ... 005 (011.001.000) 03/05 22:38:09 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 132 - Run Bytes Sent By Job 163 - Run Bytes Received By Job 132 - Total Bytes Sent By Job 163 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 9 1 893677 Memory (MB) : 8 1 1 ...","title":"Now execute"},{"location":"guides/rancher2_jenkins/","text":"Rancher 2 based Jenkins deployment/management guide Jenkins LTS Container (jenkins/jenkins:lts) - (jenkins/jenkins:2.150.3) Dockerfile on jenkinsci/docker Squid http-proxy for server access: https://help.ubuntu.com/lts/serverguide/squid.html.en Command /usr/share/jenkins/rancher/add-docker.sh Volume Mount Link ~/ws/jenkins_home folder in /var/jenkins_home and change ownership on host sudo ln -s /home/rahul/ws/jenkins_home /var/jenkins_home chown -R 1000:1000 ~/ws/jenkins_home chown 1000 /var/jenkins_home /var/run/docker.sock /usr/bin/docker /var/jenkins_home In case you are using rancher and want to deploy the container on a particular node then make sure that folder is NFS mounted and exists. # bqstage(192.168.1.123) node will be used to deploy jenkins sudo mount 192.168.1.123:/var/jenkins_home /var/jenkins_home Test Run docker run -p 8088:8080 -p 50000:50000 -v /var/jenkins_home:/var/jenkins_home jenkins/jenkins:lts Lets create a deployment in a cluster Name: jenkins-bq Image: jenkins/jenkins:lts Namespace: jenkins Port Mapping: 8080-tcp-NodePort-Random Environment Variables: No Change Node Scheduling: Run all the pods on a particular host with GPU capability Health Check: No change Volumes: Persistent Volume claim and set the mount point as /var/jenkins_home Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change Port 8080","title":"Rancher 2 based Jenkins deployment/management guide"},{"location":"guides/rancher2_jenkins/#rancher-2-based-jenkins-deploymentmanagement-guide","text":"Jenkins LTS Container (jenkins/jenkins:lts) - (jenkins/jenkins:2.150.3) Dockerfile on jenkinsci/docker Squid http-proxy for server access: https://help.ubuntu.com/lts/serverguide/squid.html.en","title":"Rancher 2 based Jenkins deployment/management guide"},{"location":"guides/rancher2_jenkins/#command","text":"/usr/share/jenkins/rancher/add-docker.sh","title":"Command"},{"location":"guides/rancher2_jenkins/#volume-mount","text":"Link ~/ws/jenkins_home folder in /var/jenkins_home and change ownership on host sudo ln -s /home/rahul/ws/jenkins_home /var/jenkins_home chown -R 1000:1000 ~/ws/jenkins_home chown 1000 /var/jenkins_home /var/run/docker.sock /usr/bin/docker /var/jenkins_home In case you are using rancher and want to deploy the container on a particular node then make sure that folder is NFS mounted and exists. # bqstage(192.168.1.123) node will be used to deploy jenkins sudo mount 192.168.1.123:/var/jenkins_home /var/jenkins_home","title":"Volume Mount"},{"location":"guides/rancher2_jenkins/#test-run","text":"docker run -p 8088:8080 -p 50000:50000 -v /var/jenkins_home:/var/jenkins_home jenkins/jenkins:lts","title":"Test Run"},{"location":"guides/rancher2_jenkins/#lets-create-a-deployment-in-a-cluster","text":"Name: jenkins-bq Image: jenkins/jenkins:lts Namespace: jenkins Port Mapping: 8080-tcp-NodePort-Random Environment Variables: No Change Node Scheduling: Run all the pods on a particular host with GPU capability Health Check: No change Volumes: Persistent Volume claim and set the mount point as /var/jenkins_home Scaling: No change Command: No Change Networking: No Change Labels: No change Security: No change","title":"Lets create a deployment in a cluster"},{"location":"guides/rancher2_jenkins/#port","text":"8080","title":"Port"},{"location":"guides/rancher2_postgresql/","text":"Setup Postgresql for Bisque Database Persistence Idea is, to not use the default Sqlite DB in a cluster environment The data directory is still being used for images/cache etc, and mounted as NFS across this cluster Reference: https://severalnines.com/blog/using-kubernetes-deploy-postgresql Create a Volume: pgdev-vol Path on the host node: /opt/bisque/pg Setup a Postgresql 10 database Image: postgres:10.4 Environment: POSTGRES_DB=postgres POSTGRES_PASSWORD=postgres POSTGRES_USER=postgres Ports: 5432/TCP - ClusterIP - Same as container port Volume: Mount pgdev-vol created above to mount point \"/var/lib/postgresql/data\" Here is the complete Postgres workload configuration screen shot for reference Make sure to claim this volume in the Postgres workload deploy configuration Test database using the nodes IP Test using psql, depending on what IP the postgres server is available at psql -h 10.42.0.15 -U postgres --password -p 5432 postgres psql -h postgres.prod -U postgres --password -p 5432 postgres Create database for a particular instance CREATE DATABASE connoisseur; GRANT ALL PRIVILEGES ON DATABASE connoisseur to postgres;","title":"Rancher2 postgresql"},{"location":"guides/rancher2_postgresql/#setup-postgresql-for-bisque-database-persistence","text":"Idea is, to not use the default Sqlite DB in a cluster environment The data directory is still being used for images/cache etc, and mounted as NFS across this cluster Reference: https://severalnines.com/blog/using-kubernetes-deploy-postgresql","title":"Setup Postgresql for Bisque Database Persistence"},{"location":"guides/rancher2_postgresql/#create-a-volume-pgdev-vol","text":"Path on the host node: /opt/bisque/pg","title":"Create a Volume: pgdev-vol"},{"location":"guides/rancher2_postgresql/#setup-a-postgresql-10-database","text":"Image: postgres:10.4 Environment: POSTGRES_DB=postgres POSTGRES_PASSWORD=postgres POSTGRES_USER=postgres Ports: 5432/TCP - ClusterIP - Same as container port Volume: Mount pgdev-vol created above to mount point \"/var/lib/postgresql/data\" Here is the complete Postgres workload configuration screen shot for reference Make sure to claim this volume in the Postgres workload deploy configuration","title":"Setup a Postgresql 10 database"},{"location":"guides/rancher2_postgresql/#test-database-using-the-nodes-ip","text":"Test using psql, depending on what IP the postgres server is available at psql -h 10.42.0.15 -U postgres --password -p 5432 postgres psql -h postgres.prod -U postgres --password -p 5432 postgres Create database for a particular instance CREATE DATABASE connoisseur; GRANT ALL PRIVILEGES ON DATABASE connoisseur to postgres;","title":"Test database using the nodes IP"},{"location":"guides/resources/module/connoisseur/","text":"Connoisseur Connoisseur is a machine learning service for BisQue which manages ML models and their creation/execution. Currently it only supports a deep learning framework Caffe. Issues to solve long running requests must run as celery tasks locking models is needed to protect binaries from parallel operations transaction during request is bad, need to update status of the resource in the middle of a long running request physical location for models store for templates need to capture output from long running caffe binary hierarchical models models loaded separately in each process/thread and thus will use more RAM and will be slower INFO API GET /connoisseur or GET /connoisseur/info CLASSIFICATION API GET /connoisseur/MODEL_ID/classify:IMAGE_ID GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points] # default, classify uniformly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points_random] # classify randomly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:segmentation] # segment image pixels producing image mask with class labels GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:regions] # partition image into regions producing low-res polygons GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient_uniform] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:image] # Parameters for all methods: points: number of output points, approximate for some methods goodness: % (0-100), minimum goodness for an output point border: % (0-100) of the image size, border in pixels will be estimated from image's dimensions parameters for point classification random and uniform GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:points[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf parameters for region partitioning GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:regions[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf parameters for segmentation GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:ids] GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:colors] formats: png MODEL INFO API GET /connoisseur/MODEL_ID/class:3/sample:1 RESTful API GET - request classification, preview or training Responses: * 204 Empty results * 400 Bad Request * 401 Unauthorized * 500 Internal Server Error * 501 Not Implemented MODEL DEFINITION See classifier_model.py CLASSIFIER OUTPUTS [table/gobs] gob type: str | gob label: str | gob vertices: [(x,y,z,...)] | goodness: float | accuracy: float | color: str [image/png] bytes... STATUS WORKFLOW new classes loaded ----- # needs to re-load all files from template and repeat all steps: 3-8 classes filtered | samples loaded ----- # needs to re-split the samples and other steps: 5-8 samples split | trained | validated | finished ------ TRAINING WORKFLOW The model resource is created by the JS UI and initially may only contain a user-given name and a dataset to train on. After the model resource is created in the system several operations may be requested following a typical training sequence. Define required model parameters a. Pick a model template to fine-tune or train from scratch b. This should define a \"mode\", currently point classification but later also fully convolutional Find classes for the model, this will lock the model for current classes and will not allow further changes Create training samples, using parameters defined in the model. This can be repeated to augment the sample list a. Absolute minimum number of samples per class b. Minimum number of samples to activate sample augmentation Train on available samples a. train the model b. validate the model and update classes performance Set classes that will be used for classification based on observed performance in the training step Model is ready for recognition Repeat step 3 and further to improve the model MODEL MODIFICATION API GET /connoisseur/create/template:CaffeNet[/dataset:UUID] GET /connoisseur/MODEL_ID/update[/classes:init][/classes:filter][/samples:init][/samples:update][/samples:split] GET /connoisseur/MODEL_ID/train[/method:finetune][/method:snapshot] GET /connoisseur/MODEL_ID/validate","title":"Connoisseur #"},{"location":"guides/resources/module/connoisseur/#connoisseur","text":"Connoisseur is a machine learning service for BisQue which manages ML models and their creation/execution. Currently it only supports a deep learning framework Caffe.","title":"Connoisseur"},{"location":"guides/resources/module/connoisseur/#issues-to-solve","text":"long running requests must run as celery tasks locking models is needed to protect binaries from parallel operations transaction during request is bad, need to update status of the resource in the middle of a long running request physical location for models store for templates need to capture output from long running caffe binary hierarchical models models loaded separately in each process/thread and thus will use more RAM and will be slower","title":"Issues to solve"},{"location":"guides/resources/module/connoisseur/#info-api","text":"GET /connoisseur or GET /connoisseur/info","title":"INFO API"},{"location":"guides/resources/module/connoisseur/#classification-api","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points] # default, classify uniformly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:points_random] # classify randomly distributed points GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:segmentation] # segment image pixels producing image mask with class labels GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:regions] # partition image into regions producing low-res polygons GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:salient_uniform] # GET /connoisseur/MODEL_ID/classify:IMAGE_ID[/method:image] # Parameters for all methods: points: number of output points, approximate for some methods goodness: % (0-100), minimum goodness for an output point border: % (0-100) of the image size, border in pixels will be estimated from image's dimensions","title":"CLASSIFICATION API"},{"location":"guides/resources/module/connoisseur/#parameters-for-point-classification-random-and-uniform","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:points[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf","title":"parameters for point classification random and uniform"},{"location":"guides/resources/module/connoisseur/#parameters-for-region-partitioning","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:regions[/points:10][/goodness:95][/border:5][/format:csv] format: xml, json, csv, hdf","title":"parameters for region partitioning"},{"location":"guides/resources/module/connoisseur/#parameters-for-segmentation","text":"GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:ids] GET /connoisseur/MODEL_ID/classify:IMAGE_ID/method:segmentation[/points:10][/goodness:95][/border:5][/colors:colors] formats: png","title":"parameters for segmentation"},{"location":"guides/resources/module/connoisseur/#model-info-api","text":"GET /connoisseur/MODEL_ID/class:3/sample:1","title":"MODEL INFO API"},{"location":"guides/resources/module/connoisseur/#restful-api","text":"GET - request classification, preview or training Responses: * 204 Empty results * 400 Bad Request * 401 Unauthorized * 500 Internal Server Error * 501 Not Implemented","title":"RESTful API"},{"location":"guides/resources/module/connoisseur/#model-definition","text":"See classifier_model.py","title":"MODEL DEFINITION"},{"location":"guides/resources/module/connoisseur/#classifier-outputs","text":"[table/gobs] gob type: str | gob label: str | gob vertices: [(x,y,z,...)] | goodness: float | accuracy: float | color: str [image/png] bytes...","title":"CLASSIFIER OUTPUTS"},{"location":"guides/resources/module/connoisseur/#status-workflow","text":"new classes loaded ----- # needs to re-load all files from template and repeat all steps: 3-8 classes filtered | samples loaded ----- # needs to re-split the samples and other steps: 5-8 samples split | trained | validated | finished ------","title":"STATUS WORKFLOW"},{"location":"guides/resources/module/connoisseur/#training-workflow","text":"The model resource is created by the JS UI and initially may only contain a user-given name and a dataset to train on. After the model resource is created in the system several operations may be requested following a typical training sequence. Define required model parameters a. Pick a model template to fine-tune or train from scratch b. This should define a \"mode\", currently point classification but later also fully convolutional Find classes for the model, this will lock the model for current classes and will not allow further changes Create training samples, using parameters defined in the model. This can be repeated to augment the sample list a. Absolute minimum number of samples per class b. Minimum number of samples to activate sample augmentation Train on available samples a. train the model b. validate the model and update classes performance Set classes that will be used for classification based on observed performance in the training step Model is ready for recognition Repeat step 3 and further to improve the model","title":"TRAINING WORKFLOW"},{"location":"guides/resources/module/connoisseur/#model-modification-api","text":"GET /connoisseur/create/template:CaffeNet[/dataset:UUID] GET /connoisseur/MODEL_ID/update[/classes:init][/classes:filter][/samples:init][/samples:update][/samples:split] GET /connoisseur/MODEL_ID/train[/method:finetune][/method:snapshot] GET /connoisseur/MODEL_ID/validate","title":"MODEL MODIFICATION API"}]}